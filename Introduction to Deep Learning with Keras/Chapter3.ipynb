{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Improving Your Model Performance\n",
    "\n",
    "In the previous chapters, you've trained a lot of models! You will now learn how to interpret learning curves to understand your models as they train. You will also visualize the effects of activation functions, batch-sizes, and batch-normalization. Finally, you will learn how to perform automatic hyperparameter optimization to your Keras models using sklearn."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Learning curves\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-29 142344.png\">\n",
    "<img src=\"image/Screenshot 2021-01-29 142401.png\">\n",
    "<img src=\"image/Screenshot 2021-01-29 142416.png\">\n",
    "<img src=\"image/Screenshot 2021-01-29 142445.png\">\n",
    "<img src=\"image/Screenshot 2021-01-29 142506.png\">\n",
    "<img src=\"image/Screenshot 2021-01-29 142536.png\">\n",
    "\n",
    "## Learning curves\n",
    "\n",
    "```\n",
    "# Store initial model weights\n",
    "init_weights = model.get_weights()\n",
    "# Lists for storing accuracies\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "```\n",
    "\n",
    "```\n",
    "for train_size in train_sizes:\n",
    "    # Split a fraction according to train_size\n",
    "    X_train_frac, _, y_train_frac, _ = train_test_split(X_train, Y_train, Train_size=train_size)\n",
    "    # Set model initial weigths\n",
    "    model.set_weights(init_weights)\n",
    "    # Fit model on the training set fractopm\n",
    "    model.fit(X_train_frac, y_train_frac, epoch=100, verbose=0, callbacks=[EarlyStopping(mornitor='loss', patience=1)])\n",
    "    # Get the accuracy for this training set fraction\n",
    "    train_acc = model.evaluate(X_train_frac, y_train_frac, verbose=0)[1]\n",
    "    train_accs.append(train_acc)\n",
    "    # Get the accuracy on the whole test set\n",
    "    test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    test_accs.append(test_acc)\n",
    "    print(\"Done with size: \", train_size)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Learning the digits\n",
    "\n",
    "You're going to build a model on the &**digits dataset**, a sample dataset that comes pre-loaded with scikit learn. The **digits dataset** consist of **8x8 pixel handwritten digits from 0 to 9**:\n",
    "\n",
    "<img src=\"image/digits_dataset_sample.png\">\n",
    "\n",
    "You want to distinguish between each of the 10 possible digits given an image, so we are dealing with multi-class classification.\n",
    "The dataset has already been partitioned into `X_train`, `y_train`, `X_test`, and `y_test`, using 30% of the data as testing data. The labels are already one-hot encoded vectors, so you don't need to use Keras `to_categorical()` function.\n",
    "\n",
    "Let's build this new `model`!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Add a `Dense` layer of 16 neurons with `relu` activation and an `input_shape` that takes the total number of pixels of the 8x8 digit image.\n",
    "- Add a `Dense` layer with 10 outputs and `softmax` activation.\n",
    "- Compile your model with `adam`, `categorical_crossentropy`, and `accuracy` metrics.\n",
    "- Make sure your model works by predicting on `X_train`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
    "model.add(Dense(16, input_shape = (64,), activation = 'relu'))\n",
    "\n",
    "# Output layer with 10 neurons (one per digit) and softmax\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Test if your model is well assembled by predicting before training\n",
    "print(model.predict(X_train))"
   ]
  },
  {
   "source": [
    "# Exercise II: Is the model overfitting?\n",
    "\n",
    "Let's train the `model` you just built and plot its learning curve to check out if it's overfitting! You can make use of the loaded function `plot_loss()` to plot training loss against validation loss, you can get both from the history callback.\n",
    "\n",
    "If you want to inspect the `plot_loss()` function code, paste this in the console: `show_code(plot_loss)`\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Train your model for 60 `epochs`, using `X_test` and `y_test` as validation data.\n",
    "- Use `plot_loss()` passing `loss` and `val_loss` as extracted from the history attribute of the `h_callback` object."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
    "h_callback = model.fit(X_train, y_train, epochs = 60, validation_data = (X_test, y_test), verbose=0)\n",
    "\n",
    "# Extract from the h_callback object loss and val_loss to plot the learning curve\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question\n",
    "\n",
    "Just by looking at the picture, do you think the learning curve shows this model is overfitting after having trained for 60 epochs?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "- Yes, it started to overfit since the test loss is higher than the training loss.\n",
    "\n",
    "- No, the test loss is not getting higher as the epochs go by. (T)"
   ]
  },
  {
   "source": [
    "# Exercise III: Do we need more data?\n",
    "\n",
    "It's time to check whether the **digits dataset** `model` you built benefits from more training examples!\n",
    "\n",
    "In order to keep code to a minimum, various things are already initialized and ready to use:\n",
    "\n",
    "    - The `model` you just built.\n",
    "`X_train`, `y_train`, `X_test`, and `y_test`.\n",
    "    - The `initial_weights` of your model, saved after using `model.get_weights()`.\n",
    "    - A pre-defined list of training sizes: `training_sizes`.\n",
    "    - A pre-defined early stopping callback monitoring loss: `early_stop`.\n",
    "    - Two empty lists to store the evaluation results: `train_accs` and `test_accs`.\n",
    "Train your model on the different training sizes and evaluate the results on `X_test`. End by plotting the results with `plot_results()`.\n",
    "\n",
    "The full code for this exercise can be found on the slides!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Get a fraction of the training data determined by the `size` we are currently evaluating in the loop.\n",
    "- Set the model weights to the `initial_weights` with `set_weights()` and train your model on the fraction of training data using `early_stop` as a callback.\n",
    "- Evaluate and store the accuracy for the training fraction and the test set.\n",
    "- Call `plot_results()` passing in the training and test accuracies for each training size."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in training_sizes:\n",
    "  \t# Get a fraction of training data (we only care about the training data)\n",
    "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
    "\n",
    "    # Reset the model to the initial weights and train it on the new training data fraction\n",
    "    model.set_weights(initial_weights)\n",
    "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
    "\n",
    "    # Evaluate and store both: the training data fraction and the complete test set results\n",
    "    train_accs.append(model.evaluate(X_train, y_train)[1])\n",
    "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
    "    \n",
    "# Plot train vs test accuracies\n",
    "plot_results(train_accs, test_accs)"
   ]
  },
  {
   "source": [
    "# (2) Activation functions\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-29 153515.png\">\n",
    "\n",
    "## Sigmoid & Tanh function\n",
    "<img src=\"image/Screenshot 2021-01-29 154557.png\">\n",
    "\n",
    "## RelU & Leaky ReLU\n",
    "<img src=\"image/Screenshot 2021-01-29 154805.png\">\n",
    "\n",
    "## Effects of activation functions\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-29 154916.png\">\n",
    "\n",
    "## Effects of Sigmoid & Tanh\n",
    "<img src=\"image/Screenshot 2021-01-29 154938.png\">\n",
    "\n",
    "## Effects of ReLU & Leaky ReLU\n",
    "<img src=\"image/Screenshot 2021-01-29 155020.png\">\n",
    "\n",
    "## Which activation function to use?\n",
    "- No magic formula\n",
    "- Different properties\n",
    "- Depends on our problem\n",
    "- Goal to archieve in a given layer\n",
    "- ReLU are a goof first choice\n",
    "- Sigmoid not recommended for deep models\n",
    "\n",
    "## Comparing activation functions\n",
    "\n",
    "```\n",
    "# Set a random seed\n",
    "np.random.seed(1)\n",
    "# Return a new model with given activation\n",
    "def get_model(act_function):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_shape=(2,), activation=act_function))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "```\n",
    "\n",
    "## Comparing activation functions\n",
    "\n",
    "```\n",
    "# Activation functions to try out\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Dictionary to store results\n",
    "activation_results = {}\n",
    "for funct in activations:\n",
    "    model = model.get_model(act_function=funct)\n",
    "    history = model.fit(X_train, y_train, validation=(X_test, y_test), epoch=100, verbose=0)\n",
    "    activation_result[funct] = history\n",
    "```\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "# Extract val_loss history of each activation function\n",
    "val_loss_per_funct = {k:v.history['val_loss'] fot k,v in activation_results.itemห()}\n",
    "\n",
    "# Turn the dictionary into a pandas dataframe\n",
    "val_loss_curves = pd.DataFrame(val_loss_per_funct)\n",
    "\n",
    "# Plot the curves\n",
    "val_loss_curves.plot(title='Loss per Activation function')\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise IV: Different activation functions\n",
    "\n",
    "The `sigmoid()`, `tanh()`, `ReLU()`, and `leaky_ReLU()` functions have been defined and ready for you to use. Each function receives an input number X and returns its corresponding Y value.\n",
    "\n",
    "Which of the statements below is **false**?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "- The `sigmoid()` takes a value of 0.5 when X = 0 whilst `tanh()` takes a value of 0.\n",
    "\n",
    "- The `leaky_ReLU()` takes a value of -0.01 when X = -1 whilst `ReLU()` takes a value of 0.\n",
    "\n",
    "- The `sigmoid()` and `tanh()` both take values close to -1 for big negative numbers. (T)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise V: Comparing activation functions\n",
    "\n",
    "Comparing activation functions involves a bit of coding, but nothing you can't do!\n",
    "\n",
    "You will try out different activation functions on the multi-label model you built for your farm irrigation machine in chapter 2. The function `get_model('relu')` returns a copy of this model and applies the `'relu'` activation function to its hidden layer.\n",
    "\n",
    "You will loop through several activation functions, generate a new model for each and train it. By storing the history callback in a dictionary you will be able to visualize which activation function performed best in the next exercise!\n",
    "\n",
    "`X_train`, `y_train`, `X_test`, `y_test` are ready for you to use when training your models.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Fill up the activation functions array with `relu`, `leaky_relu`, `sigmoid`, and `tanh`.\n",
    "- Get a new model for each iteration with `get_model()` passing the current activation function as a parameter.\n",
    "- Fit your model providing the train and `validation_data`, use 20 `epochs` and set verbose to 0.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model and store the history results\n",
    "  h_callback = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=0)\n",
    "  activation_results[act] = h_callback"
   ]
  },
  {
   "source": [
    "# Exercise VI: Comparing activation functions II\n",
    "\n",
    "What you coded in the previous exercise has been executed to obtain the `activation_results` variable, this time 100 epochs were used instead of 20. This way you will have more epochs to further compare how the training evolves per activation function.\n",
    "\n",
    "For every `h_callback` of each activation function in `activation_results`:\n",
    "\n",
    "    - The `h_callback.history['val_loss']` has been extracted.\n",
    "    - The `h_callback.history['val_acc']` has been extracted.\n",
    "\n",
    "Both are saved into two dictionaries: `val_loss_per_function` and `val_acc_per_function`.\n",
    "\n",
    "Pandas is also loaded as pd for you to use. Let's plot some quick validation loss and accuracy charts!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use `pd.DataFrame()` to create a new DataFrame from the `val_loss_per_function` dictionary.\n",
    "- Call `plot()` on the DataFrame.\n",
    "- Create another pandas DataFrame from `val_acc_per_function`.\n",
    "- Once again, plot the DataFrame."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from val_loss_per_function\n",
    "val_loss= pd.DataFrame(val_loss_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_loss.plot()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe from val_acc_per_function\n",
    "val_acc = pd.DataFrame(val_acc_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_acc.plot()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Valuation Loss per function \n",
    "<img src=\"image/2021-29-01 163353.svg\">\n",
    "\n",
    "## Valuation Accuracy per function\n",
    "<img src=\"image/2021-29-01 163523.svg\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (3) Batch size and batch normalization\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-29 164230.png\">\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-29 164321.png\">\n",
    "\n",
    "## Mini-batches\n",
    "\n",
    "**Advantages**\n",
    "- Networks train faster (more weight updates in same amount of time)\n",
    "- Less Ram memory required, can trian on huge datesets\n",
    "- Noise can help networks reach a lower error, escaping local minima\n",
    "** Disadvantages**\n",
    "- More iterations need to be run\n",
    "- Need to be adjusted, we need to find a good batch size\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-29 164644.png\">\n",
    "\n",
    "## Batch size in Keras\n",
    "\n",
    "$Standardization = \\frac{data - mean}{standard deviation} $\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-29 164846.png\">\n",
    "<img src=\"image/Screenshot 2021-01-29 164903.png\">\n",
    "\n",
    "## Batch normalization advantages\n",
    "- Improves gradient flow\n",
    "- Allow higher learning rates\n",
    "- Reduces dependence on weight initializations\n",
    "- Acts as an unintended form of regularization\n",
    "- Limits internal covariate shift\n",
    "\n",
    "## Batch normalization in Keras\n",
    "\n",
    "```\n",
    "# Import BatchNormalization from keras layers\n",
    "from keras.layers import BatchNormalization\n",
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "# Add an input layer\n",
    "model.add(Dense(3, input_shape=(2,), activation='relu'))\n",
    "# Add batch normalization for the outputs of the layer above\n",
    "model.add(BatchNormalization())\n",
    "# Add an output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise VII: Changing batch sizes\n",
    "\n",
    "You've seen models are usually trained in batches of a fixed size. The smaller a batch size, the more weight updates per epoch, but at a cost of a more unstable gradient descent. Specially if the batch size is too small and it's not representative of the entire training set.\n",
    "\n",
    "Let's see how different batch sizes affect the accuracy of a simple binary classification model that separates red from blue dots.\n",
    "\n",
    "You'll use a batch size of one, updating the weights once per sample in your training set for each epoch. Then you will use the entire dataset, updating the weights only once per epoch.\n",
    "\n",
    "### Instructions 1/2\n",
    "\n",
    "- Use `get_model()` to get a new, already compiled, model, then train your model for 5 `epochs` with a `batch_size` of 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh new model with get_model\n",
    "model = get_model()\n",
    "\n",
    "# Train your model for 5 epochs with a batch size of 1\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=1)\n",
    "print(\"\\n The accuracy when using a batch of size 1 is: \", model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "source": [
    "### Instructions 2/2\n",
    "\n",
    "- Now train a new model with `batch_size` equal to the size of the training set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "# Fit your model for 5 epochs with a batch of size the training set\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=700)\n",
    "print(\"\\n The accuracy when using the whole training set as batch-size was: \"m, model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "source": [
    "# Exercise VIII: Batch normalizing a familiar model\n",
    "\n",
    "Remember the **digits dataset** you trained in the first exercise of this chapter?\n",
    "\n",
    "<img src=\"image/digits_dataset_sample.png\">\n",
    "\n",
    "A multi-class classification problem that you solved using `softmax` and 10 neurons in your output layer.\n",
    "You will now build a new deeper model consisting of 3 hidden layers of 50 neurons each, using batch normalization in between layers. The `kernel_initializer` parameter is used to initialize weights in a similar way.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import `BatchNormalization` from keras layers.\n",
    "- Build your deep network model, use **50 neurons for each hidden layer** adding batch normalization in between layers.\n",
    "- Compile your model with stochastic gradient descent, `sgd`, as an optimizer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import batch normalization from keras layers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Build your deep network\n",
    "batchnorm_model = Sequential()\n",
    "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "# Compile your model with sgd\n",
    "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "# Exercise IX: Batch normalization effects\n",
    "\n",
    "Batch normalization tends to increase the learning speed of our models and make their learning curves more stable. Let's see how two identical models with and without batch normalization compare.\n",
    "\n",
    "The model you just built `batchnorm_model` is loaded for you to use. An exact copy of it without batch normalization: `standard_model`, is available as well. You can check their `summary()` in the console. `X_train`, `y_train`, `X_test`, and `y_test` are also loaded so that you can train both models.\n",
    "\n",
    "You will compare the accuracy learning curves for both models plotting them with `compare_histories_acc()`.\n",
    "\n",
    "You can check the function pasting `show_code(compare_histories_acc)` in the console.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Train the `standard_model` for 10 epochs passing in train and `validation data`, storing its history in `h1_callback`.\n",
    "- Train your `batchnorm_model` for 10 epochs passing in train and `validation data`, storing its history in `h2_callback`.\n",
    "- Call `compare_histories_acc` passing in `h1_callback` and `h2_callback`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your standard model, storing its history callback\n",
    "h1_callback = standard_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Train the batch normalized model you recently built, store its history callback\n",
    "h2_callback = batchnorm_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Call compare_histories_acc passing in both model histories\n",
    "compare_histories_acc(h1_callback, h2_callback)"
   ]
  },
  {
   "source": [
    "## Batch Normalization Effects\n",
    "\n",
    "<img src=\"image/2021-29-01 175735.svg\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (4) Hyperparameter tuning\n",
    "\n",
    "## Neural network hyperparameters\n",
    "- Number of layers\n",
    "- Number of neurons per layer\n",
    "- Layer order\n",
    "- Layer activations\n",
    "- Batch sizes\n",
    "- Learning rates\n",
    "- Optimizers\n",
    "- ...\n",
    "\n",
    "## Sklearn recap\n",
    "```\n",
    "# Import RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Instantiate your classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "# Define a series of parameters to look over\n",
    "params = {'max_depth':[3,None], \"max_features\":range(1,4), 'min_samples_leaf':range(1,4)}\n",
    "# Perform random search with cross validation\n",
    "tree_cv = RandomizedSearchCV(tree, params, cv=5)\n",
    "\n",
    "# Print the best parameters\n",
    "print(tree_cv.best_parmas_)\n",
    "```\n",
    "\n",
    "## Turn a Keras model into a Sklearn estimator\n",
    "\n",
    "```\n",
    "# Function that creates our Keras model\n",
    "def create_model(optimizer='adam', activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(2,), activation=activation))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "# Imprt sklearn wrapper from keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a model as a sklearn estimator\n",
    "model = KerasClassifier(build_fn=create_model, epochs=6, batch_size=16)\n",
    "```\n",
    "\n",
    "## Cross-validation\n",
    "```\n",
    "# Import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Check how your keras model performs with 5 fold crossvalidation\n",
    "kfold = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "# Print the mean accuracy per fold\n",
    "kfold.mean()\n",
    "```\n",
    "\n",
    "```\n",
    "# Print the standard deviation per fold\n",
    "kflod.std()\n",
    "```\n",
    "\n",
    "## Tips for neural networks hyperparameter tuning\n",
    "- Random search is preferred over grid search\n",
    "- Don't use many epochs\n",
    "- Use a smaller sample of your dataset\n",
    "- Play with batch size, activations, optimizers and learning rates\n",
    "\n",
    "## Random search on Keras model\n",
    "\n",
    "```\n",
    "# Define a series of parameters\n",
    "params = dict(optimizer=['sgd', 'adam'], epochs=3, batch_size=[5, 10, 20], activation=['relu', 'tanh'])\n",
    "\n",
    "# Create a random search cv object and fit it to the data\n",
    "random_search = RandomizedSearchCV(model, param_dist=params, cv=3)\n",
    "random_search_results = rnadom_search.fit(X, y)\n",
    "# Print results\n",
    "print(\"Best: %f using %s\".format(random_search_results.best_score_, random_search_results.best_parmas_))\n",
    "```\n",
    "\n",
    "## Tuning other hyperparameters\n",
    "```\n",
    "def create_mode(nl=1, nn=256):\n",
    "    model.Sequential()\n",
    "    model.add(Dense(16, input_shape(2,), activation='relu'))\n",
    "    # Add as many hidden layers as specified in nl\n",
    "    for i in range(nl):\n",
    "        model.add(Dense(nn, activation='relu'))\n",
    "    # End defining and compiling your model...\n",
    "```\n",
    "\n",
    "```\n",
    "# Define parameters, named just like in create_model()\n",
    "params = dict(nl=[1, 2, 9], nn=[128,256, 1000])\n",
    "\n",
    "# Repeat the random search...\n",
    "\n",
    "# Print results...\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise X: Preparing a model for tuning\n",
    "\n",
    "Let's tune the hyperparameters of a **binary classification** model that does well classifying the **breast cancer dataset**.\n",
    "\n",
    "You've seen that the first step to turn a model into a sklearn estimator is to build a function that creates it. The definition of this function is important since hyperparameter tuning is carried out by varying the arguments your function receives.\n",
    "\n",
    "Build a simple `create_model()` function that receives both a learning rate and an activation function as arguments. The `Adam` optimizer has been imported as an object from `keras.optimizers` so that you can also change its learning rate parameter.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Set the learning rate of the `Adam` optimizer object to the one passed in the arguments.\n",
    "- Set the hidden layers activations to the one passed in the arguments.\n",
    "- Pass the optimizer and the binary cross-entropy loss to the `.compile()` method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate, activation):\n",
    "  \n",
    "  \t# Create an Adam optimizer with the given learning rate\n",
    "  \topt = Adam(lr = learning_rate)\n",
    "  \t\n",
    "  \t# Create your binary classification model  \n",
    "  \tmodel = Sequential()\n",
    "  \tmodel.add(Dense(128, input_shape = (30,), activation = activation))\n",
    "  \tmodel.add(Dense(256, activation = activation))\n",
    "  \tmodel.add(Dense(1, activation = 'sigmoid'))\n",
    "  \t\n",
    "  \t# Compile your model with your optimizer, loss, and metrics\n",
    "  \tmodel.compile(optimizer = opt, loss = crossentropy, metrics = ['accuracy'])\n",
    "  \treturn model"
   ]
  },
  {
   "source": [
    "# Exercise XI: Tuning the model parameters\n",
    "\n",
    "It's time to try out different parameters on your model and see how well it performs!\n",
    "\n",
    "The `create_model()` function you built in the previous exercise is ready for you to use.\n",
    "\n",
    "Since fitting the `RandomizedSearchCV` object would take too long, the results you'd get are printed in the `show_results()` function. You could try `random_search.fit(X,y)` in the console yourself to check it does work after you have built everything else, but you will probably timeout the exercise (so copy your code first if you try this or you can lose your progress!).\n",
    "\n",
    "You don't need to use the optional `epochs` and `batch_size` parameters when building your `KerasClassifier` object since you are passing them as `params` to the random search and this works already.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import `KerasClassifier` from keras `scikit_learn` wrappers.\n",
    "- Use your `create_model` function when instantiating your `KerasClassifier`.\n",
    "- Set `'relu'` and `'tanh'` as `activation`, 32, 128, and 256 as `batch_size`, 50, 100, and 200 `epochs`, and `learning_rate` of 0.1, 0.01, and 0.001.\n",
    "- Pass your converted `model` and the chosen `params` as you build your `RandomizedSearchCV` object.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from keras scikit learn wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "# Define the parameters to try out\n",
    "params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], 'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "\n",
    "# Create a randomize search cv object passing in the parameters to try\n",
    "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
    "\n",
    "# Running random_search.fit(X,y) would start the search,but it takes too long! \n",
    "show_results()"
   ]
  },
  {
   "source": [
    "# Exercise XII: Training with cross-validation\n",
    "\n",
    "Time to train your model with the best parameters found: **0.001** for the **learning rate, 50 epochs, a 128 batch_size** and **relu activations**.\n",
    "\n",
    "The `create_model()` function from the previous exercise is ready for you to use. `X` and `y` are loaded as features and labels.\n",
    "\n",
    "Use the best values found for your model when creating your `KerasClassifier` object so that they are used when performing cross_validation.\n",
    "\n",
    "End this chapter by training an awesome tuned model on the **breast cancer dataset**!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import `KerasClassifier` from keras `scikit_learn` wrappers.\n",
    "- Create a `KerasClassifier` object providing the best parameters found.\n",
    "- Pass your `model`, features and labels to `cross_val_score` to perform cross-validation with 3 folds."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from keras wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model(learning_rate = 0.001, activation = 'relu'), epochs = 50, \n",
    "             batch_size = 128, verbose = 0)\n",
    "\n",
    "# Calculate the accuracy score for each fold\n",
    "kfolds = cross_val_score(model, X, y, cv = 3)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print('The mean accuracy was:', kfolds.mean())\n",
    "\n",
    "# Print the accuracy standard deviation\n",
    "print('With a standard deviation of:', kfolds.std())"
   ]
  }
 ]
}