{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Improving Your Model Performance\n",
    "\n",
    "In the previous chapters, you've trained a lot of models! You will now learn how to interpret learning curves to understand your models as they train. You will also visualize the effects of activation functions, batch-sizes, and batch-normalization. Finally, you will learn how to perform automatic hyperparameter optimization to your Keras models using sklearn."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Learning curves\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-29 142344.png\">\n",
    "<img src=\"image/Screenshot 2021-01-29 142401.png\">\n",
    "<img src=\"image/Screenshot 2021-01-29 142416.png\">\n",
    "<img src=\"image/Screenshot 2021-01-29 142445.png\">\n",
    "<img src=\"image/Screenshot 2021-01-29 142506.png\">\n",
    "<img src=\"image/Screenshot 2021-01-29 142536.png\">\n",
    "\n",
    "## Learning curves\n",
    "\n",
    "```\n",
    "# Store initial model weights\n",
    "init_weights = model.get_weights()\n",
    "# Lists for storing accuracies\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "```\n",
    "\n",
    "```\n",
    "for train_size in train_sizes:\n",
    "    # Split a fraction according to train_size\n",
    "    X_train_frac, _, y_train_frac, _ = train_test_split(X_train, Y_train, Train_size=train_size)\n",
    "    # Set model initial weigths\n",
    "    model.set_weights(init_weights)\n",
    "    # Fit model on the training set fractopm\n",
    "    model.fit(X_train_frac, y_train_frac, epoch=100, verbose=0, callbacks=[EarlyStopping(mornitor='loss', patience=1)])\n",
    "    # Get the accuracy for this training set fraction\n",
    "    train_acc = model.evaluate(X_train_frac, y_train_frac, verbose=0)[1]\n",
    "    train_accs.append(train_acc)\n",
    "    # Get the accuracy on the whole test set\n",
    "    test_acc = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    test_accs.append(test_acc)\n",
    "    print(\"Done with size: \", train_size)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Learning the digits\n",
    "\n",
    "You're going to build a model on the &**digits dataset**, a sample dataset that comes pre-loaded with scikit learn. The **digits dataset** consist of **8x8 pixel handwritten digits from 0 to 9**:\n",
    "\n",
    "<img src=\"image/digits_dataset_sample.png\">\n",
    "\n",
    "You want to distinguish between each of the 10 possible digits given an image, so we are dealing with multi-class classification.\n",
    "The dataset has already been partitioned into `X_train`, `y_train`, `X_test`, and `y_test`, using 30% of the data as testing data. The labels are already one-hot encoded vectors, so you don't need to use Keras `to_categorical()` function.\n",
    "\n",
    "Let's build this new `model`!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Add a `Dense` layer of 16 neurons with `relu` activation and an `input_shape` that takes the total number of pixels of the 8x8 digit image.\n",
    "- Add a `Dense` layer with 10 outputs and `softmax` activation.\n",
    "- Compile your model with `adam`, `categorical_crossentropy`, and `accuracy` metrics.\n",
    "- Make sure your model works by predicting on `X_train`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
    "model.add(Dense(16, input_shape = (64,), activation = 'relu'))\n",
    "\n",
    "# Output layer with 10 neurons (one per digit) and softmax\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Test if your model is well assembled by predicting before training\n",
    "print(model.predict(X_train))"
   ]
  },
  {
   "source": [
    "# Exercise II: Is the model overfitting?\n",
    "\n",
    "Let's train the `model` you just built and plot its learning curve to check out if it's overfitting! You can make use of the loaded function `plot_loss()` to plot training loss against validation loss, you can get both from the history callback.\n",
    "\n",
    "If you want to inspect the `plot_loss()` function code, paste this in the console: `show_code(plot_loss)`\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Train your model for 60 `epochs`, using `X_test` and `y_test` as validation data.\n",
    "- Use `plot_loss()` passing `loss` and `val_loss` as extracted from the history attribute of the `h_callback` object."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
    "h_callback = model.fit(X_train, y_train, epochs = 60, validation_data = (X_test, y_test), verbose=0)\n",
    "\n",
    "# Extract from the h_callback object loss and val_loss to plot the learning curve\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question\n",
    "\n",
    "Just by looking at the picture, do you think the learning curve shows this model is overfitting after having trained for 60 epochs?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "- Yes, it started to overfit since the test loss is higher than the training loss.\n",
    "\n",
    "- No, the test loss is not getting higher as the epochs go by. (T)"
   ]
  },
  {
   "source": [
    "# Exercise III: Do we need more data?\n",
    "\n",
    "It's time to check whether the **digits dataset** `model` you built benefits from more training examples!\n",
    "\n",
    "In order to keep code to a minimum, various things are already initialized and ready to use:\n",
    "\n",
    "    - The `model` you just built.\n",
    "`X_train`, `y_train`, `X_test`, and `y_test`.\n",
    "    - The `initial_weights` of your model, saved after using `model.get_weights()`.\n",
    "    - A pre-defined list of training sizes: `training_sizes`.\n",
    "    - A pre-defined early stopping callback monitoring loss: `early_stop`.\n",
    "    - Two empty lists to store the evaluation results: `train_accs` and `test_accs`.\n",
    "Train your model on the different training sizes and evaluate the results on `X_test`. End by plotting the results with `plot_results()`.\n",
    "\n",
    "The full code for this exercise can be found on the slides!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Get a fraction of the training data determined by the `size` we are currently evaluating in the loop.\n",
    "- Set the model weights to the `initial_weights` with `set_weights()` and train your model on the fraction of training data using `early_stop` as a callback.\n",
    "- Evaluate and store the accuracy for the training fraction and the test set.\n",
    "- Call `plot_results()` passing in the training and test accuracies for each training size."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in training_sizes:\n",
    "  \t# Get a fraction of training data (we only care about the training data)\n",
    "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
    "\n",
    "    # Reset the model to the initial weights and train it on the new training data fraction\n",
    "    model.set_weights(initial_weights)\n",
    "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
    "\n",
    "    # Evaluate and store both: the training data fraction and the complete test set results\n",
    "    train_accs.append(model.evaluate(X_train, y_train)[1])\n",
    "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
    "    \n",
    "# Plot train vs test accuracies\n",
    "plot_results(train_accs, test_accs)"
   ]
  },
  {
   "source": [
    "# (2) Activation functions\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-29 153515.png\">\n",
    "\n",
    "## Sigmoid & Tanh function\n",
    "<img src=\"image/Screenshot 2021-01-29 154557.png\">\n",
    "\n",
    "## RelU & Leaky ReLU\n",
    "<img src=\"image/Screenshot 2021-01-29 154805.png\">\n",
    "\n",
    "## Effects of activation functions\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-29 154916.png\">\n",
    "\n",
    "## Effects of Sigmoid & Tanh\n",
    "<img src=\"image/Screenshot 2021-01-29 154938.png\">\n",
    "\n",
    "## Effects of ReLU & Leaky ReLU\n",
    "<img src=\"image/Screenshot 2021-01-29 155020.png\">\n",
    "\n",
    "## Which activation function to use?\n",
    "- No magic formula\n",
    "- Different properties\n",
    "- Depends on our problem\n",
    "- Goal to archieve in a given layer\n",
    "- ReLU are a goof first choice\n",
    "- Sigmoid not recommended for deep models\n",
    "\n",
    "## Comparing activation functions\n",
    "\n",
    "```\n",
    "# Set a random seed\n",
    "np.random.seed(1)\n",
    "# Return a new model with given activation\n",
    "def get_model(act_function):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_shape=(2,), activation=act_function))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "```\n",
    "\n",
    "## Comparing activation functions\n",
    "\n",
    "```\n",
    "# Activation functions to try out\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Dictionary to store results\n",
    "activation_results = {}\n",
    "for funct in activations:\n",
    "    model = model.get_model(act_function=funct)\n",
    "    history = model.fit(X_train, y_train, validation=(X_test, y_test), epoch=100, verbose=0)\n",
    "    activation_result[funct] = history\n",
    "```\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "# Extract val_loss history of each activation function\n",
    "val_loss_per_funct = {k:v.history['val_loss'] fot k,v in activation_results.itemห()}\n",
    "\n",
    "# Turn the dictionary into a pandas dataframe\n",
    "val_loss_curves = pd.DataFrame(val_loss_per_funct)\n",
    "\n",
    "# Plot the curves\n",
    "val_loss_curves.plot(title='Loss per Activation function')\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise IV: Different activation functions\n",
    "\n",
    "The `sigmoid()`, `tanh()`, `ReLU()`, and `leaky_ReLU()` functions have been defined and ready for you to use. Each function receives an input number X and returns its corresponding Y value.\n",
    "\n",
    "Which of the statements below is **false**?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "- The `sigmoid()` takes a value of 0.5 when X = 0 whilst `tanh()` takes a value of 0.\n",
    "\n",
    "- The `leaky_ReLU()` takes a value of -0.01 when X = -1 whilst `ReLU()` takes a value of 0.\n",
    "\n",
    "- The `sigmoid()` and `tanh()` both take values close to -1 for big negative numbers. (T)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise V: Comparing activation functions\n",
    "\n",
    "Comparing activation functions involves a bit of coding, but nothing you can't do!\n",
    "\n",
    "You will try out different activation functions on the multi-label model you built for your farm irrigation machine in chapter 2. The function `get_model('relu')` returns a copy of this model and applies the `'relu'` activation function to its hidden layer.\n",
    "\n",
    "You will loop through several activation functions, generate a new model for each and train it. By storing the history callback in a dictionary you will be able to visualize which activation function performed best in the next exercise!\n",
    "\n",
    "`X_train`, `y_train`, `X_test`, `y_test` are ready for you to use when training your models.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Fill up the activation functions array with `relu`, `leaky_relu`, `sigmoid`, and `tanh`.\n",
    "- Get a new model for each iteration with `get_model()` passing the current activation function as a parameter.\n",
    "- Fit your model providing the train and `validation_data`, use 20 `epochs` and set verbose to 0.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model and store the history results\n",
    "  h_callback = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=0)\n",
    "  activation_results[act] = h_callback"
   ]
  },
  {
   "source": [
    "# Exercise VI: Comparing activation functions II\n",
    "\n",
    "What you coded in the previous exercise has been executed to obtain the `activation_results` variable, this time 100 epochs were used instead of 20. This way you will have more epochs to further compare how the training evolves per activation function.\n",
    "\n",
    "For every `h_callback` of each activation function in `activation_results`:\n",
    "\n",
    "    - The `h_callback.history['val_loss']` has been extracted.\n",
    "    - The `h_callback.history['val_acc']` has been extracted.\n",
    "\n",
    "Both are saved into two dictionaries: `val_loss_per_function` and `val_acc_per_function`.\n",
    "\n",
    "Pandas is also loaded as pd for you to use. Let's plot some quick validation loss and accuracy charts!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use `pd.DataFrame()` to create a new DataFrame from the `val_loss_per_function` dictionary.\n",
    "- Call `plot()` on the DataFrame.\n",
    "- Create another pandas DataFrame from `val_acc_per_function`.\n",
    "- Once again, plot the DataFrame."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from val_loss_per_function\n",
    "val_loss= pd.DataFrame(val_loss_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_loss.plot()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe from val_acc_per_function\n",
    "val_acc = pd.DataFrame(val_acc_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_acc.plot()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Valuation Loss per function \n",
    "<img src=\"image/2021-29-01 163353.svg\">\n",
    "\n",
    "## Valuation Accuracy per function\n",
    "<img src=\"image/2021-29-01 163523.svg\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}