{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# RNN Architecture\n",
    "\n",
    "You will learn about the vanishing and exploding gradient problems, often occurring in RNNs, and how to deal with them with the GRU and LSTM cells. Furthermore, you'll create embedding layers for language models and revisit the sentiment classification task."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Vanishing and exploding gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training RNN models\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 151311.png'>\n",
    "    <img src='image/Screenshot 2021-02-11 151431.png'>\n",
    "</p>\n",
    "\n",
    "Example:\n",
    "$$a_2 = f(W_a , a_1 , x_2)$$ \n",
    "$$= f(W_a , f(W_a , a_0 , x_1), x_2)$$\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 152644.png'>\n",
    "</p>\n",
    "\n",
    "**Remember that**\n",
    "$$a_T = f(W_a , a_T-1 , x_T)$$\n",
    "$a_T$ also depends on $a_T-1$ which depends on $a_T-2$ and so on!\n",
    "\n",
    "## BPTT continuation\n",
    "**Computing derivatives leads to**\n",
    "$$\\frac{\\partial a_t}{\\partial W_a = (W_a)^{t-1} g(X)}$$\n",
    "\n",
    "- $(W_a)^{t-1}$ **can cpnverge to 0**\n",
    "- **or diverge to** $+\\infty$**!**\n",
    "\n",
    "## Solutions to the gradient problems\n",
    "Some solutions are known:\n",
    "\n",
    "### Exploding gradinets\n",
    "- Gradient clipping / scaling\n",
    "\n",
    "### Vanishing gradients\n",
    "- Better initialize the matrix W\n",
    "- Use regularization\n",
    "- Use ReLU instead of tanh / sigmoid / softmax\n",
    "- **Use LSTM or GRU cells!**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Exploding gradient problem\n",
    "\n",
    "In the video exercise, you learned about two problems that may arise when working with RNN models: the vanishing and exploding gradient problems.\n",
    "\n",
    "This exercise explores the exploding gradient problem, showing that the derivative of a function can increase exponentially, and how to solve it with a simple technique.\n",
    "\n",
    "The data is already loaded on the environment as `X_train`, `X_test`, `y_train` and `y_test`.\n",
    "\n",
    "You will use a **Stochastic Gradient Descent** (SGD) optimizer and **Mean Squared Error** (MSE) as the loss function.\n",
    "\n",
    "In the first step you will observe the gradient exploding by computing the MSE on the train and test sets. On step 2, you will change the optimizer using the `clipvalue` parameter to solve the problem.\n",
    "\n",
    "The Stochastic Gradient Descent in Keras is loaded as `SGD`.\n",
    "\n",
    "### Instructions 1/2\n",
    "\n",
    "- Use `SGD()` as optimizer and `(X_test, y_test)` as validation data.\n",
    "- Evaluate train performance and print all the **MSE** values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "source": [
    "### Instructions 2/2\n",
    "\n",
    "- Set the `SGD()` parameter `clipvalue` equal to `3.0`.\n",
    "- Compute the MSE values and store them on `train_mse` and `test_mse` variables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9, clipvalue=3.0))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "source": [
    "# Exercise II: Vanishing gradient problem\n",
    "\n",
    "The other possible gradient problem is when the gradients vanish, or go to zero. This is a much harder problem to solve because it is not as easy to detect. If the loss function does not improve on every step, is it because the gradients went to zero and thus didn't update the weights? Or is it because the model is not able to learn?\n",
    "\n",
    "This problem occurs more often in RNN models when long memory is required, meaning having long sentences.\n",
    "\n",
    "In this exercise you will observe the problem on the IMDB data, with longer sentences selected. The data is loaded in `X` and `y` variables, as well as classes `Sequential`, `SimpleRNN`, `Dense` and `matplotlib.pyplot` as `plt`. The model was pre-trained with 100 epochs and its weights are stored on the file `model_weights.h5`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Add a `SimpleRNN` layer to the model.\n",
    "- Load the pre-trained weights on the model using the method `.load_weights()`.\n",
    "- Add the accuracy of the training data available on the attribute `'acc'` to the plot.\n",
    "- Display the plot using the method `.show()`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=600, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Plot the accuracy x epoch graph\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "<p align='center'>\n",
    "    <img src='image/[2021-02-11 160543].svg'>\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (2) GRU and LSTM cells\n",
    "\n",
    "## SimpleRNN cell\n",
    "<p algin='center' >\n",
    "    <img src='image/Screenshot 2021-02-12 174743.png'>\n",
    "</p>\n",
    "\n",
    "## GRU cell\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-12 174900.png'>\n",
    "</p>\n",
    "\n",
    "## LSTM cell\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-12 175037.png'>\n",
    "</p>\n",
    "\n",
    "## No more vanishing gradients\n",
    "- The `simpleRNN` cell can have gradient problems.\n",
    "    - The weight matrix power t multiplies the other terms\n",
    "- `GRU` and `LSTM` cells don't have vanishing gradient problems\n",
    "    - Because of their gates\n",
    "    - Don't have the weight matrics terms multiplying the rest\n",
    "    - Exploding gradient problems are easier to solve\n",
    "\n",
    "## Usage in keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the layers\n",
    "from keras.layers import GRU, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the layers to a model\n",
    "model.add(GRU(units=128, return_sequences=True, name='GRU layer'))\n",
    "model.add(GRU(units=64, return_sequences=False, name='LSTM layer'))"
   ]
  },
  {
   "source": [
    "# Exercise III: GRU cells are better than simpleRNN\n",
    "\n",
    "In this exercise you will re-run the same model as the first chapter of the course to compare the accuracy of the model by simpling changing the `SimpleRNN` cell to a `GRU` cell.\n",
    "\n",
    "The model was already trained with 10 epochs, as in the previous model with a `SimpleRNN` cell. In order to compare the models, a test set `(x_test, y_test)` is already loaded in the environment, as well as the old model `SimpleRNN_model`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import the `GRU` cell.\n",
    "- Print the models' summaries.\n",
    "- Print the accuracy of each model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "from keras.layers import GRU, Dense\n",
    "\n",
    "# Print the old and new model summaries\n",
    "SimpleRNN_model.summary()\n",
    "gru_model.summary()\n",
    "\n",
    "# Evaluate the models' performance (ignore the loss value)\n",
    "_, acc_simpleRNN = SimpleRNN_model.evaluate(X_test, y_test, verbose=0)\n",
    "_, acc_GRU = gru_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"SimpleRNN model's accuracy:\\t{0}\".format(acc_simpleRNN))\n",
    "print(\"GRU model's accuracy:\\t{0}\".format(acc_GRU))"
   ]
  },
  {
   "source": [
    "# Exercise IV: Stacking RNN layers\n",
    "\n",
    "Deep RNN models can have tens to hundreds of layers in order to achieve state-of-the-art results.\n",
    "\n",
    "In this exercise, you will get a glimpse of how to create deep RNN models by stacking layers of LSTM cells one after the other.\n",
    "\n",
    "To do this, you will set the `return_sequences` argument to `True` on the firsts two `LSTM` layers and to `False` on the last `LSTM` layer.\n",
    "\n",
    "To create models with even more layers, you can keep adding them one after the other or create a function that uses the `.add()` method inside a loop to add many layers with few lines of code.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import the `LSTM` layer.\n",
    "- Return the sequences in the first two layers and don't return the sequences in the last `LSTM` layer.\n",
    "- Load the pre-trained weights.\n",
    "- Print the loss and accuracy obtained."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LSTM layer\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(None, 1), return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('lstm_stack_model_weights.h5')\n",
    "\n",
    "print(\"Loss: %0.04f\\nAccuracy: %0.04f\" % tuple(model.evaluate(X_test, y_test, verbose=0)))"
   ]
  },
  {
   "source": [
    "# (3) The Embedding layer\n",
    "\n",
    "## Why embeddings\n",
    "Advantages:\n",
    "- Reduce the dimension\n",
    "```\n",
    "    one_hot = np.array((N, 100000))\n",
    "    embedd = np.array((N, 300))\n",
    "```\n",
    "- Dense representation\n",
    "    - `king - man %20 woman = queen`\n",
    "- Transfer learning\n",
    "Disadvantages:\n",
    "- Lots of parameters to train: training takes longer\n",
    "\n",
    "## How to use in keras\n",
    "In keras:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "model = Sequential()\n",
    "\n",
    "# Use as the first layer\n",
    "model.add(Embedding(input_dim=100000,\n",
    "                    output_dim=300,\n",
    "                    trainable=True,\n",
    "                    embeddings_initializer=None,\n",
    "                    input_length=120))\n"
   ]
  },
  {
   "source": [
    "## Transger learning\n",
    "Transfer learning for language models\n",
    "\n",
    "- GloVe\n",
    "- word2vec\n",
    "- BERT\n",
    "\n",
    "In keras:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from keras.initializers import Constant\n",
    "model.add(Embedding(input_dim=vocabulary_size,\n",
    "                    output_dim=embedding_dim,\n",
    "                    embeddings_initializer=Constant(pre_trained_vectors)))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Using GloVE pre-trained vectors\n",
    "Official site: https://nlp.stanford.edu/projects/glove"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GloVE vectors\n",
    "def get_glove_vectors(filename=\"glove.6B.300d.txt\"):\n",
    "    # Get all word vectors from pre-trained model\n",
    "    glove_vector_dict = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = value[0]\n",
    "            coefs = values[1:]\n",
    "            glove_vector_dict[word] = np.asarray(coefs, dtype='float32')"
   ]
  },
  {
   "source": [
    "## Using the GloVE on a specific task"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter GloVE vectors to specific task\n",
    "def filter_glove(vocabulary_dict, glove,_dict, wordvec_dim=300):\n",
    "    # Create a matrix to store the vectors\n",
    "    embedding_matrix = np.zeros((len(vocabulary_dict) + 1, wordvec_dim))\n",
    "    for worrd, i in vocabulary_dict.items():\n",
    "        embedding_vector = glove_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in glove_dict will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "source": [
    "# Exercise V: Number of parameters comparison\n",
    "\n",
    "You saw that the one-hot representation is not a good representation of words because it is very sparse. Using the `Embedding` layer creates a dense representation of the vectors, but also demands a lot of parameters to be learned.\n",
    "\n",
    "In this exercise you will compare the number of parameters of two models using `embeddings` and `one-hot` encoding to see the difference.\n",
    "\n",
    "The model `model_onehot` is already loaded in the environment, as well as the `Sequential`, `Dense` and `GRU` from `keras.` Finally, the parameters `vocabulary_size=80000` and `sentence_len=200` are also loaded.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import the `Embedding` layer from `keras.layers`.\n",
    "- On the embedding layer, use vocabulary size plus one as input dimension and sentence size as input length.\n",
    "- Compile the model.\n",
    "- Print the summary of the model with embedding."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the embedding layer\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=vocabulary_size+1, output_dim=wordvec_dim, input_length=sentence_len, trainable=True))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summaries of the one-hot model\n",
    "model_onehot.summary()\n",
    "\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "# Exercise VI: Transfer learning\n",
    "\n",
    "You saw that when training an embedding layer, you need to learn a lot of parameters.\n",
    "\n",
    "In this exercise, you will see that when using transfer learning it is possible to use the pre-trained weights and don't update them, meaning that all the parameters of the embedding layer will be fixed, and the model will only need to learn the parameters from the other layers.\n",
    "\n",
    "The function `load_glove` is already loaded on the environment and retrieves the glove matrix as a `numpy.ndarray` vector. It uses the function covered on the lesson's slides to retrieve the glove vectors with 200 embedding dimensions for the vocabulary present in this exercise.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use the pre-defined function to load the glove vectors.\n",
    "- Use the initializer `Constant` on the pre-trained vectors.\n",
    "- Add the output layer as a `Dense` with one unit.\n",
    "- Print the summary and check the trainable parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the glove pre-trained vectors\n",
    "glove_matrix = load_glove('glove_200d.zip')\n",
    "\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=wordvec_dim, \n",
    "                    embeddings_initializer=Constant(glove_matrix), \n",
    "                    input_length=sentence_len, trainable=False))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "# Exercise VII: Embeddings improves performance\n",
    "\n",
    "Does the embedding layer improves the accuracy of the model? Let's check it out in the same IMDB data.\n",
    "\n",
    "The model was already trained with 10 epochs, as in the previous model with `simpleRNN` cell. In order to compare the models, a test set `(X_test, y_test)` is available in the environment, as well as the old model `simpleRNN_model`. The old model's accuracy is loaded in the variable `acc_SimpleRNN`.\n",
    "\n",
    "All required modules and functions as loaded in the environment: `Sequential()` from `keras.models`, `Embedding` and `Dense` from `keras.layers` and `SimpleRNN` from `keras.layers.recurrent`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Add the embedding layer to the model.\n",
    "- Compute the model's accuracy and store on the variable `acc_embeddings`.\n",
    "- Print the accuracy of the old and new models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with embedding\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=max_vocabulary, output_dim=wordvec_dim, input_length=max_len))\n",
    "model.add(SimpleRNN(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('embedding_model_weights.h5')\n",
    "\n",
    "# Evaluate the models' performance (ignore the loss value)\n",
    "_, acc_embeddings = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"SimpleRNN model's accuracy:\\t{0}\\nEmbeddings model's accuracy:\\t{1}\".format(acc_simpleRNN, acc_embeddings))"
   ]
  }
 ]
}