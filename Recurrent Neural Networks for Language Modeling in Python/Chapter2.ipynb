{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# RNN Architecture\n",
    "\n",
    "You will learn about the vanishing and exploding gradient problems, often occurring in RNNs, and how to deal with them with the GRU and LSTM cells. Furthermore, you'll create embedding layers for language models and revisit the sentiment classification task."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Vanishing and exploding gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training RNN models\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 151311.png'>\n",
    "    <img src='image/Screenshot 2021-02-11 151431.png'>\n",
    "</p>\n",
    "\n",
    "Example:\n",
    "$$a_2 = f(W_a , a_1 , x_2)$$ \n",
    "$$= f(W_a , f(W_a , a_0 , x_1), x_2)$$\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 152644.png'>\n",
    "</p>\n",
    "\n",
    "**Remember that**\n",
    "$$a_T = f(W_a , a_T-1 , x_T)$$\n",
    "$a_T$ also depends on $a_T-1$ which depends on $a_T-2$ and so on!\n",
    "\n",
    "## BPTT continuation\n",
    "**Computing derivatives leads to**\n",
    "$$\\frac{\\partial a_t}{\\partial W_a = (W_a)^{t-1} g(X)}$$\n",
    "\n",
    "- $(W_a)^{t-1}$ **can cpnverge to 0**\n",
    "- **or diverge to** $+\\infty$**!**\n",
    "\n",
    "## Solutions to the gradient problems\n",
    "Some solutions are known:\n",
    "\n",
    "### Exploding gradinets\n",
    "- Gradient clipping / scaling\n",
    "\n",
    "### Vanishing gradients\n",
    "- Better initialize the matrix W\n",
    "- Use regularization\n",
    "- Use ReLU instead of tanh / sigmoid / softmax\n",
    "- **Use LSTM or GRU cells!**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Exploding gradient problem\n",
    "\n",
    "In the video exercise, you learned about two problems that may arise when working with RNN models: the vanishing and exploding gradient problems.\n",
    "\n",
    "This exercise explores the exploding gradient problem, showing that the derivative of a function can increase exponentially, and how to solve it with a simple technique.\n",
    "\n",
    "The data is already loaded on the environment as `X_train`, `X_test`, `y_train` and `y_test`.\n",
    "\n",
    "You will use a **Stochastic Gradient Descent** (SGD) optimizer and **Mean Squared Error** (MSE) as the loss function.\n",
    "\n",
    "In the first step you will observe the gradient exploding by computing the MSE on the train and test sets. On step 2, you will change the optimizer using the `clipvalue` parameter to solve the problem.\n",
    "\n",
    "The Stochastic Gradient Descent in Keras is loaded as `SGD`.\n",
    "\n",
    "### Instructions 1/2\n",
    "\n",
    "- Use `SGD()` as optimizer and `(X_test, y_test)` as validation data.\n",
    "- Evaluate train performance and print all the **MSE** values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "source": [
    "### Instructions 2/2\n",
    "\n",
    "- Set the `SGD()` parameter `clipvalue` equal to `3.0`.\n",
    "- Compute the MSE values and store them on `train_mse` and `test_mse` variables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9, clipvalue=3.0))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "source": [
    "# Exercise II: Vanishing gradient problem\n",
    "\n",
    "The other possible gradient problem is when the gradients vanish, or go to zero. This is a much harder problem to solve because it is not as easy to detect. If the loss function does not improve on every step, is it because the gradients went to zero and thus didn't update the weights? Or is it because the model is not able to learn?\n",
    "\n",
    "This problem occurs more often in RNN models when long memory is required, meaning having long sentences.\n",
    "\n",
    "In this exercise you will observe the problem on the IMDB data, with longer sentences selected. The data is loaded in `X` and `y` variables, as well as classes `Sequential`, `SimpleRNN`, `Dense` and `matplotlib.pyplot` as `plt`. The model was pre-trained with 100 epochs and its weights are stored on the file `model_weights.h5`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Add a `SimpleRNN` layer to the model.\n",
    "- Load the pre-trained weights on the model using the method `.load_weights()`.\n",
    "- Add the accuracy of the training data available on the attribute `'acc'` to the plot.\n",
    "- Display the plot using the method `.show()`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=600, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Plot the accuracy x epoch graph\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "<p align='center'>\n",
    "    <img src='image/[2021-02-11 160543].svg'>\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (2) GRU and LSTM cells\n",
    "\n",
    "## SimpleRNN cell\n",
    "<p algin='center' >\n",
    "    <img src='image/Screenshot 2021-02-12 174743.png'>\n",
    "</p>\n",
    "\n",
    "## GRU cell\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-12 174900.png'>\n",
    "</p>\n",
    "\n",
    "## LSTM cell\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-12 175037.png'>\n",
    "</p>\n",
    "\n",
    "## No more vanishing gradients\n",
    "- The `simpleRNN` cell can have gradient problems.\n",
    "    - The weight matrix power t multiplies the other terms\n",
    "- `GRU` and `LSTM` cells don't have vanishing gradient problems\n",
    "    - Because of their gates\n",
    "    - Don't have the weight matrics terms multiplying the rest\n",
    "    - Exploding gradient problems are easier to solve\n",
    "\n",
    "## Usage in keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the layers\n",
    "from keras.layers import GRU, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the layers to a model\n",
    "model.add(GRU(units=128, return_sequences=True, name='GRU layer'))\n",
    "model.add(GRU(units=64, return_sequences=False, name='LSTM layer'))"
   ]
  },
  {
   "source": [
    "# Exercise III: GRU cells are better than simpleRNN\n",
    "\n",
    "In this exercise you will re-run the same model as the first chapter of the course to compare the accuracy of the model by simpling changing the `SimpleRNN` cell to a `GRU` cell.\n",
    "\n",
    "The model was already trained with 10 epochs, as in the previous model with a `SimpleRNN` cell. In order to compare the models, a test set `(x_test, y_test)` is already loaded in the environment, as well as the old model `SimpleRNN_model`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import the `GRU` cell.\n",
    "- Print the models' summaries.\n",
    "- Print the accuracy of each model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "from keras.layers import GRU, Dense\n",
    "\n",
    "# Print the old and new model summaries\n",
    "SimpleRNN_model.summary()\n",
    "gru_model.summary()\n",
    "\n",
    "# Evaluate the models' performance (ignore the loss value)\n",
    "_, acc_simpleRNN = SimpleRNN_model.evaluate(X_test, y_test, verbose=0)\n",
    "_, acc_GRU = gru_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"SimpleRNN model's accuracy:\\t{0}\".format(acc_simpleRNN))\n",
    "print(\"GRU model's accuracy:\\t{0}\".format(acc_GRU))"
   ]
  },
  {
   "source": [
    "# Exercise IV: Stacking RNN layers\n",
    "\n",
    "Deep RNN models can have tens to hundreds of layers in order to achieve state-of-the-art results.\n",
    "\n",
    "In this exercise, you will get a glimpse of how to create deep RNN models by stacking layers of LSTM cells one after the other.\n",
    "\n",
    "To do this, you will set the `return_sequences` argument to `True` on the firsts two `LSTM` layers and to `False` on the last `LSTM` layer.\n",
    "\n",
    "To create models with even more layers, you can keep adding them one after the other or create a function that uses the `.add()` method inside a loop to add many layers with few lines of code.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import the `LSTM` layer.\n",
    "- Return the sequences in the first two layers and don't return the sequences in the last `LSTM` layer.\n",
    "- Load the pre-trained weights.\n",
    "- Print the loss and accuracy obtained."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LSTM layer\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(None, 1), return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('lstm_stack_model_weights.h5')\n",
    "\n",
    "print(\"Loss: %0.04f\\nAccuracy: %0.04f\" % tuple(model.evaluate(X_test, y_test, verbose=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}