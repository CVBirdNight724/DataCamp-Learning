{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# RNN Architecture\n",
    "\n",
    "You will learn about the vanishing and exploding gradient problems, often occurring in RNNs, and how to deal with them with the GRU and LSTM cells. Furthermore, you'll create embedding layers for language models and revisit the sentiment classification task."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Vanishing and exploding gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training RNN models\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 151311.png'>\n",
    "    <img src='image/Screenshot 2021-02-11 151431.png'>\n",
    "</p>\n",
    "\n",
    "Example:\n",
    "$$a_2 = f(W_a , a_1 , x_2)$$ \n",
    "$$= f(W_a , f(W_a , a_0 , x_1), x_2)$$\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 152644.png'>\n",
    "</p>\n",
    "\n",
    "**Remember that**\n",
    "$$a_T = f(W_a , a_T-1 , x_T)$$\n",
    "$a_T$ also depends on $a_T-1$ which depends on $a_T-2$ and so on!\n",
    "\n",
    "## BPTT continuation\n",
    "**Computing derivatives leads to**\n",
    "$$\\frac{\\partial a_t}{\\partial W_a = (W_a)^{t-1} g(X)}$$\n",
    "\n",
    "- $(W_a)^{t-1}$ **can cpnverge to 0**\n",
    "- **or diverge to** $+\\infty$**!**\n",
    "\n",
    "## Solutions to the gradient problems\n",
    "Some solutions are known:\n",
    "\n",
    "### Exploding gradinets\n",
    "- Gradient clipping / scaling\n",
    "\n",
    "### Vanishing gradients\n",
    "- Better initialize the matrix W\n",
    "- Use regularization\n",
    "- Use ReLU instead of tanh / sigmoid / softmax\n",
    "- **Use LSTM or GRU cells!**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Exploding gradient problem\n",
    "\n",
    "In the video exercise, you learned about two problems that may arise when working with RNN models: the vanishing and exploding gradient problems.\n",
    "\n",
    "This exercise explores the exploding gradient problem, showing that the derivative of a function can increase exponentially, and how to solve it with a simple technique.\n",
    "\n",
    "The data is already loaded on the environment as `X_train`, `X_test`, `y_train` and `y_test`.\n",
    "\n",
    "You will use a **Stochastic Gradient Descent** (SGD) optimizer and **Mean Squared Error** (MSE) as the loss function.\n",
    "\n",
    "In the first step you will observe the gradient exploding by computing the MSE on the train and test sets. On step 2, you will change the optimizer using the `clipvalue` parameter to solve the problem.\n",
    "\n",
    "The Stochastic Gradient Descent in Keras is loaded as `SGD`.\n",
    "\n",
    "### Instructions 1/2\n",
    "\n",
    "- Use `SGD()` as optimizer and `(X_test, y_test)` as validation data.\n",
    "- Evaluate train performance and print all the **MSE** values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "source": [
    "### Instructions 2/2\n",
    "\n",
    "- Set the `SGD()` parameter `clipvalue` equal to `3.0`.\n",
    "- Compute the MSE values and store them on `train_mse` and `test_mse` variables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9, clipvalue=3.0))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"
   ]
  },
  {
   "source": [
    "# Exercise II: Vanishing gradient problem\n",
    "\n",
    "The other possible gradient problem is when the gradients vanish, or go to zero. This is a much harder problem to solve because it is not as easy to detect. If the loss function does not improve on every step, is it because the gradients went to zero and thus didn't update the weights? Or is it because the model is not able to learn?\n",
    "\n",
    "This problem occurs more often in RNN models when long memory is required, meaning having long sentences.\n",
    "\n",
    "In this exercise you will observe the problem on the IMDB data, with longer sentences selected. The data is loaded in `X` and `y` variables, as well as classes `Sequential`, `SimpleRNN`, `Dense` and `matplotlib.pyplot` as `plt`. The model was pre-trained with 100 epochs and its weights are stored on the file `model_weights.h5`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Add a `SimpleRNN` layer to the model.\n",
    "- Load the pre-trained weights on the model using the method `.load_weights()`.\n",
    "- Add the accuracy of the training data available on the attribute `'acc'` to the plot.\n",
    "- Display the plot using the method `.show()`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=600, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Plot the accuracy x epoch graph\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "<p align='center'>\n",
    "    <img src='image/[2021-02-11 160543].svg'>\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}