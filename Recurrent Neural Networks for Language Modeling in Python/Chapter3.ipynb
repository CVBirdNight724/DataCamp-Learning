{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Multi-class classification\n",
    "\n",
    "Next, in this chapter you will learn how to prepare data for the multi-class classification task, as well as the differences between multi-class classification and binary classification (sentiment analysis). Finally, you will learn how to create models and measure their performance with Keras."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Data pre-processing\n",
    "\n",
    "## Text classification\n",
    "Application of text classification\n",
    "- Automatic news classification\n",
    "- Document classification\n",
    "- Queue segmentation for customer support\n",
    "\n",
    "## Changes from binary classification\n",
    "What change from binary to multi class:\n",
    "- Shape of the output variable `y`\n",
    "- Number of units on the output layer\n",
    "- Activation function on the output layer\n",
    "- Loss function\n",
    "\n",
    "## Changes from binary classification\n",
    "Shape of the output variable `y`:\n",
    "\n",
    "- One-hot encoding of the classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: num_classes = 3\n",
    "y[0] = [0, 1, 0]\n",
    "y.shape = (N, num_classes)"
   ]
  },
  {
   "source": [
    "## Change from binary classification\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-12 191952.png'>\n",
    "</p>\n",
    "\n",
    "Activation function on the output layer:\n",
    "\n",
    "- `softmax` gives the probability of every class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))"
   ]
  },
  {
   "source": [
    "Loss function:\n",
    "- Instead of binary, we use categorical cross-entropy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy')"
   ]
  },
  {
   "source": [
    "## Preparing text categories for keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [\"sports\", \"economy\", \"data_science\", \"sports\", \"finance\"]\n",
    "# Transform to pandas series object\n",
    "y_series = pd.Series(y, dtype=\"category\")\n",
    "# Print the category codes\n",
    "print(y_series.cat.codes)"
   ]
  },
  {
   "source": [
    "## Pre-processing y"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y = np.array([0, 1, 2])\n",
    "\n",
    "# Change to categorical\n",
    "y_prep = to_categorical(y)\n",
    "print(y_prep)"
   ]
  },
  {
   "source": [
    "# Exercise I: Prepare label vectors\n",
    "In the video exercise, you learned the differences between binary classification and multi-class classification. You learned that there are some modifications to the data preparation process that need to be done before training the models.\n",
    "\n",
    "In this exercise, you will prepare a raw dataset with labels given as text. The data is given as a `pandas.DataFrame` called `df`, with two columns: `text` with the text data and `label` with the label names. Your task is to make all the necessary transformations to the labels: change string to number and one-hot encode.\n",
    "\n",
    "The module `pandas` as `pd` and the function `to_categorical()` from `keras.utils.np_utils` are already loaded in the environment and the first lines of the dataset is printed on the console for you to see.\n",
    "\n",
    "### Instructions 1/3\n",
    "\n",
    "- Get the attribute `.cat.codes` of the column `label` contained on data frame `df` and print its shape."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerical ids of column label\n",
    "numerical_ids = df.label.cat.codes\n",
    "\n",
    "# Print initial shape\n",
    "print(numerical_ids.shape)"
   ]
  },
  {
   "source": [
    "### Instructions 2/3\n",
    "\n",
    "- One-hot encode the vector using the `to_categorical()` function and store the results in `Y` while printing the new shape."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerical ids of column label\n",
    "numerical_ids = df.label.cat.codes\n",
    "\n",
    "# Print initial shape\n",
    "print(numerical_ids.shape)\n",
    "\n",
    "# One-hot encode the indexes\n",
    "Y = to_categorical(numerical_ids)\n",
    "\n",
    "# Check the new shape of the variable\n",
    "print(Y.shape)"
   ]
  },
  {
   "source": [
    "### Instructions 3/3\n",
    "\n",
    "- Print the first 5 rows of the variable `Y`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerical ids of column label\n",
    "numerical_ids = df.label.cat.codes\n",
    "\n",
    "# Print initial shape\n",
    "print(numerical_ids.shape)\n",
    "\n",
    "# One-hot encode the indexes\n",
    "Y = to_categorical(numerical_ids)\n",
    "\n",
    "# Check the new shape of the variable\n",
    "print(Y.shape)\n",
    "\n",
    "# Print the first 5 rows\n",
    "print(Y[:5])"
   ]
  },
  {
   "source": [
    "# Exercise II: Pre-process data\n",
    "\n",
    "You learned the differences for pre-processing the data in the case of multi-class classification. Let's put that into practice by preprocessing the data in anticipation of creating a simple multi-class classification model.\n",
    "\n",
    "The dataset is loaded in the variable `news_dataset`, and has the following attributes:\n",
    "\n",
    "- `news_dataset.data`: array with texts\n",
    "- `news_dataset.target`: array with target categories as numerical indexes\n",
    "\n",
    "The sample data contains 5,000 observations.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Instantiate the `Tokenizer` class on the `tokenizer` variable.\n",
    "- Fit the `tokenizer` variable on the text data.\n",
    "- Use the `.texts_to_sequences()` method on the text data.\n",
    "- Use the `to_categorical()` function to prepare the target indexes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(news_dataset.data)\n",
    "\n",
    "# Prepare the data\n",
    "prep_data = tokenizer.texts_to_sequences(news_dataset.data)\n",
    "prep_data = pad_sequences(prep_data, maxlen=200)\n",
    "\n",
    "# Prepare the labels\n",
    "prep_labels = to_categorical(news_dataset.target)\n",
    "\n",
    "# Print the shapes\n",
    "print(prep_data.shape)\n",
    "print(prep_labels.shape)"
   ]
  },
  {
   "source": [
    "# (2) Transfer learning for language models\n",
    "\n",
    "## The idea behind transfer learning\n",
    "Transfer learning:\n",
    "- Start with better than random initial weights\n",
    "- Use models trained on very big datasets\n",
    "- \"Open-source\" data science models\n",
    "\n",
    "## Available architectures\n",
    "Base example: `I really loved this movie`\n",
    "\n",
    "- Word2Vec\n",
    "    - Continuous Bags of words (CBOW) `X = [I, really, this, movie], y = loved`\n",
    "    - Skip-gram `X = loved, y = [I, really, this, movie]`\n",
    "- FastText `X= [I, rea, eal, all, lly, really, ...], y = loved`\n",
    "    - Uses word and n-grams of chars\n",
    "- ELMo `X = [I, really, loved, this], y = movie`\n",
    "    - Uses words, embeddings per context\n",
    "    - Uses Deep bidirectional language models (biLM)\n",
    "- Word2Vec and FastText are FastText are available on package `gensim` and ELMo on `tensorflow_hub`\n",
    "\n",
    "## Example using Word2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "# Train the model\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=embedding_dim,\n",
    "                                window=neighbor_words_num, iter=100)\n",
    "# Get top 3 similar words to \"Captain\"\n",
    "w2v_model.wv.model_similar([\"captain\"], topn=3)"
   ]
  },
  {
   "source": [
    "## Example using FastText"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.model import fasttext\n",
    "# Instantiate the model\n",
    "ft_model = fasttext.FastText(size=embedding_dim, window=neighbor_words_num)\n",
    "# Build vocabulary\n",
    "ft_model.build_vocab(sentences=tokenized_corpus)\n",
    "# Train the model\n",
    "ft_model.train(sentences=tokenized_corpus,\n",
    "                total_examples=len(tokenized_corpus),\n",
    "                epochs=100)"
   ]
  },
  {
   "source": [
    "# Exercise III: Transfer learning starting point\n",
    "\n",
    "In this exercise you will see the benefit of using pre-trained vectors as a starting point for your model.\n",
    "\n",
    "You will compare the accuracy of two models trained with two epochs. The architecture of the models is the same: One embedding layer, one LSTM layer with 128 units and the output layer with 5 units which is the number of classes in the sample data. The difference is that one model uses pre-trained vectors on the embedding layer (transfer learning) and the other doesn't.\n",
    "\n",
    "The pre-trained vectors used were the `GloVE` with 200 dimension. The training accuracy history of the validation set of both models are available in the variables `history_no_emb` and `history_emb`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import module `matplotlib.pyplot` as `plt`.\n",
    "- Add the list of accuracy from the model without embeddings to the plot.\n",
    "- Add the list of accuracy from the model with embeddings to the plot.\n",
    "- Display the plot using the method `.show()`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting package\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Insert lists of accuracy obtained on the validation set\n",
    "plt.plot(history_no_emb['acc'], marker='o')\n",
    "plt.plot(history_emb['acc'], marker='o')\n",
    "\n",
    "# Add extra descriptions to plot\n",
    "plt.title('Learning with and without pre-trained embedding vectors')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['no_embeddings', 'with_embeddings'], loc='upper left')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "<p align='center'>\n",
    "    <img src='image/[2021-02-14 120104].svg'>\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise IV: Word2Vec\n",
    "\n",
    "In this exercise you will create a Word2Vec model using Keras.\n",
    "\n",
    "The corpus used to pre-train the model is the script of all episodes of the The Big Bang Theory TV show, divided sentence by sentence. It is available in the variable `bigbang`.\n",
    "\n",
    "The text on the corpus was transformed to lower case and all words were tokenized. The result is stored in the `tokenized_corpus` variable.\n",
    "\n",
    "A `Word2Vec` model was pre-trained using a window size of 10 words for context (5 before and 5 after the center word), words with less than 3 occurrences were removed and the skip gram model method was used with 50 dimension. The model is saved on the file `bigbang_word2vec.model`.\n",
    "\n",
    "The class `Word2Vec` is already loaded in the environment from `gensim.models.word2vec`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Load the pre-trained Word2Vec model.\n",
    "- Store a `list` with the words `\"bazinga\", \"penny\", \"universe\", \"spock\", \"brain\"` in the variable `words_of_interest`, keeping them in that order.\n",
    "- Iterate over each word of interest while using the `.most_similar()` method present on attribute `wv` and append the top 5 similar words to `top5_similar_words` as a dictionary.\n",
    "- Print the found top 5 words for each of the words of interest.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model\n",
    "w2v_model = Word2Vec.load('bigbang_word2vec.model')\n",
    "\n",
    "# Selected words to check similarities\n",
    "words_of_interest = [\"bazinga\", \"penny\", \"universe\", \"spock\", \"brain\"]\n",
    "\n",
    "# Compute top 5 similar words for each of the words of interest\n",
    "top5_similar_words = []\n",
    "for word in words_of_interest:\n",
    "    top5_similar_words.append(\n",
    "      {word: [item[0] for item in w2v_model.wv.most_similar([word], topn=5)]}\n",
    "    )\n",
    "\n",
    "# Print the similar words\n",
    "print(top5_similar_words)"
   ]
  },
  {
   "source": [
    "# (3) Multi-class classification models\n",
    "\n",
    "## Review of the Sentiment classification model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 128))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "## Model architecture\n",
    "Same architecture can be used"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 128))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "# Output layer has 'num_classes' units and uses 'softmax'\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "## 20 News Group dataset\n",
    "29 News Groups Dataset\n",
    "\n",
    "- Available on `sklearn.datasets import fetch_20newsgroups`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function to load the data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Download train and test sets\n",
    "news_train = fetch_20newsgroups(subset='train')\n",
    "news_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "source": [
    "The data has the following attributes:\n",
    "- `news_train.DESCR`: Documentation.\n",
    "- `news_train.data`: Text data.\n",
    "- `news_train.filenames`: Path to the files on disk.\n",
    "- `news_train.target`: Numerical index of the classes.\n",
    "- `news_train.target_names`: Unique names of the classes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Pre-processing text data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# Create and fit the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(news_train.data)\n",
    "# Create the (X, Y) variables\n",
    "X_train = tokenizer.texts_to_sequences(news_train.data)\n",
    "X_train = pad_sequences(X_train, maxlen=400)\n",
    "Y_train = to_categorical(news_train.target)"
   ]
  },
  {
   "source": [
    "## Training on data\n",
    "Train the model on training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, Y_train, batch_size=64, epochs=100)\n",
    "# Evaluate on test data\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "source": [
    "# Exercise V: Exploring 20 News Groups dataset\n",
    "\n",
    "In this exercise, you will be given a sample of the **20 News Groups** dataset obtained using the `fetch_20newsgroups()` function from `sklearn.datasets`, filtering only three classes: `sci.space`, `alt.atheism` and `soc.religion.christian`.\n",
    "\n",
    "The dataset is loaded in the variable `news_dataset`. Its attributes are printed so you can explore them on the console.\n",
    "\n",
    "Fore more details on how to use this function, see the [Sklearn documentation](https://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset).\n",
    "\n",
    "You will tokenize the texts and one-hot encode the labels step by step to understand how the transformations happen.\n",
    "\n",
    "### Instructions 1/4\n",
    "\n",
    "- Print the example article with index `5` from `news_dataset.data`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See example article\n",
    "print(news_dataset.data[5])"
   ]
  },
  {
   "source": [
    "### Instructions 2/4\n",
    "\n",
    "- Transform the data into a sequence of numerical indexes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See example article\n",
    "print(news_dataset.data[5])\n",
    "\n",
    "# Transform the text into numerical indexes\n",
    "news_num_indices = tokenizer.texts_to_sequences(news_dataset.data)"
   ]
  },
  {
   "source": [
    "### Instructions 3/4\n",
    "\n",
    "- Print the transformed example (index `5`) article."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See example article\n",
    "print(news_dataset.data[5])\n",
    "\n",
    "# Transform the text into numerical indexes\n",
    "news_num_indices = tokenizer.texts_to_sequences(news_dataset.data)\n",
    "\n",
    "# Print the transformed example article\n",
    "print(news_num_indices[5])"
   ]
  },
  {
   "source": [
    "### Instructions 4/4\n",
    "\n",
    "- Transform the labels into one-hot vectors using the function `to_categorical()` and print the original text label and the transformed one-hot vector at index `5` to see the transformed example."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See example article\n",
    "print(news_dataset.data[5])\n",
    "\n",
    "# Transform the text into numerical indexes\n",
    "news_num_indices = tokenizer.texts_to_sequences(news_dataset.data)\n",
    "\n",
    "# Print the transformed example article\n",
    "print(news_num_indices[5])\n",
    "\n",
    "# Transform the labels into one-hot encoded vectors\n",
    "labels_onehot = to_categorical(news_dataset.target)\n",
    "\n",
    "# Check before and after for the sample article\n",
    "print(\"Before: {0}\\nAfter: {1}\".format(news_dataset.target[5], labels_onehot[5]))"
   ]
  },
  {
   "source": [
    "# Exercise VI: Classifying news articles\n",
    "\n",
    "In this exercise you will create a multi-class classification model.\n",
    "\n",
    "The dataset is already loaded in the environment as `news_novel`. Also, all the pre-processing of the training data is already done and `tokenizer` is also available in the environment.\n",
    "\n",
    "A RNN model was pre-trained with the following architecture: use the `Embedding` layer, one `LSTM` layer and the output `Dense` layer expecting three classes: `sci.space`, `alt.atheism`, and `soc.religion.christian`. The weights of this trained model are available on the `classify_news_weights.h5` file.\n",
    "\n",
    "You will pre-process the novel data and evaluate on a new dataset `news_novel`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Transform the data present on `news_novel.data` using the loaded `tokenizer`.\n",
    "- Pad the obtained sequences of numerical indexes.\n",
    "- Transform the labels present on `news_novel.target` into a one-hot representation.\n",
    "- Evaluate the model using the method `.evaluate()` and print the loss and accuracy obtained.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change text for numerical ids and pad\n",
    "X_novel = tokenizer.texts_to_sequences(news_novel.data)\n",
    "X_novel = pad_sequences(X_novel, maxlen=400)\n",
    "\n",
    "# One-hot encode the labels\n",
    "Y_novel = to_categorical(news_novel.target)\n",
    "\n",
    "# Load the model pre-trained weights\n",
    "model.load_weights('classify_news_weights.h5')\n",
    "\n",
    "# Evaluate the model on the new dataset\n",
    "loss, acc = model.evaluate(X_novel, Y_novel, batch_size=64)\n",
    "\n",
    "# Print the loss and accuracy obtained\n",
    "print(\"Loss:\\t{0}\\nAccuracy:\\t{1}\".format(loss, acc))"
   ]
  }
 ]
}