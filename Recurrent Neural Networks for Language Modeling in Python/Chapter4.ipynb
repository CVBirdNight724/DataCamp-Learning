{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Sequence to Sequence Models\n",
    "\n",
    "This chapter introduces you to two applications of RNN models: Text Generation and Neural Machine Translation. You will learn how to prepare the text data to the format needed by the models. The Text Generation model is used for replicating a character's way of speech and will have some fun mimicking Sheldon from The Big Bang Theory. Neural Machine Translation is used for example by Google Translate in a much more complex model. In this chapter, you will create a model that translates Portuguese small phrases into English."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Sequence to Sequence Models\n",
    "\n",
    "## Sequence to sequence\n",
    "Possible architectures:\n",
    "- Many inputs with one output\n",
    "    - Sentiment analysis\n",
    "    - Classification\n",
    "- Many inputs to many outputs\n",
    "    - Text generation\n",
    "    - Neural Machine Translation (NMT)\n",
    "\n",
    "## Text generation: example\n",
    "Text generation: example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model\n",
    "model.generate_sheldon_phrase()"
   ]
  },
  {
   "source": [
    "## Text generation: modeling\n",
    "How to build text generation models:\n",
    "- Decide if a token will be characters or words\n",
    "    - Words demands very large datasets (hundred of millions sentences)\n",
    "    - Chars can be trained faster, but can generate typos\n",
    "- Prepare the data\n",
    "    - Build training sample with (past tokens, next token) examples\n",
    "- Design the model architecture\n",
    "    - Embedding layer, number of layers, etc.\n",
    "- Train and experiment\n",
    "\n",
    "## NMT: example\n",
    "Neural Machine Translation: example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model\n",
    "model.translate(\"Vamos jogar futebol?\")"
   ]
  },
  {
   "source": [
    "## NMT: modeling\n",
    "How to build `NMT` models:\n",
    "- Get a sample of translated sentences\n",
    "    - For example, the **Anki project**\n",
    "- Prepare the data\n",
    "    - Tokenize input language sentences\n",
    "    - Tokenize output language sentences\n",
    "- Design the model architecture\n",
    "    - Encoder and decoder\n",
    "- Train and experiment\n",
    "\n",
    "## Chapter outliner\n",
    "In this chapter:\n",
    "- Text Generation\n",
    "    - Use pre-trained model to generate a sentence\n",
    "    - Learn to prepare the data and build the model\n",
    "- Neural Machine Translation (NMT)\n",
    "    - All-in-one NMT model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Text generation examples\n",
    "\n",
    "In this exercise, you are going to experiment on two pre-trained models for text generation.\n",
    "\n",
    "The first model will generate one phrase based on the character Sheldon of The Big Bang Theory TV show, and the second model will generate a Shakespeare poems up to 400 characters.\n",
    "\n",
    "The models are loaded on the `sheldon_model` and `poem_model` variables. Also, two custom functions to help generate text are available: `generate_sheldon_phrase()` and `generate_poem()`. Both receive the pre-trained model and a context string as parameters.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use pre-defined function `generate_sheldon_phrase()` with parameters `sheldon_model` and `sheldon_context` and store the output in the `sheldon_phrase` variable.\n",
    "- Print the obtained phrase.\n",
    "- Store the given text into the `poem_context` variable.\n",
    "- Print the poem generated by applying the function `generate_poem()` with the `poem_model` and `poem_context` parameters.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context for Sheldon phrase\n",
    "sheldon_context = \"Iâ€™m not insane, my mother had me tested. \"\n",
    "\n",
    "# Generate one Sheldon phrase\n",
    "sheldon_phrase = generate_sheldon_phrase(sheldon_model, sheldon_context)\n",
    "\n",
    "# Print the phrase\n",
    "print(sheldon_phrase)\n",
    "\n",
    "# Context for poem\n",
    "poem_context = \"May thy beauty forever remain\"\n",
    "\n",
    "# Print the poem\n",
    "print(generate_poem(poem_model, poem_context))"
   ]
  },
  {
   "source": [
    "# Exercise II: NMT example\n",
    "\n",
    "This exercise aims to build on the sneak peek you got of NMT at the beginning of the course. You will continue to translate Portuguese small phrases into English.\n",
    "\n",
    "Some sample sentences are available on the `sentences` variable and are printed on the console.\n",
    "\n",
    "Also, a pre-trained model is available on the `model` variable and you will use two custom functions to simplify some steps:\n",
    "\n",
    "- `encode_sequences()`: Change texts into sequence of numerical indexes and pad them.\n",
    "- `translate_many()`: Uses the pre-trained model to translate a list of sentences from Portuguese into English. Later you will code this function yourself.\n",
    "\n",
    "For more details on the functions, use `help()`. The package `pandas` is loaded as `pd`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use the `encode_sequences()` function to pre-process the texts and save the results in the `X` variable.\n",
    "- Translate the `sentences` using the `translate_many()` function by passing `X` as a parameter.\n",
    "- Create a `pd.DataFrame()` with the original and translated lists as columns.\n",
    "- Print the data frame.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text into sequence of indexes and pad\n",
    "X = encode_sequences(sentences)\n",
    "\n",
    "# Print the sequences of indexes\n",
    "print(X)\n",
    "\n",
    "# Translate the sentences\n",
    "translated = translate_many(model, X)\n",
    "\n",
    "# Create pandas DataFrame with original and translated\n",
    "df = pd.DataFrame({'Original': sentences, 'Translated': translated})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "source": [
    "# (2) The Text Generating Function\n",
    "\n",
    "## Generating sentences\n",
    "- Sentence is determined by punctuation. For example, `.` (period), `!` (exclamation) or `?` (question).\n",
    "    - The punctuation marks need to be in the vocabulary.\n",
    "- There is a sentence token, e.g. `<SENT>` and `</SENT>`, that determines when a sentence begins and ends.\n",
    "    - Need to pre-process the data to insert the labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ''\n",
    "# Loop untill end of sentence\n",
    "while next_char != '.':\n",
    "    # Predict next char: Get pred array in position 0\n",
    "    pred = model.predict(X)[0]\n",
    "    char_index = np.argmax(pred)\n",
    "    next_char = index_to_char(char_index)\n",
    "    # Concatenate to sentence\n",
    "    sentence = sentence + next_char"
   ]
  },
  {
   "source": [
    "## Probability scaling\n",
    "Scale the probability distribution.\n",
    "- **Temperature**: name from physics\n",
    "    - Small values: makes prediction more  confident\n",
    "    - Value equal to one: no scaling\n",
    "    - Higher values: makes prediction more creative\n",
    "    - Hyper-parameter: Try different values to fit the predictions to your need"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_softmax(softmax_pred, temperature=1.0):\n",
    "    # Take the logarithm\n",
    "    scaled_pred = np.log(softmax_pred) / temperature\n",
    "    # Re-apply the exponential\n",
    "    scaled_pred = np.exp(scaled_pred)\n",
    "    # Build probability distribution\n",
    "    scaled_pred = np.random.multinomial(1, scaled_pred, 1)\n",
    "    # Return simulated class\n",
    "    return np.argmax(scaled_pred)"
   ]
  },
  {
   "source": [
    "# Example III: Predict next character\n",
    "\n",
    "In this exercise, you will code the function to predict the next character given a trained model. You will use the past 20 chars to predict the next one. You will learn how to train the model in the next lesson, as this step is integral before model training.\n",
    "\n",
    "This is the initial step to create rules for generating sentences, paragraphs, short texts or other blocks of text as needed.\n",
    "\n",
    "The variables `n_vocab`, `chars_window` and the dictionary `index_to_char` are already loaded in the environment. Also, the functions below are already created for you:\n",
    "\n",
    "- `initialize_X()`: Transforms the text input into a sequence of index numbers with the correct shape.\n",
    "- `predict_next_char()`: Gets the next character using the `.predict()` method of the model class and the `index_to_char` dictionary.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Define the function `get_next_char()` and add the parameters `initial_text` and `chars_window` without default values.\n",
    "- Use `initialize_X()` function and pass variable `char_to_index` to obtain a vector of zeros to be used for prediction.\n",
    "- Use the `predict_next_char()` function to obtain the prediction and store it in the `next_char` variable.\n",
    "- Print the predicted character by applying the defined function on the given `initial_text`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_char(model, initial_text, char_window, char_to_index, index_to_char):\n",
    "  \t# Initialize the X vector with zeros\n",
    "    X = initialize_X(initial_text, chars_window, char_to_index)\n",
    "    \n",
    "    # Get next character using the model\n",
    "    next_char = predict_next_char(model, X, index_to_char)\n",
    "\t\n",
    "    return next_char\n",
    "\n",
    "# Define context sentence and print the generated text\n",
    "initial_text = \"I am not insane, \"\n",
    "print(\"Next character: {0}\".format(get_next_char(model, initial_text, 20, char_to_index, index_to_char)))"
   ]
  },
  {
   "source": [
    "# Exercise IV: Generate sentence with context\n",
    "\n",
    "In this exercise, you are going to experiment on a pre-trained model for text generation. The model is already loaded in the environment in the `model` variable, as well as the `initialize_params()` and `get_next_token()` functions.\n",
    "\n",
    "This later uses the pre-trained model to predict the next character and return three variables: the next character `next_char`, the updated sentence `res` and the the shifted text `seq` that will be used to predict the next one.\n",
    "\n",
    "You will define a function that receives a pre-trained model and a string that will be the start of the generated sentence as inputs. This is a good practice to generate text with context. The sentence limit of `100` characters is an example, you can use other limits (or even without limit) in your applications.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Pass the `initial_text` variable to the `initialize_params()` function.\n",
    "- Create conditions to stop the loop when the counter reaches 100 or a dot (`r'.'`) is found.\n",
    "- Pass the initial values `res`, `seq` to the `get_next_token()` function to obtain the next char.\n",
    "- Print the example phrase generated by the defined function.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase(model, initial_text):\n",
    "    # Initialize variables  \n",
    "    res, seq, counter, next_char = initialize_params(initial_text)\n",
    "    \n",
    "    # Loop until stop conditions are met\n",
    "    while counter < 100 and next_char != r'.':\n",
    "      \t# Get next char using the model and append to the sentence\n",
    "        next_char, res, seq = get_next_token(model, res, seq)\n",
    "        # Update the counter\n",
    "        counter = counter + 1\n",
    "    return res\n",
    "  \n",
    "# Create a phrase\n",
    "print(generate_phrase(model, \"I am not insane, \"))"
   ]
  },
  {
   "source": [
    "# Exercise V: Change the probability scale\n",
    "\n",
    "In this exercise, you will see the difference in the resulted sentence when using different values of `temperature` to scale the probability distribution.\n",
    "\n",
    "The function `generate_phrase()` is an adaptation of the function you created before and is already loaded in the environment. It receives the parameters `model` with the pre-trained model, `initial_text` with the context text and `temperature` that is the value to scale the `softmax()` function.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Store the list of temperatures to the `temperatures` variable.\n",
    "- Loop a variable `temperature` over the `temperatures` list.\n",
    "- Generate a phrase using the pre-loaded function `generate_phrase()`.\n",
    "- Print the temperature and the generated sentence.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial text\n",
    "initial_text = \"Spock and me \"\n",
    "\n",
    "# Define a vector with temperature values\n",
    "temperatures = [0.2, 0.8, 1.0, 3.0, 10.0]\n",
    "\n",
    "# Loop over temperatures and generate phrases\n",
    "for temperature in temperatures:\n",
    "\t# Generate a phrase\n",
    "\tphrase = generate_phrase(model, initial_text, temperature)\n",
    "    \n",
    "\t# Print the phrase\n",
    "\tprint('Temperature {0}: {1}'.format(temperature, phrase))"
   ]
  },
  {
   "source": [
    "# (3) Text Generation Models\n",
    "\n",
    "## Similar to a classification model\n",
    "The text Generation Model:\n",
    "- Uses the vocabulary as classes\n",
    "- The last layer applies a softmax with vocabulary size units\n",
    "- Uses `categorical_crossentropy` as loss function\n",
    "\n",
    "## Example model using keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units, input_shape=(chars_window, n_vocab),\n",
    "                dropout=0.15, recurrent_dripout=0.15,               \n",
    "                return_sequneces=True))\n",
    "model.add(LSTM(units, dropout=dropout, recurrent_dropout=0.15,\n",
    "                return_sequneces=False))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "source": [
    "## But not really classification model\n",
    "Difference to classification:\n",
    "- Computes loss, but not performance metrics (accuracy)\n",
    "    - Humans see results and evaluate performance.\n",
    "    - If not good, train more epochs or add complexity to the model (add more memory cells, add layers, etc.).\n",
    "- Used with generation rules according to task\n",
    "    - Generate next char\n",
    "    - Generate one word\n",
    "    - Generate one sentence\n",
    "    - Generate one paragraph\n",
    "\n",
    "## Other applications\n",
    "- Name creation\n",
    "    - Baby names\n",
    "    - New star names, etc.\n",
    "- Generate marked text\n",
    "    - LaTeX\n",
    "    - Markdown\n",
    "    - XML, etc.\n",
    "    - Programming code\n",
    "\n",
    "## Data prep\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-17 012655.png'>\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise VI: Create vectors of sentences and next characters\n",
    "\n",
    "This exercise aims to emphasize more the value of data preparation. You will use texts containing phrases of the character Sheldon from The Big Bang Theory TV show as input and will create vectors of sentence indexes and next characters that are needed before creating a text generation model.\n",
    "\n",
    "The text is available in the `sheldon` variable, as well as the vocabulary (characters) on the `vocabulary` variable and the hyperparameters `chars_window` and `step` defined with values `20` and `3`. This means that a sequence of 20 characters will be used to predict the next one, and the window will shift 3 characters on every iteration.\n",
    "\n",
    "Also, the package `pandas` as `pd` is loaded in the environment.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Split the text by line break to loop through sentences.\n",
    "- Loop until the end of the sentence minus chars_window.\n",
    "- Append the portion of the sentence that has `chars_window` characters to the `sentences` variable and append the next character to the `next_chars` variable.\n",
    "- Use the obtained vectors to create a `pd.DataFrame()` and print its first rows.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the vectors\n",
    "sentences = []\n",
    "next_chars = []\n",
    "# Loop for every sentence\n",
    "for sentence in sheldon.split('\\n'):\n",
    "    # Get 20 previous chars and next char; then shift by step\n",
    "    for i in range(0, len(sentence) - chars_window, step):\n",
    "        sentences.append(sentence[i:i + chars_window])\n",
    "        next_chars.append(sentence[i + chars_window])\n",
    "\n",
    "# Define a Data Frame with the vectors\n",
    "df = pd.DataFrame({'sentence': sentences, 'next_char': next_chars})\n",
    "\n",
    "# Print the initial rows\n",
    "print(df.head())"
   ]
  },
  {
   "source": [
    "# Exercise VII: Preparing the data for training\n",
    "\n",
    "In this exercise, you will continue to prepare the data to train the model. After creating the arrays of sentences and next characters, you need to transform them to numerical values that can be used on the model.\n",
    "\n",
    "This step is necessary because the RNN models expect numbers only and not strings. You will create numerical arrays that have zeros or ones in the positions representing the characters present on the sentences. Ones (or `True`) represent the corresponding character is present, while zeros (or `False`) represent the absence of the character in that position of the sentence.\n",
    "\n",
    "The variables `sentences`, `next_char`, `n_vocab`, `chars_window`, `num_seqs` (number of sentences in the training data) are already loaded in the environment, as well as `numpy` as `np`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Instantiate a `np.array()` with zeros and shape `(number of sentences, characters window, vocabulary size)`.\n",
    "- Use the dictionary `char_to_index` to set the position of the current char to `1`.\n",
    "- Set the current next character to `1`.\n",
    "- Print the first position of each array.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the variables with zeros\n",
    "numerical_sentences = np.zeros((num_seqs, chars_window, n_vocab), dtype=np.bool)\n",
    "numerical_next_chars = np.zeros((num_seqs, n_vocab), dtype=np.bool)\n",
    "\n",
    "# Loop for every sentence\n",
    "for i, sentence in enumerate(sentences):\n",
    "  # Loop for every character in sentence\n",
    "  for t, char in enumerate(sentence):\n",
    "    # Set position of the character to 1\n",
    "    numerical_sentences[i, t, char_to_index[char]] = 1\n",
    "    # Set next character to 1\n",
    "    numerical_next_chars[i, char_to_index[next_chars[i]]] = 1\n",
    "\n",
    "# Print the first position of each\n",
    "print(numerical_sentences[0], numerical_next_chars[0], sep=\"\\n\")"
   ]
  },
  {
   "source": [
    "# Exercise VIII: Creating the text generation model\n",
    "\n",
    "In this exercise, you will define a text generation model using Keras.\n",
    "\n",
    "The variables `n_vocab` containing the vocabulary size and `input_shape` containing the shape of the data used for training are already loaded in the environment. Also, the weights of a pre-trained model is available on file `model_weights.h5`. The model was trained with `40` epochs on the training data. Recap that to train a model in Keras, you just use the method `.fit()` on the training data `(X, y)`, and the parameter `epochs`. For example:\n",
    "\n",
    "```\n",
    "model.fit(X_train, y_train, epochs=40)\n",
    "```\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Add one `LSTM` layer returning the sequences.\n",
    "- Add one `LSTM` layer not returning the sequences.\n",
    "- Add the output layer with `n_vocab units`.\n",
    "- Display the model summary.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Sequential(name=\"LSTM model\")\n",
    "\n",
    "# Add two LSTM layers\n",
    "model.add(LSTM(64, input_shape=input_shape, dropout=0.15, recurrent_dropout=0.15, return_sequences=True, name=\"Input_layer\"))\n",
    "model.add(LSTM(64, dropout=0.15, recurrent_dropout=0.15, return_sequences=False, name=\"LSTM_hidden\"))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(n_vocab, activation='softmax', name=\"Output_layer\"))\n",
    "\n",
    "# Compile and load weights\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.load_weights('model_weights.h5')\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "# (4) Neural Machine Translation\n",
    "\n",
    "## Encoder and decoders\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-17 022641.png'>\n",
    "</p>\n",
    "\n",
    "## Encoder example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Sequential()\n",
    "# Embedding layer for input language\n",
    "model.add(Embedding(input_language_size, input_wordvec_dim,\n",
    "                    input_length=input_language_len, mask_zero=True))\n",
    "# Add LSTM\n",
    "model.add(LSTM(128))\n",
    "# Repeat the last vector\n",
    "mode.add(RepeatVector(output_language_len))"
   ]
  },
  {
   "source": [
    "## Decoder example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right after the encoder\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "# Add TIme Distributed\n",
    "model.add(TimeDistributed(Dense(eng_vocab_size, activation='softmax')))"
   ]
  },
  {
   "source": [
    "## Data prep\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-17 023347.png'>\n",
    "</p>\n",
    "\n",
    "## Data preparation for the input language"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Tokenizer class\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(input_text_list)\n",
    "# Text to sequence of numerical indexes\n",
    "X = tokenizer.texts_to_sequences(input_text_list)\n",
    "# Pad sequences\n",
    "X = pad_sequencers(X, maxlen=length, padding='post')"
   ]
  },
  {
   "source": [
    "## Tokenize the output language"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Tokenizer class\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(output_texts_list)\n",
    "# Text to sequence of numerical indexes\n",
    "Y = tokenizer.texts_to_sequences(output_texts_list)\n",
    "# Pad sequences\n",
    "Y = pad_sequences(Y, maxlen=length, padding='post')"
   ]
  },
  {
   "source": [
    "## One-hot encode the output language"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a temporary variable\n",
    "ylist = list()\n",
    "# Loop over the sequence of numerical indexes\n",
    "for sequence in Y:\n",
    "    # One-hot encoder each index on current sentence\n",
    "    encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "    # Append one-hot encoded values to the list\n",
    "    ylist.append(encoded)\n",
    "# Transform to np.array and reshape\n",
    "Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], vocab_size)"
   ]
  },
  {
   "source": [
    "## Note on training and evaluating\n",
    "Training the model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, epochs=N)"
   ]
  },
  {
   "source": [
    "Evaluating:\n",
    "- Use BLEU\n",
    "    - `nltk.translate.bleu_score`  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Example IX: Preparing the input text\n",
    "\n",
    "You have seen in the video how to prepare the input and output texts. This exercise aims to show a common practice that is to use the maximum length of the sentences to pad all of them, this way no information will be lost.\n",
    "\n",
    "Since the RNN models need the inputs to have the same size, this is a way to pad all sentences and just add zeros to the smaller sentences, without cutting the larger ones.\n",
    "\n",
    "Also, you will use words instead of characters to represent the tokens, this is a common approach for NMT models.\n",
    "\n",
    "The Portuguese texts are loaded on the `pt_sentences` variable and a fitted tokenizer on the `input_tokenizer` variable.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use the `.split()` method on each sentence to split by white space and obtain the number of words in the sentence.\n",
    "- Use the `.texts_to_sequences()` method to transform text into sequence of indexes.\n",
    "- Use the obtained maximum length of sentences to pad them.\n",
    "- Print the first transformed sentence.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maximum length of the sentences\n",
    "pt_length = max([len(sentence.split()) for sentence in pt_sentences])\n",
    "\n",
    "# Transform text to sequence of numerical indexes\n",
    "X = input_tokenizer.texts_to_sequences(pt_sentences)\n",
    "\n",
    "# Pad the sequences\n",
    "X = pad_sequences(X, maxlen=pt_length, padding='post')\n",
    "\n",
    "# Print first sentence\n",
    "print(pt_sentences[0])\n",
    "\n",
    "# Print transformed sentence\n",
    "print(X[0])"
   ]
  },
  {
   "source": [
    "# Exercise X: Preparing the output text\n",
    "\n",
    "In this exercise, you will prepare the output texts to be used on the translation model. Apart from transforming the text to sequences of indexes, you also need to one-hot encode each index.\n",
    "\n",
    "The English texts are loaded on the `en_sentences` variable, the fitted tokenizer on the `output_tokenizer` variable and the English vocabulary size on `en_vocab_size`.\n",
    "\n",
    "Also, a function to perform the first steps of transforming the output language (transformation of texts into sequence of indexes) is already created. The function is loaded on the environment as `transform_text_to_sequences()` and has two parameters: `sentences` that expect a list of sentences in English and `tokenizer` that expects a fitted Tokenizer object from the `keras.preprocessing.text` module.\n",
    "\n",
    "`numpy` is loaded as `np`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Pass the `en_sentences` and `output_tokenizer` variables to the `transform_text_to_sequences()` function to initialize the `Y` variable.\n",
    "- Use the `to_categorical()` function to one-hot encode the sentences. Use the `en_vocab_size` variable as number of classes.\n",
    "- Transform the temporary list to numpy array and reshape to have shape equal to `(num_sentences, sentences_len, en_vocab_size)`.\n",
    "- Print the raw text and the transformed one.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variable\n",
    "Y = transform_text_to_sequences(en_sentences, output_tokenizer)\n",
    "\n",
    "# Temporary list\n",
    "ylist = list()\n",
    "for sequence in Y:\n",
    "  \t# One-hot encode sentence and append to list\n",
    "    ylist.append(to_categorical(sequence, num_classes=en_vocab_size))\n",
    "\n",
    "# Update the variable\n",
    "Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], en_vocab_size)\n",
    "\n",
    "# Print the raw sentence and its transformed version\n",
    "print(\"Raw sentence: {0}\\nTransformed: {1}\".format(en_sentences[0], Y[0]))"
   ]
  },
  {
   "source": [
    "# Exercise XI: Translate Portuguese to English\n",
    "\n",
    "This is the last exercise of the course, congratulations on getting here!\n",
    "\n",
    "You will learn how to use NMT models for making translations.\n",
    "\n",
    "A model that encodes Portuguese small phrases and decodes them into English small phrases was pre-trained and is loaded in the `model` variable.\n",
    "\n",
    "Also, the function `predict_one()` is already loaded, use `help()` for details and the dataset is available on the `test` (raw text) and `X_test` (tokenized) variables.\n",
    "\n",
    "You will define a function to translate a list of sentences. In the parameters, `sentences` is a list of phrases to be translated, `index_to_word` is a `dict` containing numerical indexes as keys and words as values for the English language, loaded in the `en_index_to_word` variable.\n",
    "\n",
    "The model summary has been printed for your consideration.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Loop over the enumerated iterator of the phrases.\n",
    "- Use the pre-loaded function `predict_one()` to translate one phrase.\n",
    "- Print the translation result.\n",
    "- Call the defined function to translate the initial 10 phrases of the `X_test` variable.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict many phrases\n",
    "def predict_many(model, sentences, index_to_word, raw_dataset):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Translate the Portuguese sentence\n",
    "        translation = predict_one(model, sentence, index_to_word)\n",
    "        \n",
    "        # Get the raw Portuguese and English sentences\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        \n",
    "        # Print the correct Portuguese and English sentences and the predicted\n",
    "        print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\n",
    "predict_many(model, X_test[:10], en_index_to_word, test)"
   ]
  },
  {
   "source": [
    "# Congratuations!\n",
    "\n",
    "## Wrap-up\n",
    "- Introduction to language task:\n",
    "    - Sentiment classification\n",
    "    - Multi-class classification\n",
    "    - Text Generation\n",
    "    - Neural Machine Translation\n",
    "- Sequence to sequence models\n",
    "- Implementation in Keras\n",
    "\n",
    "## RNN pitfalls and different cell types\n",
    "- Vanishing and exploding gradient problems\n",
    "- GRU and LSTM cells\n",
    "- Word vectors and the Embedding layer\n",
    "- Better sentiment analysis\n",
    "\n",
    "## Multi-class classification\n",
    "- Data preparation\n",
    "- Transfer learning\n",
    "- Keras models\n",
    "- Model performance\n",
    "\n",
    "## Text generation and NMT\n",
    "- Text Generation\n",
    "    - Chars as token\n",
    "    - Data preparation\n",
    "    - Generation sentences mimicking Sheldon\n",
    "- Neural Machine Translation\n",
    "    - Words as tokens\n",
    "    - Translate Portuguese to English"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}