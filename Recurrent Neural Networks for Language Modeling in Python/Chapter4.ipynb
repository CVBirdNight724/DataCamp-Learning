{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Sequence to Sequence Models\n",
    "\n",
    "This chapter introduces you to two applications of RNN models: Text Generation and Neural Machine Translation. You will learn how to prepare the text data to the format needed by the models. The Text Generation model is used for replicating a character's way of speech and will have some fun mimicking Sheldon from The Big Bang Theory. Neural Machine Translation is used for example by Google Translate in a much more complex model. In this chapter, you will create a model that translates Portuguese small phrases into English."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Sequence to Sequence Models\n",
    "\n",
    "## Sequence to sequence\n",
    "Possible architectures:\n",
    "- Many inputs with one output\n",
    "    - Sentiment analysis\n",
    "    - Classification\n",
    "- Many inputs to many outputs\n",
    "    - Text generation\n",
    "    - Neural Machine Translation (NMT)\n",
    "\n",
    "## Text generation: example\n",
    "Text generation: example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model\n",
    "model.generate_sheldon_phrase()"
   ]
  },
  {
   "source": [
    "## Text generation: modeling\n",
    "How to build text generation models:\n",
    "- Decide if a token will be characters or words\n",
    "    - Words demands very large datasets (hundred of millions sentences)\n",
    "    - Chars can be trained faster, but can generate typos\n",
    "- Prepare the data\n",
    "    - Build training sample with (past tokens, next token) examples\n",
    "- Design the model architecture\n",
    "    - Embedding layer, number of layers, etc.\n",
    "- Train and experiment\n",
    "\n",
    "## NMT: example\n",
    "Neural Machine Translation: example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model\n",
    "model.translate(\"Vamos jogar futebol?\")"
   ]
  },
  {
   "source": [
    "## NMT: modeling\n",
    "How to build `NMT` models:\n",
    "- Get a sample of translated sentences\n",
    "    - For example, the **Anki project**\n",
    "- Prepare the data\n",
    "    - Tokenize input language sentences\n",
    "    - Tokenize output language sentences\n",
    "- Design the model architecture\n",
    "    - Encoder and decoder\n",
    "- Train and experiment\n",
    "\n",
    "## Chapter outliner\n",
    "In this chapter:\n",
    "- Text Generation\n",
    "    - Use pre-trained model to generate a sentence\n",
    "    - Learn to prepare the data and build the model\n",
    "- Neural Machine Translation (NMT)\n",
    "    - All-in-one NMT model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Text generation examples\n",
    "\n",
    "In this exercise, you are going to experiment on two pre-trained models for text generation.\n",
    "\n",
    "The first model will generate one phrase based on the character Sheldon of The Big Bang Theory TV show, and the second model will generate a Shakespeare poems up to 400 characters.\n",
    "\n",
    "The models are loaded on the `sheldon_model` and `poem_model` variables. Also, two custom functions to help generate text are available: `generate_sheldon_phrase()` and `generate_poem()`. Both receive the pre-trained model and a context string as parameters.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use pre-defined function `generate_sheldon_phrase()` with parameters `sheldon_model` and `sheldon_context` and store the output in the `sheldon_phrase` variable.\n",
    "- Print the obtained phrase.\n",
    "- Store the given text into the `poem_context` variable.\n",
    "- Print the poem generated by applying the function `generate_poem()` with the `poem_model` and `poem_context` parameters.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context for Sheldon phrase\n",
    "sheldon_context = \"Iâ€™m not insane, my mother had me tested. \"\n",
    "\n",
    "# Generate one Sheldon phrase\n",
    "sheldon_phrase = generate_sheldon_phrase(sheldon_model, sheldon_context)\n",
    "\n",
    "# Print the phrase\n",
    "print(sheldon_phrase)\n",
    "\n",
    "# Context for poem\n",
    "poem_context = \"May thy beauty forever remain\"\n",
    "\n",
    "# Print the poem\n",
    "print(generate_poem(poem_model, poem_context))"
   ]
  },
  {
   "source": [
    "# Exercise II: NMT example\n",
    "\n",
    "This exercise aims to build on the sneak peek you got of NMT at the beginning of the course. You will continue to translate Portuguese small phrases into English.\n",
    "\n",
    "Some sample sentences are available on the `sentences` variable and are printed on the console.\n",
    "\n",
    "Also, a pre-trained model is available on the `model` variable and you will use two custom functions to simplify some steps:\n",
    "\n",
    "- `encode_sequences()`: Change texts into sequence of numerical indexes and pad them.\n",
    "- `translate_many()`: Uses the pre-trained model to translate a list of sentences from Portuguese into English. Later you will code this function yourself.\n",
    "\n",
    "For more details on the functions, use `help()`. The package `pandas` is loaded as `pd`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use the `encode_sequences()` function to pre-process the texts and save the results in the `X` variable.\n",
    "- Translate the `sentences` using the `translate_many()` function by passing `X` as a parameter.\n",
    "- Create a `pd.DataFrame()` with the original and translated lists as columns.\n",
    "- Print the data frame.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text into sequence of indexes and pad\n",
    "X = encode_sequences(sentences)\n",
    "\n",
    "# Print the sequences of indexes\n",
    "print(X)\n",
    "\n",
    "# Translate the sentences\n",
    "translated = translate_many(model, X)\n",
    "\n",
    "# Create pandas DataFrame with original and translated\n",
    "df = pd.DataFrame({'Original': sentences, 'Translated': translated})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "source": [
    "# (2) The Text Generating Function\n",
    "\n",
    "## Generating sentences\n",
    "- Sentence is determined by punctuation. For example, `.` (period), `!` (exclamation) or `?` (question).\n",
    "    - The punctuation marks need to be in the vocabulary.\n",
    "- There is a sentence token, e.g. `<SENT>` and `</SENT>`, that determines when a sentence begins and ends.\n",
    "    - Need to pre-process the data to insert the labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ''\n",
    "# Loop untill end of sentence\n",
    "while next_char != '.':\n",
    "    # Predict next char: Get pred array in position 0\n",
    "    pred = model.predict(X)[0]\n",
    "    char_index = np.argmax(pred)\n",
    "    next_char = index_to_char(char_index)\n",
    "    # Concatenate to sentence\n",
    "    sentence = sentence + next_char"
   ]
  },
  {
   "source": [
    "## Probability scaling\n",
    "Scale the probability distribution.\n",
    "- **Temperature**: name from physics\n",
    "    - Small values: makes prediction more  confident\n",
    "    - Value equal to one: no scaling\n",
    "    - Higher values: makes prediction more creative\n",
    "    - Hyper-parameter: Try different values to fit the predictions to your need"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_softmax(softmax_pred, temperature=1.0):\n",
    "    # Take the logarithm\n",
    "    scaled_pred = np.log(softmax_pred) / temperature\n",
    "    # Re-apply the exponential\n",
    "    scaled_pred = np.exp(scaled_pred)\n",
    "    # Build probability distribution\n",
    "    scaled_pred = np.random.multinomial(1, scaled_pred, 1)\n",
    "    # Return simulated class\n",
    "    return np.argmax(scaled_pred)"
   ]
  },
  {
   "source": [
    "# Example III: Predict next character\n",
    "\n",
    "In this exercise, you will code the function to predict the next character given a trained model. You will use the past 20 chars to predict the next one. You will learn how to train the model in the next lesson, as this step is integral before model training.\n",
    "\n",
    "This is the initial step to create rules for generating sentences, paragraphs, short texts or other blocks of text as needed.\n",
    "\n",
    "The variables `n_vocab`, `chars_window` and the dictionary `index_to_char` are already loaded in the environment. Also, the functions below are already created for you:\n",
    "\n",
    "- `initialize_X()`: Transforms the text input into a sequence of index numbers with the correct shape.\n",
    "- `predict_next_char()`: Gets the next character using the `.predict()` method of the model class and the `index_to_char` dictionary.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Define the function `get_next_char()` and add the parameters `initial_text` and `chars_window` without default values.\n",
    "- Use `initialize_X()` function and pass variable `char_to_index` to obtain a vector of zeros to be used for prediction.\n",
    "- Use the `predict_next_char()` function to obtain the prediction and store it in the `next_char` variable.\n",
    "- Print the predicted character by applying the defined function on the given `initial_text`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_char(model, initial_text, char_window, char_to_index, index_to_char):\n",
    "  \t# Initialize the X vector with zeros\n",
    "    X = initialize_X(initial_text, chars_window, char_to_index)\n",
    "    \n",
    "    # Get next character using the model\n",
    "    next_char = predict_next_char(model, X, index_to_char)\n",
    "\t\n",
    "    return next_char\n",
    "\n",
    "# Define context sentence and print the generated text\n",
    "initial_text = \"I am not insane, \"\n",
    "print(\"Next character: {0}\".format(get_next_char(model, initial_text, 20, char_to_index, index_to_char)))"
   ]
  },
  {
   "source": [
    "# Exercise IV: Generate sentence with context\n",
    "\n",
    "In this exercise, you are going to experiment on a pre-trained model for text generation. The model is already loaded in the environment in the `model` variable, as well as the `initialize_params()` and `get_next_token()` functions.\n",
    "\n",
    "This later uses the pre-trained model to predict the next character and return three variables: the next character `next_char`, the updated sentence `res` and the the shifted text `seq` that will be used to predict the next one.\n",
    "\n",
    "You will define a function that receives a pre-trained model and a string that will be the start of the generated sentence as inputs. This is a good practice to generate text with context. The sentence limit of `100` characters is an example, you can use other limits (or even without limit) in your applications.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Pass the `initial_text` variable to the `initialize_params()` function.\n",
    "- Create conditions to stop the loop when the counter reaches 100 or a dot (`r'.'`) is found.\n",
    "- Pass the initial values `res`, `seq` to the `get_next_token()` function to obtain the next char.\n",
    "- Print the example phrase generated by the defined function.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase(model, initial_text):\n",
    "    # Initialize variables  \n",
    "    res, seq, counter, next_char = initialize_params(initial_text)\n",
    "    \n",
    "    # Loop until stop conditions are met\n",
    "    while counter < 100 and next_char != r'.':\n",
    "      \t# Get next char using the model and append to the sentence\n",
    "        next_char, res, seq = get_next_token(model, res, seq)\n",
    "        # Update the counter\n",
    "        counter = counter + 1\n",
    "    return res\n",
    "  \n",
    "# Create a phrase\n",
    "print(generate_phrase(model, \"I am not insane, \"))"
   ]
  },
  {
   "source": [
    "# Exercise V: Change the probability scale\n",
    "\n",
    "In this exercise, you will see the difference in the resulted sentence when using different values of `temperature` to scale the probability distribution.\n",
    "\n",
    "The function `generate_phrase()` is an adaptation of the function you created before and is already loaded in the environment. It receives the parameters `model` with the pre-trained model, `initial_text` with the context text and `temperature` that is the value to scale the `softmax()` function.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Store the list of temperatures to the `temperatures` variable.\n",
    "- Loop a variable `temperature` over the `temperatures` list.\n",
    "- Generate a phrase using the pre-loaded function `generate_phrase()`.\n",
    "- Print the temperature and the generated sentence.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial text\n",
    "initial_text = \"Spock and me \"\n",
    "\n",
    "# Define a vector with temperature values\n",
    "temperatures = [0.2, 0.8, 1.0, 3.0, 10.0]\n",
    "\n",
    "# Loop over temperatures and generate phrases\n",
    "for temperature in temperatures:\n",
    "\t# Generate a phrase\n",
    "\tphrase = generate_phrase(model, initial_text, temperature)\n",
    "    \n",
    "\t# Print the phrase\n",
    "\tprint('Temperature {0}: {1}'.format(temperature, phrase))"
   ]
  }
 ]
}