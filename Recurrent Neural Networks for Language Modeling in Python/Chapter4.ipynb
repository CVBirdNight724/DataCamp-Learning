{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Sequence to Sequence Models\n",
    "\n",
    "This chapter introduces you to two applications of RNN models: Text Generation and Neural Machine Translation. You will learn how to prepare the text data to the format needed by the models. The Text Generation model is used for replicating a character's way of speech and will have some fun mimicking Sheldon from The Big Bang Theory. Neural Machine Translation is used for example by Google Translate in a much more complex model. In this chapter, you will create a model that translates Portuguese small phrases into English."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Sequence to Sequence Models\n",
    "\n",
    "## Sequence to sequence\n",
    "Possible architectures:\n",
    "- Many inputs with one output\n",
    "    - Sentiment analysis\n",
    "    - Classification\n",
    "- Many inputs to many outputs\n",
    "    - Text generation\n",
    "    - Neural Machine Translation (NMT)\n",
    "\n",
    "## Text generation: example\n",
    "Text generation: example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model\n",
    "model.generate_sheldon_phrase()"
   ]
  },
  {
   "source": [
    "## Text generation: modeling\n",
    "How to build text generation models:\n",
    "- Decide if a token will be characters or words\n",
    "    - Words demands very large datasets (hundred of millions sentences)\n",
    "    - Chars can be trained faster, but can generate typos\n",
    "- Prepare the data\n",
    "    - Build training sample with (past tokens, next token) examples\n",
    "- Design the model architecture\n",
    "    - Embedding layer, number of layers, etc.\n",
    "- Train and experiment\n",
    "\n",
    "## NMT: example\n",
    "Neural Machine Translation: example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model\n",
    "model.translate(\"Vamos jogar futebol?\")"
   ]
  },
  {
   "source": [
    "## NMT: modeling\n",
    "How to build `NMT` models:\n",
    "- Get a sample of translated sentences\n",
    "    - For example, the **Anki project**\n",
    "- Prepare the data\n",
    "    - Tokenize input language sentences\n",
    "    - Tokenize output language sentences\n",
    "- Design the model architecture\n",
    "    - Encoder and decoder\n",
    "- Train and experiment\n",
    "\n",
    "## Chapter outliner\n",
    "In this chapter:\n",
    "- Text Generation\n",
    "    - Use pre-trained model to generate a sentence\n",
    "    - Learn to prepare the data and build the model\n",
    "- Neural Machine Translation (NMT)\n",
    "    - All-in-one NMT model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Text generation examples\n",
    "\n",
    "In this exercise, you are going to experiment on two pre-trained models for text generation.\n",
    "\n",
    "The first model will generate one phrase based on the character Sheldon of The Big Bang Theory TV show, and the second model will generate a Shakespeare poems up to 400 characters.\n",
    "\n",
    "The models are loaded on the `sheldon_model` and `poem_model` variables. Also, two custom functions to help generate text are available: `generate_sheldon_phrase()` and `generate_poem()`. Both receive the pre-trained model and a context string as parameters.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use pre-defined function `generate_sheldon_phrase()` with parameters `sheldon_model` and `sheldon_context` and store the output in the `sheldon_phrase` variable.\n",
    "- Print the obtained phrase.\n",
    "- Store the given text into the `poem_context` variable.\n",
    "- Print the poem generated by applying the function `generate_poem()` with the `poem_model` and `poem_context` parameters.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context for Sheldon phrase\n",
    "sheldon_context = \"Iâ€™m not insane, my mother had me tested. \"\n",
    "\n",
    "# Generate one Sheldon phrase\n",
    "sheldon_phrase = generate_sheldon_phrase(sheldon_model, sheldon_context)\n",
    "\n",
    "# Print the phrase\n",
    "print(sheldon_phrase)\n",
    "\n",
    "# Context for poem\n",
    "poem_context = \"May thy beauty forever remain\"\n",
    "\n",
    "# Print the poem\n",
    "print(generate_poem(poem_model, poem_context))"
   ]
  },
  {
   "source": [
    "# Exercise II: NMT example\n",
    "\n",
    "This exercise aims to build on the sneak peek you got of NMT at the beginning of the course. You will continue to translate Portuguese small phrases into English.\n",
    "\n",
    "Some sample sentences are available on the `sentences` variable and are printed on the console.\n",
    "\n",
    "Also, a pre-trained model is available on the `model` variable and you will use two custom functions to simplify some steps:\n",
    "\n",
    "- `encode_sequences()`: Change texts into sequence of numerical indexes and pad them.\n",
    "- `translate_many()`: Uses the pre-trained model to translate a list of sentences from Portuguese into English. Later you will code this function yourself.\n",
    "\n",
    "For more details on the functions, use `help()`. The package `pandas` is loaded as `pd`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use the `encode_sequences()` function to pre-process the texts and save the results in the `X` variable.\n",
    "- Translate the `sentences` using the `translate_many()` function by passing `X` as a parameter.\n",
    "- Create a `pd.DataFrame()` with the original and translated lists as columns.\n",
    "- Print the data frame.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text into sequence of indexes and pad\n",
    "X = encode_sequences(sentences)\n",
    "\n",
    "# Print the sequences of indexes\n",
    "print(X)\n",
    "\n",
    "# Translate the sentences\n",
    "translated = translate_many(model, X)\n",
    "\n",
    "# Create pandas DataFrame with original and translated\n",
    "df = pd.DataFrame({'Original': sentences, 'Translated': translated})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  }
 ]
}