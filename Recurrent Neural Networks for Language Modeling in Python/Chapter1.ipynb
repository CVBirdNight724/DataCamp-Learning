{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Recurrent Neural Networks and Keras\n",
    "\n",
    "In this chapter, you will learn the foundations of Recurrent Neural Networks (RNN). Starting with some prerequisites, continuing to understanding how information flows through the network and finally seeing how to implement such models with Keras in the sentiment classification task."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Introduction to the course\n",
    "\n",
    "## Text data is avaliable online\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135329.png\">\n",
    "\n",
    "## Applications of machine learning to text data\n",
    "Four applications:\n",
    "\n",
    "- Sentiment analysis\n",
    "- Multi-class classification\n",
    "- Text generation\n",
    "- Machine neural translation\n",
    "\n",
    "## Setiment analysis\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135621.png\">\n",
    "\n",
    "## Multi-class classification\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135718.png\">\n",
    "\n",
    "## Text generation\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135759.png\">\n",
    "\n",
    "## Neural machine translation\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135844.png\">\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135932.png\">\n",
    "\n",
    "## Sequence to sequence models\n",
    "**Many to one: classification**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140041.png\">\n",
    "\n",
    "**Many to many: text generation**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140152.png\">\n",
    "\n",
    "**Many to many: neural machine translation**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140252.png\">\n",
    "\n",
    "**Many to many: language model**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140416.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Comparing the number of parameter of RNN and ANN\n",
    "\n",
    "In this exercise, you will compare the number of parameters of an artificial neural network (ANN) with the recurrent neural network (RNN) architectures. Here, the vocabulary size is equal to `10,000` for both models.\n",
    "\n",
    "The models have been defined for you with similar architectures of only one layer with `256` units (Dense or RNN) plus the output layer. They are stored on variables `ann_model` and `rnn_model`.\n",
    "\n",
    "Use the method `.summary()` to print the models' architecture and number of parameters and select the correct statement.\n",
    "\n",
    "### Posible Answers\n",
    "\n",
    "- The ANN model has more parameters on the second `Dense` layer than the RNN model.\n",
    "\n",
    "- The RNN model has fewer parameters than the ANN model. (T)\n",
    "\n",
    "- The RNN model needs to train approximately the same number of parameters as the ANN model.\n",
    "\n",
    "- The one-hot encoding allows the RNN model to have fewer parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise II: Sentiment analysis\n",
    "\n",
    "In the video exercise, you were exposed to the various applications of sequence to sequence models. In this exercise you will see how to use a pre-trained model for sentiment analysis.\n",
    "\n",
    "The model is pre-loaded in the environment on variable `model`. Also, the tokenized test set variables `X_test` and `y_test` and the pre-processed original text data `sentences` from IMDb are also available.You will learn how to pre-process the text data and how to create and train the model using Keras later in the course.\n",
    "\n",
    "You will use the pre-trained model to obtain predictions of sentiment. The model returns a number between zero and one representing the probability of the sentence to have a positive sentiment. So, you will create a decision rule to set the prediction to positive or negative.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use the `.predict()` method to make predictions on the test data.\n",
    "- Make the prediction equal to `\"positive\"` if its value is greater than 0.5 and `\"negative\"` otherwise and store the result in the `pred_sentiment` variable.\n",
    "- Create a `pd.DataFrame` containing the pre-processed text, the prediction obtained in the previous step and their true values contained in the `y_test` variable.\n",
    "- Print the first rows using the `.head()` method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first sentence on `X_test`\n",
    "print(X_test[0])\n",
    "\n",
    "# Get the predicion for all the sentences\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Transform the predition into positive (> 0.5) or negative (<= 0.5)\n",
    "pred_sentiment = [\"positive\" if x>0.5 else \"negative\" for x in pred]\n",
    "\n",
    "# Create a data frame with sentences, predictions and true values\n",
    "result = pd.DataFrame({'sentence': sentences, 'y_pred': pred_sentiment, 'y_true': y_test})\n",
    "\n",
    "# Print the first lines of the data frame\n",
    "print(result.head())"
   ]
  },
  {
   "source": [
    "# Exercise III: Sequence to sequence models\n",
    "\n",
    "In the video exercise, you learned about four types of sequence to sequence models: many-to-one (classification) and many-to-many (text generation, neural machine translation and language models). In this exercise, you have to choose the correct type of model given the following problem description:\n",
    "\n",
    "You are helping your friend who is a specialist in speech recognition. Your friend built a model that can recognize different accents of English, but the model is failing to distinguish homophones - words with the same pronunciation but have different meaning such as \"sea\" vs \"see\" or \"write\" vs \"right\".\n",
    "\n",
    "You propose to use a model that will use the context around the words to identify the semantic meaning of the words. By learning the meaning of the words, the new model would avoid outputs like \"Did you sea that car?\" - it would identify that in this case, the correct word would be \"see\".\n",
    "\n",
    "What type of sequence-to-sequence model is appropriate?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "- Many-to-many, because it is a classification model.\n",
    "\n",
    "- Many-to-one, because it is a classification model.\n",
    "\n",
    "- Many-to-many, this problem can be solved with a language model. (T)\n",
    "\n",
    "- Many-to-one, because it is a prediction problem.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (2) Intruction to language models\n",
    "\n",
    "## Sentence probability\n",
    "Many available models\n",
    "\n",
    "- Probability of \"I loved this movie\"\n",
    "- Unigram\n",
    "    - $$P(sentence) = P(I) P(loved) P(this) P(movie)$$\n",
    "- N-gram\n",
    "    - N = 2 (biagram): $$P(sentense) = P(I) P(loved|I) P(this|loved) P(movie|this)$$\n",
    "    - N = 3 (trigram): $$P(sentense) = P(I) P(loved|I) P(this|I + loved) P(movie|loved + this)$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}