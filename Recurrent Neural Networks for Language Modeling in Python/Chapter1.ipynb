{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Recurrent Neural Networks and Keras\n",
    "\n",
    "In this chapter, you will learn the foundations of Recurrent Neural Networks (RNN). Starting with some prerequisites, continuing to understanding how information flows through the network and finally seeing how to implement such models with Keras in the sentiment classification task."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Introduction to the course\n",
    "\n",
    "## Text data is avaliable online\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135329.png\">\n",
    "\n",
    "## Applications of machine learning to text data\n",
    "Four applications:\n",
    "\n",
    "- Sentiment analysis\n",
    "- Multi-class classification\n",
    "- Text generation\n",
    "- Machine neural translation\n",
    "\n",
    "## Setiment analysis\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135621.png\">\n",
    "\n",
    "## Multi-class classification\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135718.png\">\n",
    "\n",
    "## Text generation\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135759.png\">\n",
    "\n",
    "## Neural machine translation\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135844.png\">\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135932.png\">\n",
    "\n",
    "## Sequence to sequence models\n",
    "**Many to one: classification**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140041.png\">\n",
    "\n",
    "**Many to many: text generation**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140152.png\">\n",
    "\n",
    "**Many to many: neural machine translation**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140252.png\">\n",
    "\n",
    "**Many to many: language model**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140416.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Comparing the number of parameter of RNN and ANN\n",
    "\n",
    "In this exercise, you will compare the number of parameters of an artificial neural network (ANN) with the recurrent neural network (RNN) architectures. Here, the vocabulary size is equal to `10,000` for both models.\n",
    "\n",
    "The models have been defined for you with similar architectures of only one layer with `256` units (Dense or RNN) plus the output layer. They are stored on variables `ann_model` and `rnn_model`.\n",
    "\n",
    "Use the method `.summary()` to print the models' architecture and number of parameters and select the correct statement.\n",
    "\n",
    "### Posible Answers\n",
    "\n",
    "- The ANN model has more parameters on the second `Dense` layer than the RNN model.\n",
    "\n",
    "- The RNN model has fewer parameters than the ANN model. (T)\n",
    "\n",
    "- The RNN model needs to train approximately the same number of parameters as the ANN model.\n",
    "\n",
    "- The one-hot encoding allows the RNN model to have fewer parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise II: Sentiment analysis\n",
    "\n",
    "In the video exercise, you were exposed to the various applications of sequence to sequence models. In this exercise you will see how to use a pre-trained model for sentiment analysis.\n",
    "\n",
    "The model is pre-loaded in the environment on variable `model`. Also, the tokenized test set variables `X_test` and `y_test` and the pre-processed original text data `sentences` from IMDb are also available.You will learn how to pre-process the text data and how to create and train the model using Keras later in the course.\n",
    "\n",
    "You will use the pre-trained model to obtain predictions of sentiment. The model returns a number between zero and one representing the probability of the sentence to have a positive sentiment. So, you will create a decision rule to set the prediction to positive or negative.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use the `.predict()` method to make predictions on the test data.\n",
    "- Make the prediction equal to `\"positive\"` if its value is greater than 0.5 and `\"negative\"` otherwise and store the result in the `pred_sentiment` variable.\n",
    "- Create a `pd.DataFrame` containing the pre-processed text, the prediction obtained in the previous step and their true values contained in the `y_test` variable.\n",
    "- Print the first rows using the `.head()` method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first sentence on `X_test`\n",
    "print(X_test[0])\n",
    "\n",
    "# Get the predicion for all the sentences\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Transform the predition into positive (> 0.5) or negative (<= 0.5)\n",
    "pred_sentiment = [\"positive\" if x>0.5 else \"negative\" for x in pred]\n",
    "\n",
    "# Create a data frame with sentences, predictions and true values\n",
    "result = pd.DataFrame({'sentence': sentences, 'y_pred': pred_sentiment, 'y_true': y_test})\n",
    "\n",
    "# Print the first lines of the data frame\n",
    "print(result.head())"
   ]
  },
  {
   "source": [
    "# Exercise III: Sequence to sequence models\n",
    "\n",
    "In the video exercise, you learned about four types of sequence to sequence models: many-to-one (classification) and many-to-many (text generation, neural machine translation and language models). In this exercise, you have to choose the correct type of model given the following problem description:\n",
    "\n",
    "You are helping your friend who is a specialist in speech recognition. Your friend built a model that can recognize different accents of English, but the model is failing to distinguish homophones - words with the same pronunciation but have different meaning such as \"sea\" vs \"see\" or \"write\" vs \"right\".\n",
    "\n",
    "You propose to use a model that will use the context around the words to identify the semantic meaning of the words. By learning the meaning of the words, the new model would avoid outputs like \"Did you sea that car?\" - it would identify that in this case, the correct word would be \"see\".\n",
    "\n",
    "What type of sequence-to-sequence model is appropriate?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "- Many-to-many, because it is a classification model.\n",
    "\n",
    "- Many-to-one, because it is a classification model.\n",
    "\n",
    "- Many-to-many, this problem can be solved with a language model. (T)\n",
    "\n",
    "- Many-to-one, because it is a prediction problem.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (2) Intruction to language models\n",
    "\n",
    "## Sentence probability\n",
    "Many available models\n",
    "\n",
    "- Probability of \"I loved this movie\"\n",
    "- Unigram\n",
    "    - $$P(sentence) = P('I') P('loved') P('this') P('movie')$$\n",
    "- N-gram\n",
    "    - N = 2 (biagram): $$P(sentense) = P('I') P('loved'|'I') P('this'|'loved') P('movie'|'this')$$\n",
    "    - N = 3 (trigram): $$P(sentense) = P('I') P('loved'|'I') P('this'|'I loved') P('movie'|'loved this')$$\n",
    "\n",
    "- Skip gram\n",
    "    - $$P(sentense) = P('context of I'|'I') P('context of loved'|'loved') P('context of this'|'this') P('context of movie'|'movie')\n",
    "\n",
    "- Neural Networks\n",
    "    - The probability of the sentence is given by a `softmax` function on the output layer of the network\n",
    "\n",
    "## Link to RNNs\n",
    "Language models are everywhere in RNNs!\n",
    "\n",
    "- The network itself\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 111619.png'>\n",
    "</p>\n",
    "\n",
    "- Embedding layer\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 111759.png'>\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Buiilding vocabulary dictionaries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique words\n",
    "unique_words = list(set(text.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary: word is key, index is value\n",
    "word_to_index = {k:v for (v,k) in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary: word is key, word is value\n",
    "index_to_word = {k:v for (k,v) in enumerate(unique_words)}"
   ]
  },
  {
   "source": [
    "## Preprocessing input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize varizbles X and y\n",
    "X = []\n",
    "y = []\n",
    "# Loop over the text: length 'sentence_size' per time with step equal to 'step'\n",
    "for i in range(0, len(text) - sentense_size, step):\n",
    "    X.append(text[i:i + sentense_size])\n",
    "    y.append(text[i + sentense_size])"
   ]
  },
  {
   "source": [
    "## Transforming new texts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to keep the sentences of indexes \n",
    "new_text_split = []\n",
    "# Loop and get the indexes from dictionary\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        ix = wd_to_index[wd]\n",
    "        sent_split.append(ix)\n",
    "    new_text_split.append(sent_split)"
   ]
  },
  {
   "source": [
    "# Exercise IV: Getting used to text data\n",
    "\n",
    "In this exercise, you will play with text data by analyzing quotes from Sheldon Cooper in The Big Bang Theory TV show. This will give you a chance to analyze sentences to obtain insights on what it's like to deal with real-world text data.\n",
    "\n",
    "You will use dictionary comprehensions to create dictionaries that map words to indexes and vice versa. The use of dictionaries instead of, for example, a `pandas.DataFrame` is because they are more intuitive and don't add unnecessary extra complexity.\n",
    "\n",
    "The data is available in `sheldon_quotes` with the first two sentences already printed for you.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- `join` the sentences into one variable and then extract all the words and store this list in `all_words`.\n",
    "- Remove the duplicated words by applying `list(set())` on the list of words and store them in `unique_words`.\n",
    "- Create a dictionary with indexes as keys and words as values using dictionary comprehensions.\n",
    "- Create a dictionary with words as keys and indexes as values using dictionary comprehensions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the list of sentences into a list of words\n",
    "all_words = ' '.join(sheldon_quotes).split(' ')\n",
    "\n",
    "# Get number of unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Dictionary of indexes as keys and words as values\n",
    "index_to_word = {i:wd for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(index_to_word)\n",
    "\n",
    "# Dictionary of words as keys and indexes as values\n",
    "word_to_index = {wd:i for i,wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "source": [
    "# Exercise V: Preparing text data for model input\n",
    "Previously, you learned how to create dictionaries of indexes to words and vice versa. In this exercise, you will split the text by characters and continue to prepare the data for supervised learning.\n",
    "\n",
    "Splitting the texts into characters may seem strange, but it is often done for text generation. Also, the process to prepare the data is the same, the only change is how to split the texts.\n",
    "\n",
    "You will create the training data containing a list of fixed-length texts and their labels, which are the corresponding next characters.\n",
    "\n",
    "You will continue to use the dataset containing quotes from Sheldon (The Big Bang Theory), available in the `sheldon_quotes` variable.\n",
    "\n",
    "The `print_examples()` function print the pairs so you can see how the data was transformed. Use `help()` for details.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Define `step` equal to `2` and `chars_window` equal to `10`.\n",
    "- Append the next sentence to the variable `sentences`.\n",
    "- Append the correct position of the text `sheldon` to the variable `next_chars`.\n",
    "- Use the `print_examples()` function to print `10` sentences and next characters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to keep the sentences and the next character\n",
    "sentences = []   # ~ Training data\n",
    "next_chars = []  # ~ Training labels\n",
    "\n",
    "# Define hyperparameters\n",
    "step = 2          # ~ Step to take when reading the texts in characters\n",
    "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
    "\n",
    "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
    "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
    "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
    "    next_chars.append(sheldon_quotes[i + chars_window])\n",
    "\n",
    "# Print 10 pairs\n",
    "print_examples(sentences, next_chars, 10)"
   ]
  },
  {
   "source": [
    "# Exercise VI: Transforming new text\n",
    "In this exercise, you will transform a new text into sequences of numerical indexes on the dictionaries created before.\n",
    "\n",
    "This is useful when you already have a trained model and want to apply it on a new dataset. The preprocessing steps done on the training data should also be applied to the new text, so the model can make predictions/classifications.\n",
    "\n",
    "Here, you will also use a special token `'<UKN/>'` to represent words that are not in the vocabulary. Typically, these special tokens are the first indexes of the dictionaries, the position `0`.\n",
    "\n",
    "The variables `word_to_index`, `index_to_word` and `vocabulary` are already loaded in the environment. Also, the variable with the new text is also loaded as `new_text`. The new text has been printed for you to have a look.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Loop through the list `new_text` containing the sentences.\n",
    "- Set to `0` the index in case the word is not found in the dictionary.\n",
    "- Append the sentence with indexes to the variable `new_text_split`.\n",
    "- Convert the indexes back to text using the dictionary `index_to_word`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the sentences and get indexes\n",
    "new_text_split = []\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        index = word_to_index.get(wd, 0)\n",
    "        sent_split.append(index)\n",
    "    new_text_split.append(sent_split)\n",
    "\n",
    "# Print the first sentence's indexes\n",
    "print(new_text_split[0])\n",
    "\n",
    "# Print the sentence converted using the dictionary\n",
    "print(' '.join([index_to_word[index] for index in new_text_split[0]]))"
   ]
  }
 ]
}