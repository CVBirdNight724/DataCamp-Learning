{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Recurrent Neural Networks and Keras\n",
    "\n",
    "In this chapter, you will learn the foundations of Recurrent Neural Networks (RNN). Starting with some prerequisites, continuing to understanding how information flows through the network and finally seeing how to implement such models with Keras in the sentiment classification task."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Introduction to the course\n",
    "\n",
    "## Text data is avaliable online\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135329.png\">\n",
    "\n",
    "## Applications of machine learning to text data\n",
    "Four applications:\n",
    "\n",
    "- Sentiment analysis\n",
    "- Multi-class classification\n",
    "- Text generation\n",
    "- Machine neural translation\n",
    "\n",
    "## Setiment analysis\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135621.png\">\n",
    "\n",
    "## Multi-class classification\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135718.png\">\n",
    "\n",
    "## Text generation\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135759.png\">\n",
    "\n",
    "## Neural machine translation\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135844.png\">\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 135932.png\">\n",
    "\n",
    "## Sequence to sequence models\n",
    "**Many to one: classification**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140041.png\">\n",
    "\n",
    "**Many to many: text generation**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140152.png\">\n",
    "\n",
    "**Many to many: neural machine translation**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140252.png\">\n",
    "\n",
    "**Many to many: language model**\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-03 140416.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Comparing the number of parameter of RNN and ANN\n",
    "\n",
    "In this exercise, you will compare the number of parameters of an artificial neural network (ANN) with the recurrent neural network (RNN) architectures. Here, the vocabulary size is equal to `10,000` for both models.\n",
    "\n",
    "The models have been defined for you with similar architectures of only one layer with `256` units (Dense or RNN) plus the output layer. They are stored on variables `ann_model` and `rnn_model`.\n",
    "\n",
    "Use the method `.summary()` to print the models' architecture and number of parameters and select the correct statement.\n",
    "\n",
    "### Posible Answers\n",
    "\n",
    "- The ANN model has more parameters on the second `Dense` layer than the RNN model.\n",
    "\n",
    "- The RNN model has fewer parameters than the ANN model. (T)\n",
    "\n",
    "- The RNN model needs to train approximately the same number of parameters as the ANN model.\n",
    "\n",
    "- The one-hot encoding allows the RNN model to have fewer parameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise II: Sentiment analysis\n",
    "\n",
    "In the video exercise, you were exposed to the various applications of sequence to sequence models. In this exercise you will see how to use a pre-trained model for sentiment analysis.\n",
    "\n",
    "The model is pre-loaded in the environment on variable `model`. Also, the tokenized test set variables `X_test` and `y_test` and the pre-processed original text data `sentences` from IMDb are also available.You will learn how to pre-process the text data and how to create and train the model using Keras later in the course.\n",
    "\n",
    "You will use the pre-trained model to obtain predictions of sentiment. The model returns a number between zero and one representing the probability of the sentence to have a positive sentiment. So, you will create a decision rule to set the prediction to positive or negative.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use the `.predict()` method to make predictions on the test data.\n",
    "- Make the prediction equal to `\"positive\"` if its value is greater than 0.5 and `\"negative\"` otherwise and store the result in the `pred_sentiment` variable.\n",
    "- Create a `pd.DataFrame` containing the pre-processed text, the prediction obtained in the previous step and their true values contained in the `y_test` variable.\n",
    "- Print the first rows using the `.head()` method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first sentence on `X_test`\n",
    "print(X_test[0])\n",
    "\n",
    "# Get the predicion for all the sentences\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Transform the predition into positive (> 0.5) or negative (<= 0.5)\n",
    "pred_sentiment = [\"positive\" if x>0.5 else \"negative\" for x in pred]\n",
    "\n",
    "# Create a data frame with sentences, predictions and true values\n",
    "result = pd.DataFrame({'sentence': sentences, 'y_pred': pred_sentiment, 'y_true': y_test})\n",
    "\n",
    "# Print the first lines of the data frame\n",
    "print(result.head())"
   ]
  },
  {
   "source": [
    "# Exercise III: Sequence to sequence models\n",
    "\n",
    "In the video exercise, you learned about four types of sequence to sequence models: many-to-one (classification) and many-to-many (text generation, neural machine translation and language models). In this exercise, you have to choose the correct type of model given the following problem description:\n",
    "\n",
    "You are helping your friend who is a specialist in speech recognition. Your friend built a model that can recognize different accents of English, but the model is failing to distinguish homophones - words with the same pronunciation but have different meaning such as \"sea\" vs \"see\" or \"write\" vs \"right\".\n",
    "\n",
    "You propose to use a model that will use the context around the words to identify the semantic meaning of the words. By learning the meaning of the words, the new model would avoid outputs like \"Did you sea that car?\" - it would identify that in this case, the correct word would be \"see\".\n",
    "\n",
    "What type of sequence-to-sequence model is appropriate?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "- Many-to-many, because it is a classification model.\n",
    "\n",
    "- Many-to-one, because it is a classification model.\n",
    "\n",
    "- Many-to-many, this problem can be solved with a language model. (T)\n",
    "\n",
    "- Many-to-one, because it is a prediction problem.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (2) Intruction to language models\n",
    "\n",
    "## Sentence probability\n",
    "Many available models\n",
    "\n",
    "- Probability of \"I loved this movie\"\n",
    "- Unigram\n",
    "    - $$P(sentence) = P('I') P('loved') P('this') P('movie')$$\n",
    "- N-gram\n",
    "    - N = 2 (biagram): $$P(sentense) = P('I') P('loved'|'I') P('this'|'loved') P('movie'|'this')$$\n",
    "    - N = 3 (trigram): $$P(sentense) = P('I') P('loved'|'I') P('this'|'I loved') P('movie'|'loved this')$$\n",
    "\n",
    "- Skip gram\n",
    "    - $$P(sentense) = P('context of I'|'I') P('context of loved'|'loved') P('context of this'|'this') P('context of movie'|'movie')\n",
    "\n",
    "- Neural Networks\n",
    "    - The probability of the sentence is given by a `softmax` function on the output layer of the network\n",
    "\n",
    "## Link to RNNs\n",
    "Language models are everywhere in RNNs!\n",
    "\n",
    "- The network itself\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 111619.png'>\n",
    "</p>\n",
    "\n",
    "- Embedding layer\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 111759.png'>\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Buiilding vocabulary dictionaries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique words\n",
    "unique_words = list(set(text.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary: word is key, index is value\n",
    "word_to_index = {k:v for (v,k) in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary: word is key, word is value\n",
    "index_to_word = {k:v for (k,v) in enumerate(unique_words)}"
   ]
  },
  {
   "source": [
    "## Preprocessing input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize varizbles X and y\n",
    "X = []\n",
    "y = []\n",
    "# Loop over the text: length 'sentence_size' per time with step equal to 'step'\n",
    "for i in range(0, len(text) - sentense_size, step):\n",
    "    X.append(text[i:i + sentense_size])\n",
    "    y.append(text[i + sentense_size])"
   ]
  },
  {
   "source": [
    "## Transforming new texts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to keep the sentences of indexes \n",
    "new_text_split = []\n",
    "# Loop and get the indexes from dictionary\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        ix = wd_to_index[wd]\n",
    "        sent_split.append(ix)\n",
    "    new_text_split.append(sent_split)"
   ]
  },
  {
   "source": [
    "# Exercise IV: Getting used to text data\n",
    "\n",
    "In this exercise, you will play with text data by analyzing quotes from Sheldon Cooper in The Big Bang Theory TV show. This will give you a chance to analyze sentences to obtain insights on what it's like to deal with real-world text data.\n",
    "\n",
    "You will use dictionary comprehensions to create dictionaries that map words to indexes and vice versa. The use of dictionaries instead of, for example, a `pandas.DataFrame` is because they are more intuitive and don't add unnecessary extra complexity.\n",
    "\n",
    "The data is available in `sheldon_quotes` with the first two sentences already printed for you.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- `join` the sentences into one variable and then extract all the words and store this list in `all_words`.\n",
    "- Remove the duplicated words by applying `list(set())` on the list of words and store them in `unique_words`.\n",
    "- Create a dictionary with indexes as keys and words as values using dictionary comprehensions.\n",
    "- Create a dictionary with words as keys and indexes as values using dictionary comprehensions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the list of sentences into a list of words\n",
    "all_words = ' '.join(sheldon_quotes).split(' ')\n",
    "\n",
    "# Get number of unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Dictionary of indexes as keys and words as values\n",
    "index_to_word = {i:wd for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(index_to_word)\n",
    "\n",
    "# Dictionary of words as keys and indexes as values\n",
    "word_to_index = {wd:i for i,wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "source": [
    "# Exercise V: Preparing text data for model input\n",
    "Previously, you learned how to create dictionaries of indexes to words and vice versa. In this exercise, you will split the text by characters and continue to prepare the data for supervised learning.\n",
    "\n",
    "Splitting the texts into characters may seem strange, but it is often done for text generation. Also, the process to prepare the data is the same, the only change is how to split the texts.\n",
    "\n",
    "You will create the training data containing a list of fixed-length texts and their labels, which are the corresponding next characters.\n",
    "\n",
    "You will continue to use the dataset containing quotes from Sheldon (The Big Bang Theory), available in the `sheldon_quotes` variable.\n",
    "\n",
    "The `print_examples()` function print the pairs so you can see how the data was transformed. Use `help()` for details.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Define `step` equal to `2` and `chars_window` equal to `10`.\n",
    "- Append the next sentence to the variable `sentences`.\n",
    "- Append the correct position of the text `sheldon` to the variable `next_chars`.\n",
    "- Use the `print_examples()` function to print `10` sentences and next characters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to keep the sentences and the next character\n",
    "sentences = []   # ~ Training data\n",
    "next_chars = []  # ~ Training labels\n",
    "\n",
    "# Define hyperparameters\n",
    "step = 2          # ~ Step to take when reading the texts in characters\n",
    "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
    "\n",
    "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
    "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
    "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
    "    next_chars.append(sheldon_quotes[i + chars_window])\n",
    "\n",
    "# Print 10 pairs\n",
    "print_examples(sentences, next_chars, 10)"
   ]
  },
  {
   "source": [
    "# Exercise VI: Transforming new text\n",
    "In this exercise, you will transform a new text into sequences of numerical indexes on the dictionaries created before.\n",
    "\n",
    "This is useful when you already have a trained model and want to apply it on a new dataset. The preprocessing steps done on the training data should also be applied to the new text, so the model can make predictions/classifications.\n",
    "\n",
    "Here, you will also use a special token `'<UKN/>'` to represent words that are not in the vocabulary. Typically, these special tokens are the first indexes of the dictionaries, the position `0`.\n",
    "\n",
    "The variables `word_to_index`, `index_to_word` and `vocabulary` are already loaded in the environment. Also, the variable with the new text is also loaded as `new_text`. The new text has been printed for you to have a look.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Loop through the list `new_text` containing the sentences.\n",
    "- Set to `0` the index in case the word is not found in the dictionary.\n",
    "- Append the sentence with indexes to the variable `new_text_split`.\n",
    "- Convert the indexes back to text using the dictionary `index_to_word`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the sentences and get indexes\n",
    "new_text_split = []\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        index = word_to_index.get(wd, 0)\n",
    "        sent_split.append(index)\n",
    "    new_text_split.append(sent_split)\n",
    "\n",
    "# Print the first sentence's indexes\n",
    "print(new_text_split[0])\n",
    "\n",
    "# Print the sentence converted using the dictionary\n",
    "print(' '.join([index_to_word[index] for index in new_text_split[0]]))"
   ]
  },
  {
   "source": [
    "# (3) Introduction to RNN inside Keras\n",
    "\n",
    "## What is keras?\n",
    "- High-level API\n",
    "- Can use Tensorflow, CNTK or Theano frameworks\n",
    "- Easy to install and use\n",
    "\n",
    "```\n",
    "$pip install keras\n",
    "```\n",
    "\n",
    "Fast experimentation:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "source": [
    "## Keras.models\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 120856.png'>\n",
    "</p>\n",
    "\n",
    "## keras.layers\n",
    "1. `LSTM`\n",
    "2. `GRU`\n",
    "3. `Dense`\n",
    "\n",
    "## keras.preprocessing\n",
    "\n",
    "`keras.preprocessing.sequence.pad_sequences(texts, maxlen=3)\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-11 121339.png'>\n",
    "</p>\n",
    "\n",
    "## keras.datasets\n",
    "**Many useful datasets**\n",
    "- IMDB Movie reviews\n",
    "- Reuters newswire\n",
    "\n",
    "And more!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creating a model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model class\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the layers\n",
    "model.add(Dense(64, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "## Training the model\n",
    "\n",
    "The method `.fit()` trains the model on the training set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_siz=32)"
   ]
  },
  {
   "source": [
    "1. `epochs` determine how many weight updates will be done on the model\n",
    "2. `batch_size` size of the data on each step"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model evaluation and usage\n",
    "**Evaluation and usage**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "source": [
    "**Make predictions on new data:**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(new_data)"
   ]
  },
  {
   "source": [
    "## Full example: IMDB Sentiment Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 120))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "score, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "source": [
    "# Exercise VII: Keras models\n",
    "\n",
    "In this exercise you'll practice using two classes from the `keras.models` module. You will create one model using the two classes `Sequential` and `Model`.\n",
    "\n",
    "The `Sequential` class is easier since the layers are assumed to be in order, while the `Model` class is more flexible and allows multiple inputs, multiple outputs and shared layers (shared weights).\n",
    "\n",
    "The `Model` class needs to explicitly declare the input layer, while in the `Sequential` class, this is done with the `input_shape` parameter.\n",
    "\n",
    "The objects and modules `Sequential`, `Model`, `Dense`, `Input`, `LSTM` and `np` (`numpy`) are already loaded on the environment.\n",
    "\n",
    "### Instructions 1/2\n",
    "\n",
    "- Instantiate the `Sequential` model with name `sequential_model`.\n",
    "- Add a `LSTM` and a `Dense` layers, and print the summary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Instantiate the class\n",
    "model = Sequential(name=\"sequential_model\")\n",
    "\n",
    "# One LSTM layer (defining the input shape because it is the \n",
    "# initial layer)\n",
    "model.add(LSTM(128, input_shape=(None, 10), name=\"LSTM\"))\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
    "\n",
    "# The summary shows the layers and the number of parameters \n",
    "# that will be trained\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "### Intructions 2/2\n",
    "\n",
    "- Create an `Input` layer, add `LSTM` and `Dense` layers and store in `main_output`.\n",
    "- Instantiate the model and print its summary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "main_input = Input(shape=(None, 10), name=\"input\")\n",
    "\n",
    "# One LSTM layer (input shape is already defined)\n",
    "lstm_layer = LSTM(128, name=\"LSTM\")(main_input)\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "main_output = Dense(1, activation=\"sigmoid\", name=\"output\")(lstm_layer)\n",
    "\n",
    "# Instantiate the class at the end\n",
    "model = Model(inputs=main_input, outputs=main_output, name=\"modelclass_model\")\n",
    "\n",
    "# Same amount of parameters to train as before (71,297)\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "# Exercise VIII: Keras preprocessing\n",
    "\n",
    "The second most important module of Keras is `keras.preprocessing`. You will see how to use the most important modules and functions to prepare raw data to the correct input shape. Keras provides functionalities that substitute the dictionary approach you learned before.\n",
    "\n",
    "You will use the module `keras.preprocessing.text.Tokenizer` to create a dictionary of words using the method `.fit_on_texts()` and change the texts into numerical ids representing the index of each word on the dictionary using the method `.texts_to_sequences()`.\n",
    "\n",
    "Then, use the function `.pad_sequences()` from `keras.preprocessing.sequence` to make all the sequences have the same size (necessary for the model) by adding zeros on the small texts and cutting the big ones.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import `Tokenizer` and `pad_sequences` from relevant modules.\n",
    "- Fit the `tokenizer` object on the sample data stored in `texts`.\n",
    "- Transform the texts into sequences of numerical indexes using the method `.texts_to_sequences()`.\n",
    "- Fix the size of the texts by padding them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant classes/functions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Build the dictionary of indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Change texts into sequence of indexes\n",
    "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
    "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n",
    "\n",
    "# Pad the sequences\n",
    "texts_pad = pad_sequences(texts_numeric, 60)\n",
    "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))"
   ]
  },
  {
   "source": [
    "# Exercise IX: Your first RNN model\n",
    "\n",
    "In this exercise you will put in practice the Keras modules to build your first `RNN` model and use it to classify sentiment on movie reviews.\n",
    "\n",
    "This first model has one recurrent layer with the vanilla `RNN` cell: `SimpleRNN`, and the output layer with two possible values: `0` representing negative sentiment and `1` representing positive sentiment.\n",
    "\n",
    "You will use the `IMDB` dataset contained in `keras.datasets`. A model was already trained and its weights stored in the file `model_weights.h5`. You will build the model's architecture and use the pre-loaded variables `x_test` and `y_test` to check the its performance.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Add the `SimpleRNN` cell with `128` units.\n",
    "- Add a `Dense` layer with one unit for sentiment classification.\n",
    "- Use the proper loss function for binary classification.\n",
    "- Evaluate the model on the pre-trained validation set: `(x_test, y_test)`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=128, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Method '.evaluate()' shows the loss and accuracy\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Loss: {0} \\nAccuracy: {1}\".format(loss, acc))"
   ]
  }
 ]
}