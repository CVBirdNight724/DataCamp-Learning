{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Fine-tuning your model\n",
    "\n",
    "Having trained your model, your next task is to evaluate its performance. In this chapter, you will learn about some of the other metrics available in scikit-learn that will allow you to assess your model's performance in a more nuanced manner. Next, learn to optimize your classification and regression models using hyperparameter tuning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) How good is your model?\n",
    "\n",
    "## Classification metrics\n",
    "- Measuring model performance with accuracy\n",
    "    - Fraction of correctly classified samples\n",
    "\n",
    "## Class imbalence example: Emails\n",
    "- Spam classification\n",
    "    - 99% of emails are real; 1% of emails are spam\n",
    "- Could build a classifier that predicts ALL emails as real\n",
    "    - 99% accurate\n",
    "    - But horrible at actually classifying spam\n",
    "    - Fails at its original purpose\n",
    "- Need more nuanced metrics\n",
    "\n",
    "## Diagnosing classification predictions\n",
    "- Confusion matrix\n",
    "\n",
    "| | Predicted: | Predicted: |\n",
    "| :-: | :-: | :-: |\n",
    "| | Spam Email | Real Email |\n",
    "| Actual: Spam Email | True Positive | False Negative |\n",
    "| Acutal: Real Email | False Positive | True Negative |\n",
    "\n",
    "- Accuracy:\n",
    "$$\\frac{tp + tn}{tp + tn + fp + fn}$$\n",
    "\n",
    "## Metrics from the confusion matrix\n",
    "- Precision $\\frac{tp}{tp + fp}$\n",
    "- Recall $\\frac{tp}{tp + fn}$\n",
    "- F1score: $2\\cdot \\frac{precision * recall}{precision + recall}$\n",
    "- High precision: Not many real emails predicted as spam\n",
    "- High recall: Predicted most spam emails correctly\n",
    "\n",
    "## Confusion matrix in scikit-learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "X_trian, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "# Exercise I: Metrics for classification\n",
    "\n",
    "In Chapter 1, you evaluated the performance of your k-NN classifier based on its accuracy. However, as Andy discussed, accuracy is not always an informative metric. In this exercise, you will dive more deeply into evaluating the performance of binary classifiers by computing a confusion matrix and generating a classification report.\n",
    "\n",
    "You may have noticed in the video that the classification report consisted of three rows, and an additional support column. The support gives the number of samples of the true response that lie in that class - so in the video example, the support was the number of Republicans or Democrats in the test set on which the classification report was computed. The precision, recall, and f1-score columns, then, gave the respective metrics for that particular class.\n",
    "\n",
    "Here, you'll work with the [PIMA Indians](https://www.kaggle.com/uciml/pima-indians-diabetes-database) dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and number of pregnancies. Therefore, it is a binary classification problem. A target value of `0` indicates that the patient does not have diabetes, while a value of `1` indicates that the patient does have diabetes. As in Chapters 1 and 2, the dataset has been preprocessed to deal with missing values.\n",
    "\n",
    "The dataset has been loaded into a DataFrame `df` and the feature and target variable arrays `X` and `y` have been created for you. In addition, `sklearn.model_selection.train_test_split` and `sklearn.neighbors.KNeighborsClassifier` have already been imported.\n",
    "\n",
    "Your job is to train a k-NN classifier to the data and evaluate its performance by generating a confusion matrix and classification report.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import `classification_report` and `confusion_matrix` from `sklearn.metrics`.\n",
    "- Create training and testing sets with 40% of the data used for testing. Use a random state of `42`.\n",
    "- Instantiate a k-NN classifier with `6` neighbors, fit it to the training data, and predict the labels of the test set.\n",
    "- Compute and print the confusion matrix and classification report using the `confusion_matrix()` and `classification_report()` functions.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Instantiate a k-NN classifier: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "source": [
    "# (2) Logistic regression and the ROC curve\n",
    "\n",
    "## Logistic regression for binary classification\n",
    "- Logistic regression output probabilities\n",
    "- If the probability 'p' is greater than 0.5:\n",
    "    - The data is labeled '1'\n",
    "- If the probability 'p' is less than 0.5:\n",
    "- The data is labeled '0'\n",
    "\n",
    "## Linear decision boundary\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-02 015911.png\">\n",
    "\n",
    "## Logistic regression in scikit-learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "logreg = LogisticRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "source": [
    "## Probability thresholds\n",
    "- By default, logistic regression threshold = 0.5\n",
    "- Not specific to logistic regression\n",
    "\n",
    "## The ROC curve\n",
    "\n",
    "<img src=\"image/Screenshot 2021-02-02 020337.png\">\n",
    "\n",
    "## Plot the ROC curve"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "<img src=\"image/Screenshot 2021-02-02 020732.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "source": [
    "# Exercise II: Building a logistic regression model\n",
    "\n",
    "Time to build your first logistic regression model! As Hugo showed in the video, scikit-learn makes it very easy to try different models, since the Train-Test-Split/Instantiate/Fit/Predict paradigm applies to all classifiers and regressors - which are known in scikit-learn as 'estimators'. You'll see this now for yourself as you train a logistic regression model on exactly the same data as in the previous exercise. Will it outperform k-NN? There's only one way to find out!\n",
    "\n",
    "The feature and target variable arrays `X` and `y` have been pre-loaded, and `train_test_split` has been imported for you from `sklearn.model_selection`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import:\n",
    "    - `LogisticRegression` from `sklearn.linear_model`.\n",
    "    - `confusion_matrix` and `classification_report` from `sklearn.metrics`.\n",
    "- Create training and test sets with 40% (or `0.4`) of the data used for testing. Use a random state of `42`. This has been done for you.\n",
    "- Instantiate a `LogisticRegression` classifier called `logreg`.\n",
    "- Fit the classifier to the training data and predict the labels of the test set.\n",
    "- Compute and print the confusion matrix and classification report. This has been done for you, so hit 'Submit Answer' to see how logistic regression compares to k-NN!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "# Create the classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "source": [
    "# Exercise III: Plotting an ROC curve\n",
    "\n",
    "Great job in the previous exercise - you now have a new addition to your toolbox of classifiers!\n",
    "\n",
    "Classification reports and confusion matrices are great methods to quantitatively evaluate model performance, while ROC curves provide a way to visually evaluate models. As Hugo demonstrated in the video, most classifiers in scikit-learn have a `.predict_proba()` method which returns the probability of a given sample being in a particular class. Having built a logistic regression model, you'll now evaluate its performance by plotting an ROC curve. In doing so, you'll make use of the `.predict_proba()` method and become familiar with its functionality.\n",
    "\n",
    "Here, you'll continue working with the PIMA Indians diabetes dataset. The classifier has already been fit to the training data and is available as `logreg`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import `roc_curve` from `sklearn.metrics`.\n",
    "- Using the `logreg` classifier, which has been fit to the training data, compute the predicted probabilities of the labels of the test set `X_test`. Save the result as `y_pred_prob`.\n",
    "- Use the `roc_curve()` function with `y_test` and `y_pred_prob` and unpack the result into the variables `fpr`, `tpr`, and `thresholds`.\n",
    "- Plot the ROC curve with `fpr` on the x-axis and `tpr` on the y-axis.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Plot\n",
    "<img src=\"image/2021-02-02-022118.svg\" width=50%>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise IV: Precision-recall Curve\n",
    "\n",
    "When looking at your ROC curve, you may have noticed that the y-axis (True positive rate) is also known as recall. Indeed, in addition to the ROC curve, there are other ways to visually evaluate model performance. One such way is the precision-recall curve, which is generated by plotting the precision and recall for different thresholds. As a reminder, precision and recall are defined as:\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "On the right, a precision-recall curve has been generated for the diabetes dataset. The classification report and confusion matrix are displayed in the IPython Shell.\n",
    "\n",
    "Study the precision-recall curve and then consider the statements given below. Choose the one statement that is **not** true. Note that here, the class is positive (1) if the individual has diabetes.\n",
    "\n",
    "<img src=\"image/2021-02-02-022453.svg\">\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- A recall of 1 corresponds to a classifier with a low threshold in which all females who contract diabetes were correctly classified as such, at the expense of many misclassifications of those who did not have diabetes.\n",
    "\n",
    "- Precision is undefined for a classifier which makes no positive predictions, that is, classifies everyone as not having diabetes.\n",
    "\n",
    "- When the threshold is very close to 1, precision is also 1, because the classifier is absolutely certain about its predictions.\n",
    "\n",
    "- Precision and recall take true negatives into consideration. (T)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}