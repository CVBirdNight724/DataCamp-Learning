{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "#  Clustering for dataset exploration\n",
    "\n",
    "Learn how to discover the underlying groups (or \"clusters\") in a dataset. By the end of this chapter, you'll be clustering companies using their stock market prices, and distinguishing different species by clustering their measurements. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Unsupervised Learning\n",
    "\n",
    "## Unsupervised Learning\n",
    "- Unsupervised learning finds patterns in data\n",
    "- E.g., clustering customers by their purchaces\n",
    "- Compressigng the data using purchaces patterns (dimension reduction)\n",
    "\n",
    "## Supervised vs unsupervised learning\n",
    "- Supervised learning finds patterns for a prediction task\n",
    "- E.g., classify tumors as benign or cancerous (labels)\n",
    "- Unsupervised learning finds patterns in data\n",
    "- ... but without a specific prediction task in mind\n",
    "\n",
    "## Iris dataset\n",
    "- Measurements of many iris plants\n",
    "- Three species of iris:\n",
    "    - setosa\n",
    "    - versicolor\n",
    "    - virginica\n",
    "- Petal length, petal width, sepal length (the features of the dataset)\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-18 000035.png'>\n",
    "</p>\n",
    "\n",
    "## Arrays, features & samples\n",
    "- 2D NumPy array\n",
    "- Columns are measurements (the features)\n",
    "- Rows represent iris plants (the samples)\n",
    "\n",
    "## Iris data is 4-dimensional\n",
    "- Iris samples are points in 4 dimensional space\n",
    "- Dimension = number of features\n",
    "- Dimension too high to visualize!\n",
    "- ... but un supervised learning gives insight\n",
    "\n",
    "## k-means clustering\n",
    "- Find clusters of samples\n",
    "- Number of clusters must be spescified\n",
    "- Implemented in `sklearn` (\"scikit-learn\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.predict(samples)"
   ]
  },
  {
   "source": [
    "## Cluster labels for new samples\n",
    "- New samples can be assigned to existing clusters\n",
    "- k-means remembers the mean of each cluster (the \"centroids\")\n",
    "- Finds the nearest centroid to each new sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = model.predict(new_samples)\n",
    "print(new_labels)  "
   ]
  },
  {
   "source": [
    "## Scatter plots\n",
    "- Scatter plot of sepal length vs. petal length\n",
    "- Each point represents an iris sample\n",
    "- Color points by cluster labels\n",
    "- PyPlot (`matplotlib.pyplot`)\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-18 010119.png'>\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matploylib.pyplot as plt\n",
    "xs = samples[:, 0]\n",
    "ys = samples[:, 2]\n",
    "plt.scatter(xs, ys, c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Exercise I: How many clusters?\n",
    "\n",
    "You are given an array `points` of size 300x2, where each row gives the (x, y) co-ordinates of a point on a map. Make a scatter plot of these points, and use the scatter plot to guess how many clusters there are.\n",
    "\n",
    "`matplotlib.pyplot` has already been imported as `plt`. In the IPython Shell:\n",
    "\n",
    "    Create an array called `xs` that contains the values of `points[:,0]` - that is, column `0` of `points`.\n",
    "    Create an array called `ys` that contains the values of `points[:,1]` - that is, column `1` of `points`.\n",
    "    Make a scatter plot by passing `xs` and `ys` to the `plt.scatter()` function.\n",
    "    Call the `plt.show()` function to show your plot.\n",
    "\n",
    "How many clusters do you see?\n",
    "\n",
    "### Instructions\n",
    "#### Possible Answers\n",
    "- 2\n",
    "- 3 (T)\n",
    "- 300"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise II: Clustering 2D points\n",
    "\n",
    "From the scatter plot of the previous exercise, you saw that the points seem to separate into 3 clusters. You'll now create a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise. After the model has been fit, you'll obtain the cluster labels for some new points using the `.predict()` method.\n",
    "\n",
    "You are given the array `points` from the previous exercise, and also an array `new_points`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import `KMeans` from `sklearn.cluster`.\n",
    "- Using `KMeans()`, create a `KMeans` instance called `model` to find `3` clusters. To specify the number of clusters, use the `n_clusters` keyword argument.\n",
    "- Use the `.fit()` method of `model` to fit the model to the array of points `points`.\n",
    "- Use the `.predict()` method of `model` to predict the cluster labels of `new_points`, assigning the result to `labels`.\n",
    "- Hit 'Submit Answer' to see the cluster labels of `new_points`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a KMeans instance with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(points)\n",
    "\n",
    "# Determine the cluster labels of new_points: labels\n",
    "labels = model.predict(new_points)\n",
    "\n",
    "# Print cluster labels of new_points\n",
    "print(labels)\n"
   ]
  },
  {
   "source": [
    "# Exercise III: Inspect your clustering\n",
    "\n",
    "Let's now inspect the clustering you performed in the previous exercise!\n",
    "\n",
    "A solution to the previous exercise has already run, so `new_points` is an array of points and `labels` is the array of their cluster labels.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import `matplotlib.pyplot` as `plt`.\n",
    "- Assign column `0` of `new_points` to `xs`, and column `1` of `new_points` to `ys`.\n",
    "- Make a scatter plot of `xs` and `ys`, specifying the `c=labels` keyword arguments to color the points by their cluster label. Also specify `alpha=0.5`.\n",
    "- Compute the coordinates of the centroids using the `.cluster_centers_` attribute of `model`.\n",
    "- Assign column `0` of `centroids` to `centroids_x`, and column `1` of `centroids` to `centroids_y`.\n",
    "- Make a scatter plot of `centroids_x` and `centroids_y`, using `'D'` (a diamond) as a `marker` by specifying the marker parameter. Set the size of the markers to be `50` using `s=50`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign the columns of new_points: xs and ys\n",
    "xs = new_points[:,0]\n",
    "ys = new_points[:,1]\n",
    "\n",
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "plt.scatter(xs, ys, c=labels, alpha=0.5)\n",
    "\n",
    "# Assign the cluster centers: centroids\n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]\n",
    "\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "plt.scatter(centroids_x, centroids_y, marker='D', s=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "## Plot\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/[2021-02-18] 012831.svg' width=30%>\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (2) Evaluation a clustering\n",
    "\n",
    "## Evaluation a clustering\n",
    "- Can check correspondence with e.g. iris species\n",
    "- ... but what if there are no species to check against?\n",
    "- Measure quality of a clustering\n",
    "- Informs choice of how many clusters to look for\n",
    "\n",
    "## Iris: Clusters vs species\n",
    "- k-means found 3 clusters amongst the iris samples\n",
    "- Do the clusters correspond to the species?\n",
    "\n",
    "| species (label) | setosa | versicolor | virginica |\n",
    "| :-: | :-: | :-: | :-:|\n",
    "| 0 | 0 | 2 | 36 |\n",
    "| 1 | 50 | 0 | 0 |\n",
    "| 2 | 0 | 48 | 14 |\n",
    "\n",
    "## Cross tabulation with pandas\n",
    "- Clusters vs species is a \"cross-tabulation\"\n",
    "- Use the `pandas` library\n",
    "- Given the species of each sample as a list `species`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(species)"
   ]
  },
  {
   "source": [
    "## Alining lables and species"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'labels': labels, 'species': species})\n",
    "print(df)"
   ]
  },
  {
   "source": [
    "| | labels | species |\n",
    "| :-: | :-: | :-: |\n",
    "| 0 | 1 | setosa |\n",
    "| 1 | 1 | setosa |\n",
    "| 2 | 2 | versicolor |\n",
    "| 3 | 3 | virginica |\n",
    "| 4 | 1 | setosa |\n",
    "\n",
    "## Crosstab of labels and species"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab(df['labels'], df['species'])\n",
    "print(ct)"
   ]
  },
  {
   "source": [
    "| species | setosa | versicolor | virginica |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0 | 0 | 2 | 56 |\n",
    "| 1 | 50 | 0 | 0 |\n",
    "| 2 | 0 | 48 | 14 |\n",
    "\n",
    "How to evaluate a clustering, if there were no species information?\n",
    "\n",
    "## Measuring clustering quality\n",
    "- Using only samples and their cluster labels\n",
    "- A good clustering has tight clusters\n",
    "- Samples in each cluster bunched together\n",
    "\n",
    "## Inertia measures clustering quality\n",
    "- Measures how spread out the clusters are (lower is better)\n",
    "- Distance from each sample to centroid of its cluster\n",
    "- After `fit()`, available as attribute `inertia_`\n",
    "- k-means attempts to minimize the inertia when choosing clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(samples)\n",
    "print(model.inertia_)"
   ]
  },
  {
   "source": [
    "## The number of clusters\n",
    "- Clusterings of the iris dataset with different numbers of clusters\n",
    "- More clusters means lowers inertia\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-18 024118.png'>\n",
    "</p>\n",
    "\n",
    "## How many clusters to choose?\n",
    "- A good clustering has tight clusters (so low inertia)\n",
    "- ... but not too many clusters!\n",
    "- Choose an \"elbow\" in the inertia plot\n",
    "- Where inertia begins to decrease more slowly\n",
    "- E.g., for iris dataset, 3 is a good choice"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise IV: How many clusters of grain?\n",
    "\n",
    "In the video, you learned how to choose a good number of clusters for a dataset using the k-means inertia graph. You are given an array `samples` containing the measurements (such as area, perimeter, length, and several others) of samples of grain. What's a good number of clusters in this case?\n",
    "\n",
    "`KMeans` and PyPlot (`plt`) have already been imported for you.\n",
    "\n",
    "This dataset was sourced from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/seeds).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- For each of the given values of `k`, perform the following steps:\n",
    "- Create a `KMeans` instance called `model` with `k` clusters.\n",
    "- Fit the model to the grain data `samples`.\n",
    "- Append the value of the `inertia_` attribute of `model` to the list `inertias`.\n",
    "- The code to plot `ks` vs `inertias` has been written for you, so hit 'Submit Answer' to see the plot!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 6)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(samples)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "## Plot\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/[2021-02-18] 025003.svg'>\n",
    "</p>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KMeans model with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Use fit_predict to fit model and obtain cluster labels: labels\n",
    "labels = model.fit_predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)\n"
   ]
  },
  {
   "source": [
    "# (3) Trnasforming features for better clusterings\n",
    "\n",
    "## Piedmont wines dataset\n",
    "- 178 samples from 3 distinct varieties of red wine: Barolo, Grignolino and Barbera\n",
    "- Features measure chemical composition e.g. alcohol content\n",
    "- Visual properties like \"color intersity\"\n",
    "\n",
    "## Cluster vs. varieties"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "print(ct)"
   ]
  },
  {
   "source": [
    "| varieties | Barbera | Barolo | Grignolino |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0 | 29 | 13 | 20 |\n",
    "| 1 | 0 | 46 | 1 |\n",
    "| 2 | 19 | 0 | 50 |\n",
    "\n",
    "## Feature variances\n",
    "- The wine features have very different variances!\n",
    "- Variance of a feature measures spread of its values\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-18 203600.png'>\n",
    "    <img src='image/Screenshot 2021-02-18 203859.png'>\n",
    "</p>\n",
    "\n",
    "## Standard Scaler\n",
    "- In kmeans: feature varience = feature influence\n",
    "- `StandardScaler` transforms each feature to have mean 0 and varience 1\n",
    "- Features are said to be \"standardized\"\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='image/Screenshot 2021-02-18 205354.png'>\n",
    "</p>\n",
    "\n",
    "## sklearn StandardScaler"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(samples)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "samples_scaled = scaler.transform(smaples)"
   ]
  },
  {
   "source": [
    "## Similar methods\n",
    "- `StandardScaler` and `KMeans` have similar methods\n",
    "- Use `fit()` / `transform()` with `StandardScaler`\n",
    "- Use `fit()` / `predict()` with `KMeans`\n",
    "\n",
    "## StandardScaler, then KMeans\n",
    "- Need to perform two steps: `StandardScaler`, then `KMeans`\n",
    "- Use `sklearn` pipeline to combine multiple steps\n",
    "- Data flows from one step into the next\n",
    "\n",
    "## Pipelines combine multiple steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "pipeline.fit(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pipeline.predict(samples)"
   ]
  },
  {
   "source": [
    "## Feature standardization improves clustering\n",
    "with feature standardization\n",
    "\n",
    "| varieties | Barbera | Barolo | Grignolino |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0 | 0 | 59 | 3 |\n",
    "| 1 | 48 | 0 | 3 |\n",
    "| 2 | 0 | 0 | 65 |\n",
    "\n",
    "Without feature standardization was very bad\n",
    "\n",
    "| varieties | Barbera | Barolo | Grignolino |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 0 | 29 | 13 | 20 |\n",
    "| 1 | 0 | 46 | 1 |\n",
    "| 2 | 19 | 0 | 50 |\n",
    "\n",
    "## sklearn preprocessing steps\n",
    "- `StandardScaler` is a \"preprocessing\" step\n",
    "- `MaxAbsScaler` and `Normalizer` are other examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise V: Scaling fish data for clustering\n",
    "\n",
    "You are given an array `samples` giving measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, you'll need to standardize these features first. In this exercise, you'll build a pipeline to standardize and cluster the data.\n",
    "\n",
    "These fish measurement data were sourced from the [Journal of Statistics Education](ww2.amstat.org/publications/jse/jse_data_archive.htm).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import:\n",
    "    - `make_pipeline`from `sklearn.pipeline`.\n",
    "    - `StandardScaler` from `sklearn.preprocessing`.\n",
    "    - `KMeans` from `sklearn.cluster`.\n",
    "- Create an instance of `StandardScaler` called `scaler`.\n",
    "- Create an instance of `KMeans` with `4` clusters called `kmeans`.\n",
    "- Create a pipeline called `pipeline` that chains `scaler` and `kmeans`. To do this, you just need to pass them in as arguments to `make_pipeline()`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans)\n"
   ]
  },
  {
   "source": [
    "# Exercise VI: Clustering the fish data\n",
    "\n",
    "You'll now use your standardization and clustering pipeline from the previous exercise to cluster the fish by their measurements, and then create a cross-tabulation to compare the cluster labels with the fish species.\n",
    "\n",
    "As before, `samples` is the 2D array of fish measurements. Your `pipeline` is available as pipeline, and the species of every fish sample is given by the list `species`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import `pandas` as `pd`.\n",
    "- Fit the pipeline to the fish measurements `samples`.\n",
    "- Obtain the cluster labels for `samples` by using the `.predict()` method of `pipeline`.\n",
    "- Using `pd.DataFrame()`, create a DataFrame `df` with two columns named `'labels'` and `'species'`, using `labels` and `species`, respectively, for the column values.\n",
    "- Using `pd.crosstab()`, create a cross-tabulation `ct` of `df['labels']` and `df['species']`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the pipeline to samples\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and species as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'species': species})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'], df['species'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)\n"
   ]
  },
  {
   "source": [
    "# Exercise VII: Clustering stocks using KMeans\n",
    "\n",
    "In this exercise, you'll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day). You are given a NumPy array movements of daily price `movements` from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day.\n",
    "\n",
    "Some stocks are more expensive than others. To account for this, include a `Normalizer` at the beginning of your pipeline. The Normalizer will separately transform each company's stock price to a relative scale before the clustering begins.\n",
    "\n",
    "Note that `Normalizer()` is different to `StandardScaler()`, which you used in the previous exercise. While `StandardScaler()` standardizes **features** (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, `Normalizer()` rescales **each sample** - here, each company's stock price - independently of the other.\n",
    "\n",
    "`KMeans` and `make_pipeline` have already been imported for you.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import `Normalizer` from `sklearn.preprocessing`.\n",
    "- Create an instance of `Normalizer` called `normalizer`.\n",
    "- Create an instance of `KMeans` called `kmeans` with `10` clusters.\n",
    "- Using `make_pipeline()`, create a pipeline called `pipeline` that chains `normalizer` and `kmeans`.\n",
    "- Fit the pipeline to the `movements` array.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Normalizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create a normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a KMeans model with 10 clusters: kmeans\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
    "pipeline = make_pipeline(normalizer, kmeans)\n",
    "\n",
    "# Fit pipeline to the daily price movements\n",
    "pipeline.fit(movements)\n"
   ]
  },
  {
   "source": [
    "# Exercise VIII: Which stocks move together?\n",
    "\n",
    "In the previous exercise, you clustered companies by their daily stock price movements. So which company have stock prices that tend to change in the same way? You'll now inspect the cluster labels from your clustering to find out.\n",
    "\n",
    "Your solution to the previous exercise has already been run. Recall that you constructed a Pipeline `pipeline` containing a KMeans model and fit it to the NumPy array `movements` of daily stock movements. In addition, a list `companies` of the company names is available.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Import `pandas` as `pd`.\n",
    "- Use the `.predict()` method of the pipeline to predict the labels for `movements`.\n",
    "- Align the cluster labels with the list of company names `companies` by creating a DataFrame `df` with `labels` and `companies` as columns. This has been done for you.\n",
    "- Use the `.sort_values()` method of `df` to sort the DataFrame by the `'labels'` column, and print the result.\n",
    "- Hit 'Submit Answer' and take a moment to see which companies are together in each cluster!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Predict the cluster labels: labels\n",
    "labels = pipeline.predict(movements)\n",
    "\n",
    "# Create a DataFrame aligning labels and companies: df\n",
    "df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "print(df.sort_values('labels'))\n"
   ]
  }
 ]
}