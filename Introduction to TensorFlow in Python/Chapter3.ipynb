{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "\n",
    "The previous chapters taught you how to build models in TensorFlow 2. In this chapter, you will apply those same tools to build, train, and make predictions with neural networks. You will learn how to define dense layers, apply activation functions, select an optimizer, and apply regularization to reduce overfitting. You will take advantage of TensorFlow's flexibility by using both low-level linear algebra and high-level Keras API operations to define and train models."
   ]
  },
  {
   "source": [
    "# (1) Dense layers\n",
    "\n",
    "## The linear regression model\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-24 232039.png\">\n",
    "\n",
    "## What is a nueral network?\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-24 232132.png\">\n",
    "\n",
    "<imge src=\"image/Screenshot 2021-01-24 232213.png\">\n",
    "\n",
    "A dense layer applies weights to all nodes from the previous layer.\n",
    "\n",
    "## A simple dense layer\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "```\n",
    "\n",
    "```\n",
    "# Define inputs (features)\n",
    "inputs = tf.constants([[1, 35]])\n",
    "```\n",
    "\n",
    "```\n",
    "# Define weights\n",
    "weights = tf.Variable([[-0.05], [-0.01]])\n",
    "```\n",
    "\n",
    "```\n",
    "# Define the bias\n",
    "bias = tf.Variable([0.5])\n",
    "```\n",
    "\n",
    "```\n",
    "# Multiply input (features) by the weights\n",
    "product = tf.matmul(inputs, weights)\n",
    "```\n",
    "\n",
    "```\n",
    "# Define dense layer\n",
    "dense = tf.keras.activations.sigmoid(product+bias)\n",
    "```\n",
    "<img src=\"image/Screenshot 2021-01-24 232706.png\">\n",
    "\n",
    "## Defining a complete model\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "```\n",
    "\n",
    "```\n",
    "# Define input (features) layer\n",
    "inputs = tf.constant(data, tf.float32)\n",
    "```\n",
    "\n",
    "```\n",
    "# Define first dense layer\n",
    "dense1 = tf.keras.layers.Dense(10, activation='sigmod')(input)\n",
    "```\n",
    "\n",
    "```\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(5, activation='sigmod')(dense1)\n",
    "```\n",
    "\n",
    "```\n",
    "# Define output (predictions) layer\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "```\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-24 233110.png\">\n",
    "\n",
    "## High-level versus low-level approach\n",
    "\n",
    "- **High-level approach**\n",
    "    - High-level API operations\n",
    "\n",
    "```\n",
    "dense = keras.layers.Dense(10, activation='sigmoid')\n",
    "```\n",
    "\n",
    "- **Low-level approach**\n",
    "    - Linear-algebraic operations\n",
    "\n",
    "```\n",
    "prod = matmul(inputs, weights)\n",
    "dense = keras.activations.sigmoid(prod)\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: The linear algebra of dense layers\n",
    "\n",
    "There are two ways to define a dense layer in `tensorflow`. The first involves the use of low-level, linear algebraic operations. The second makes use of high-level `keras` operations. In this exercise, we will use the first method to construct the network shown in the image below.\n",
    "\n",
    "<img src=\"image/3_2_1_network2.png\">\n",
    "\n",
    "The input layer contains 3 features -- education, marital status, and age -- which are available as `borrower_features`. The hidden layer contains 2 nodes and the output layer contains a single node.\n",
    "\n",
    "For each layer, you will take the previous layer as an input, initialize a set of weights, compute the product of the inputs and weights, and then apply an activation function. Note that `Variable()`, `ones()`, `matmul()`, and `keras()` have been imported from `tensorflow`.\n",
    "\n",
    "### Instructions \n",
    "\n",
    "- Initialize `weights1` as a variable using a 3x2 tensor of ones.\n",
    "- Compute the product of `borrower_features` by `weights1` using matrix multiplication.\n",
    "- Use a sigmoid activation function to transform `product1 + bias1`.\n",
    "\n",
    "- Initialize `weights2` as a variable using a 2x1 tensor of ones.\n",
    "- Compute the product of `dense1` by `weights2` using matrix multiplication.\n",
    "- Use a sigmoid activation function to transform `product2 + bias2`.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous step\n",
    "bias1 = Variable(1.0)\n",
    "weights1 = Variable(ones((3, 2)))\n",
    "product1 = matmul(borrower_features, weights1)\n",
    "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Initialize bias2 and weights2\n",
    "bias2 = Variable(1.0)\n",
    "weights2 = Variable(ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = matmul(dense1, weights2)\n",
    "\n",
    "# Apply activation to product2 + bias2 and print the prediction\n",
    "prediction = keras.activations.sigmoid(product2 + bias2)\n",
    "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
    "print('\\n actual: 1')"
   ]
  },
  {
   "source": [
    "# Exercise II: The low-level approach with multiple examples\n",
    "\n",
    "In this exercise, we'll build further intuition for the low-level approach by constructing the first dense hidden layer for the case where we have multiple examples. We'll assume the model is trained and the first layer weights, `weights1`, and bias, `bias1`, are available. We'll then perform matrix multiplication of the `borrower_features` tensor by the `weights1` variable. Recall that the `borrower_features` tensor includes education, marital status, and age. Finally, we'll apply the sigmoid function to the elements of `products1 + bias1`, yielding `dense1`.\n",
    "\n",
    "$products1 = \\begin{bmatrix} 3 & 3 & 23 \\\\ 2 & 1 & 24 \\\\ 1 & 1 & 49 \\\\ 1 & 1 & 49 \\\\ 2 & 1 & 49\\end{bmatrix}\\begin{bmatrix} -0.6 & 0.6 \\\\ 0.8 & -0.3 \\\\ -0.09 & -0.08\\end{bmatrix}$  \n",
    "\n",
    "Note that `matmul()` and `keras()` have been imported from `tensorflow`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "\n",
    "- Compute `products1` by matrix multiplying the features tensor by the weights.\n",
    "- Use a sigmoid activation function to transform `products1 + bias1`.\n",
    "- Print the shapes of `borrower_features`, `weights1`, `bias1`, and `dense1`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the product of borrower_features and weights1\n",
    "products1 = matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply a sigmoid activation function to products1 + bias1\n",
    "dense1 = keras.activations.sigmoid(products1 + bias1)\n",
    "\n",
    "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
    "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
    "print('\\n shape of weights1: ', weights1.shape)\n",
    "print('\\n shape of bias1: ', bias1.shape)\n",
    "print('\\n shape of dense1: ', dense1.shape)"
   ]
  },
  {
   "source": [
    "# Exercise III: Using the dense layer operation\n",
    "\n",
    "We've now seen how to define dense layers in `tensorflow` using linear algebra. In this exercise, we'll skip the linear algebra and let `keras` work out the details. This will allow us to construct the network below, which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features.\n",
    "\n",
    "<img src=\"image/10_7_3_1_network.png\">\n",
    "\n",
    "To construct this network, we'll need to define three dense layers, each of which takes the previous layer as an input, multiplies it by weights, and applies an activation function. Note that input data has been defined and is available as a 100x10 tensor: `borrower_features`. Additionally, the `keras.layers` module is available.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "\n",
    "- Set `dense1` to be a dense layer with 7 output nodes and a sigmoid activation function.\n",
    "- Define `dense2` to be dense layer with 3 output nodes and a sigmoid activation function.\n",
    "- Define `predictions` to be a dense layer with 1 output node and a sigmoid activation function.\n",
    "- Print the shapes of `dense1`, `dense2`, and `predictions` in that order using the `.shape` method. Why does each of these tensors have 100 rows?\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first dense layer\n",
    "dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = keras.layers.Dense(3, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node\n",
    "predictions = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print the shapes of dense1, dense2, and predictions\n",
    "print('\\n shape of dense1: ', dense1.shape)\n",
    "print('\\n shape of dense2: ', dense2.shape)\n",
    "print('\\n shape of predictions: ', predictions.shape)"
   ]
  },
  {
   "source": [
    "# (2) Activation functions\n",
    "\n",
    "## What is an activation function?\n",
    "\n",
    "- **Components of a typical hidden layer**\n",
    "    - **Linear**: Matrix multiplication\n",
    "    - **Nonlinear**: Activation function\n",
    "    \n",
    "## Why nonlinearities are important\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-25 000204.png\">\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-25 000254.png\">\n",
    "\n",
    "## A simple example\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define example borrower features\n",
    "young, old = 0.3, 0.6\n",
    "low_bill, high_bill = 0.1, 0.5\n",
    "```\n",
    "\n",
    "```\n",
    "# Apply matrix multiplication step for all feature combinations\n",
    "young_high = 1.0*young + 2.0*high_bill\n",
    "young_low = 1.0*young + 2.0*low_bill\n",
    "old_high = 1.0*old + 2.0*high_bill\n",
    "old_low = 1.0*old + 2.0*low_bill\n",
    "```\n",
    "\n",
    "```\n",
    "# Difference in default predictions for young\n",
    "print(young_high - young_low)\n",
    "\n",
    "# Difference in default predictions for old\n",
    "print(old_high - old_low)\n",
    "```\n",
    "\n",
    "```\n",
    "# Difference in default predictions for young\n",
    "print(tf.keras.activations.sigmoid(young_high).numpy() - \\\n",
    "    tf.keras.activations.sigmoid(young_low).numpy())\n",
    "\n",
    "# Difference in default predictions for old\n",
    "print(tf.keras.activations.sigmoid(old_high).numpy() - \\\n",
    "    tf.keras.activations.sigmoid(old_low).numpy())\n",
    "```\n",
    "\n",
    "## The sigmoid activation function\n",
    "\n",
    "- **Sigmoid activation function**\n",
    "    - Binary classification\n",
    "    - Low-level: `tf.keras.activations.sigmoid()`\n",
    "    - High-level: `sigmoid`\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-25 001511.png\">\n",
    "\n",
    "## The relu activation function\n",
    "\n",
    "- **ReLu activation function**\n",
    "    - Hidden layers\n",
    "    - Low-level: `tf.keras.activations.relu()`\n",
    "    - High-level: `relu`\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-25 001708.png\">\n",
    "\n",
    "## The softmax activation function\n",
    "\n",
    "- **Softmax activaiton function**\n",
    "    - Output layer (>2 classes)\n",
    "    - Low-level: `tf.keras.activation.softmax()`\n",
    "    - High-level: `softmax`\n",
    "\n",
    "## Activation functions in neural networks\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "```\n",
    "\n",
    "```\n",
    "# Define dense layer 1\n",
    "dense1 = tf.keras.layers.Dense(16, activation='relu')(inputs)\n",
    "```\n",
    "\n",
    "```\n",
    "# Define dense layer 2\n",
    "dense2 = tf.keras.layers.Dense(8, activation='sigmoid')(dense1)\n",
    "```\n",
    "\n",
    "```\n",
    "# Define dense layer output\n",
    "outputs = tf.keras.layers.Dense(4, activation='softmax')(dense2)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise IV: Binary classification problems\n",
    "\n",
    "In this exercise, you will again make use of credit card data. The target variable, `default`, indicates whether a credit card holder defaults on his or her payment in the following period. Since there are only two options--default or not--this is a binary classification problem. While the dataset has many features, you will focus on just three: the size of the three latest credit card bills. Finally, you will compute predictions from your untrained network, `outputs`, and compare those the target variable, `default`.\n",
    "\n",
    "The tensor of features has been loaded and is available as `bill_amounts`. Additionally, the `constant()`, `float32`, and `keras.layers.Dense()` operations are available.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "\n",
    "- Define `inputs` as a 32-bit floating point constant tensor using `bill_amounts`.\n",
    "- Set `dense1` to be a dense layer with 3 output nodes and a `relu` activation function.\n",
    "- Set `dense2` to be a dense layer with 2 output nodes and a `relu` activation function.\n",
    "- Set the output layer to be a dense layer with a single output node and a `sigmoid` activation function.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct input layer from features\n",
    "inputs = constant(bill_amounts, float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print error for first five examples\n",
    "error = default[:5] - outputs.numpy()[:5]\n",
    "print(error)"
   ]
  },
  {
   "source": [
    "# Exercise V: Multiclass classification problems\n",
    "\n",
    "In this exercise, we expand beyond binary classification to cover multiclass problems. A multiclass problem has targets that can take on three or more values. In the credit card dataset, the education variable can take on 6 different values, each corresponding to a different level of education. We will use that as our target in this exercise and will also expand the feature set from 3 to 10 columns.\n",
    "\n",
    "As in the previous problem, you will define an input layer, dense layers, and an output layer. You will also print the untrained model's predictions, which are probabilities assigned to the classes. The tensor of features has been loaded and is available as `borrower_features`. Additionally, the `constant()`, `float32`, and `keras.layers.Dense()` operations are available.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "\n",
    "- Define the input layer as a 32-bit constant tensor using `borrower_features`.\n",
    "- Set the first dense layer to have 10 output nodes and a `sigmoid` activation function.\n",
    "- Set the second dense layer to have 8 output nodes and a rectified linear unit activation function.\n",
    "- Set the output layer to have 6 output nodes and the appropriate activation function.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct input layer from borrower features\n",
    "inputs = constant(borrower_features, float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:5])"
   ]
  },
  {
   "source": [
    "# (3) Optimizers\n",
    "\n",
    "## How to find a minimum\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-25 003337.png\">\n",
    "\n",
    "## The gradient descent optimizer\n",
    "\n",
    "- **Stochastic gradient descent (SGD) optimizer**\n",
    "    - `tf.keras.optimizer.SGD()`\n",
    "    - `learing_rate`\n",
    "- **Simple and easy to interpret**\n",
    "\n",
    "## The RMS prop optimizer\n",
    "\n",
    "- ** Root mean squared (RMS) propagation optimizer**\n",
    "    - Applies different learning rates to each feature\n",
    "    - `tf.keras.optimizers.RMSprop()`\n",
    "    - `learning_rate`\n",
    "    - `momentum`\n",
    "    - `decay`\n",
    "- **Allows for momentum to both build and decay\n",
    "\n",
    "## The adam optimizer\n",
    "\n",
    "- **Adaptive moment (adam) optimizer\n",
    "    - `tf.keras.optimizer.Adam()`\n",
    "    - `learning_rate`\n",
    "    - `beta1`\n",
    "- **Performs well with default parameter values**\n",
    "\n",
    "## A complete example\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the model function\n",
    "def model(bias, weights, features=bollower_features):\n",
    "    product = tf.matmul(features, weights)\n",
    "    return tf.keras.activations.sigmoid(product+bias)\n",
    "```\n",
    "\n",
    "```\n",
    "# Compute the predicted values and loss\n",
    "def loss_function(bias, weights, targets=default, features=borrower_features):\n",
    "    return tf.keras.losses.binary_crossentropy(targets, predictions)\n",
    "```\n",
    "\n",
    "```\n",
    "# Minimize the loss function with RMS propagation\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.9)\n",
    "opt.minimize(lamda: loss_function(bias, weights), var_list=[bias, weights])\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise VI: The dangers of local minima\n",
    "\n",
    "Consider the plot of the following loss function, `loss_function()`, which contains a global minimum, marked by the dot on the right, and several local minima, including the one marked by the dot on the left.\n",
    "\n",
    "<img src=\"image/local_minima_dots_4_10.png\">\n",
    "\n",
    "In this exercise, you will try to find the global minimum of `loss_function()` using `keras.optimizers.SGD()`. You will do this twice, each time with a different initial value of the input to `loss_function()`. First, you will use `x_1`, which is a variable with an initial value of 6.0. Second, you will use `x_2`, which is a variable with an initial value of 0.3. Note that `loss_function()` has been defined and is available.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "\n",
    "- Set `opt` to use the stochastic gradient descent optimizer (SGD) with a learning rate of 0.01.\n",
    "- Perform minimization using the `loss function`, loss_function(), and the variable with an initial value of 6.0, `x_1`.\n",
    "- Perform minimization using the loss function, `loss_function()`, and the variable with an initial value of 0.3, `x_2`.\n",
    "- Print `x_1` and `x_2` as `numpy` arrays and check whether the values differ. These are the minima that the algorithm identified.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(6.0,float32)\n",
    "x_2 = Variable(0.3,float32)\n",
    "\n",
    "# Define the optimization operation\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Perform minimization using the loss function and x_1\n",
    "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "\t# Perform minimization using the loss function and x_2\n",
    "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "source": [
    "# Exercise VII: Avoiding local minima\n",
    "\n",
    "The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as `loss_function()`.\n",
    "\n",
    "<img src=\"image/local_minima_dots_4_10.png\">\n",
    "\n",
    "Several optimizers in `tensorflow` have a momentum parameter, including `SGD` and `RMSprop`. You will make use of `RMSprop` in this exercise. Note that `x_1` and `x_2` have been initialized to the same value this time. Furthermore, `keras.optimizers.RMSprop()` has also been imported for you from `tensorflow`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "\n",
    "- Set the `opt_1` operation to use a learning rate of 0.01 and a momentum of 0.99.\n",
    "- Set `opt_2` to use the root mean square propagation (RMS) optimizer with a learning rate of 0.01 and a momentum of 0.00.\n",
    "- Define the minimization operation for `opt_2`.\n",
    "- Print `x_1` and `x_2` as numpy arrays.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(0.05,float32)\n",
    "x_2 = Variable(0.05,float32)\n",
    "\n",
    "# Define the optimization operation for opt_1 and opt_2\n",
    "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "\n",
    "for j in range(100):\n",
    "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Define the minimization operation for opt_2\n",
    "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "source": [
    "# (4) Training a network in TensorFlow\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-25 010036.png\">\n",
    "\n",
    "# Random initializers\n",
    "\n",
    "- **Ofter needed to initialize thousands of the varibles**\n",
    "    - `tf.ones()` may perform poorly\n",
    "    - Tedious and difficult to initialize variables individually\n",
    "- **Alternatively, draw initial values form distribution**\n",
    "    - Normal\n",
    "    - Uniform\n",
    "    - Glorot initializer\n",
    "\n",
    "## Initializing variable in TensorFlow\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define 500x500 random nirmal variable\n",
    "weights = tf.Variable(tf.random.normal([500, 500]))\n",
    "\n",
    "# Define 500x500 truncated random normal variable\n",
    "weights = tf.Variable(tf.random.truncated_normal([500, 500 ]))\n",
    "```\n",
    "\n",
    "```\n",
    "# Define a dense layer with the default initializer\n",
    "dense = tf.keras.layers.Dense(32, activation='relu')\n",
    "\n",
    "# Define a dense layer with the zeros initializer\n",
    "dense = tf.keras.layers.Dense(32, activation='relu', kernel_initializer='zeros')\n",
    "```\n",
    "\n",
    "## Neural Networks and overfitting\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-25 011037.png\">\n",
    "\n",
    "## Apply dropout\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-25 011131.png\">\n",
    "\n",
    "## Implementing dropout in a network\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define input data\n",
    "inputs = np.array(borrower_features, np.float32)\n",
    "```\n",
    "\n",
    "```\n",
    "# Define dense layer 1\n",
    "dense1 = tf.keras.layers.Dense(32, activation='relu')(inputs)\n",
    "```\n",
    "\n",
    "```\n",
    "# Define dense layer 2\n",
    "dense2 = tf.keras.layers.Dense(16, activation='relu')(dense1)\n",
    "```\n",
    "\n",
    "```\n",
    "# Apply dropout operation\n",
    "dropout1 = tf.keras.layers.Dropout(0.25)(dense2)\n",
    "```\n",
    "\n",
    "```\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dropout1)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise VIII: Initialization in TensorFlow\n",
    "\n",
    "A good initialization can reduce the amount of time needed to find the global minimum. In this exercise, we will initialize weights and biases for a neural network that will be used to predict credit card default decisions. To build intuition, we will use the low-level, linear algebraic approach, rather than making use of convenience functions and high-level `keras` operations. We will also expand the set of input features from 3 to 23. Several operations have been imported from `tensorflow`: `Variable()`, `random()`, and `ones()`\n",
    "\n",
    "### Instructions\n",
    "\n",
    "\n",
    "- Initialize the layer 1 weights, `w1`, as a `Variable()` with shape `[23, 7]`, drawn from a normal distribution.\n",
    "- Initialize the layer 1 bias using ones.\n",
    "- Use a draw from the normal distribution to initialize `w2` as a `Variable()` with shape `[7, 1]`.\n",
    "- Define `b2` as a `Variable()` and set its initial value to 0.0.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layer 1 weights\n",
    "w1 = Variable(random.normal([23, 7]))\n",
    "\n",
    "# Initialize the layer 1 bias\n",
    "b1 = Variable(ones([7]))\n",
    "\n",
    "# Define the layer 2 weights\n",
    "w2 = Variable(random.normal([7, 1]))\n",
    "\n",
    "# Define the layer 2 bias\n",
    "b2 = Variable(0)"
   ]
  },
  {
   "source": [
    "# Exercise IX: Defining the model and loss function\n",
    "\n",
    "In this exercise, you will train a neural network to predict whether a credit card holder will default. The features and targets you will use to train your network are available in the Python shell as `borrower_features` and `default`. You defined the weights and biases in the previous exercise.\n",
    "\n",
    "Note that the `predictions layer` is defined as $\\sigma(layer1 * w2 + b2)$ , where $\\sigma$ is the sigmoid activation, `layer1` is a tensor of nodes for the first hidden dense layer, `w2` is a tensor of weights, and `b2` is the bias tensor.\n",
    "\n",
    "The trainable variables are `w1`, `b1`, `w2`, and `b2`. Additionally, the following operations have been imported for you: `keras.activations.relu()` and `keras.layers.Dropout()`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Apply a rectified linear unit activation function to the first layer.\n",
    "- Apply 25% dropout to `layer1`.\n",
    "- Pass the `target`, targets, and the predicted values, `predictions`, to the cross entropy loss function.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def model(w1, b1, w2, b2, features = borrower_features):\n",
    "\t# Apply relu activation functions to layer 1\n",
    "\tlayer1 = keras.activations.relu(matmul(features, w1) + b1)\n",
    "    # Apply dropout\n",
    "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
    "\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
    "\tpredictions = model(w1, b1, w2, b2)\n",
    "\t# Pass targets and predictions to the cross entropy loss\n",
    "\treturn keras.losses.binary_crossentropy(targets, predictions)"
   ]
  },
  {
   "source": [
    "# Exercise X: Training neural networks with TensorFlow\n",
    "\n",
    "In the previous exercise, you defined a model, `model(w1, b1, w2, b2, features)`, and a loss function, `loss_function(w1, b1, w2, b2, features, targets)`, both of which are available to you in this exercise. You will now train the model and then evaluate its performance by predicting default outcomes in a test set, which consists of `test_features` and `test_targets` and is available to you. The trainable variables are `w1`, `b1`, `w2`, and `b2`. Additionally, the following operations have been imported for you: `keras.activations.relu()` and `keras.layers.Dropout()`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "\n",
    "- Set the optimizer to perform minimization.\n",
    "- Add the four trainable variables to `var_list` in the order in which they appear as arguments to `loss_function()`.\n",
    "- Use the model and `test_features` to predict the values for `test_targets`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for j in range(100):\n",
    "    # Complete the optimizer\n",
    "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
    "                 var_list=[w1, b1, w2, b2])\n",
    "\n",
    "# Make predictions with model\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "confusion_matrix(test_targets, model_predictions)"
   ]
  }
 ]
}