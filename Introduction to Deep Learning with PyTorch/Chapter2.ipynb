{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "In this second chapter, we delve deeper into Artificial Neural Networks, learning how to train them with real datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) Activation functions\n",
    "\n",
    "## Motivation\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-26 171633.png\">\n",
    "\n",
    "```\n",
    "input_layer = torch.tensor([2., 1.])\n",
    "weight_1 = torch.tensor([[0.45, 0.32], [-0.12, 0.29]])\n",
    "hidden_layer = torch.matmul(input_layer, weight_1)\n",
    "weight_2 = torch.tensor([[0.48, -0.12], [0.64, 0.91]])\n",
    "output_layer = torch.matmul(hidden_layer, weight_2)\n",
    "print(output_layer)\n",
    "```\n",
    "\n",
    "## Matrix multiplication is a linear tranformation\n",
    "\n",
    "```\n",
    "input_layer = torch.tensor([2., 1.])\n",
    "weight_1 = torch.tensor([[0.45, 0.32], [-0.12, 0.29]])\n",
    "hidden_layer = torch.matmul(input_layer, weight_1)\n",
    "weight_2 = torch.tensor([[0.48, -0.12], [0.64, 0.91]])\n",
    "weight = torch.matmul(weight_1, weight_2)\n",
    "output_layer = torch.matmul(hidden_layer, weight_2)\n",
    "print(output_layer)\n",
    "print(weight)\n",
    "```\n",
    "\n",
    " ## Non linear separable datasets\n",
    "\n",
    " <img src=\"image/Screenshot 2021-01-26 172104.png\">\n",
    "\n",
    " ## Activation functions\n",
    "\n",
    " <img src=\"image/Screenshot 2021-01-26 172208.png\">\n",
    "\n",
    " ## ReLU activation function\n",
    "\n",
    " ```\n",
    " RelU(x) = max(0, x)\n",
    " ```\n",
    "\n",
    " ```\n",
    "import torch\n",
    "relu = nn.ReLU()\n",
    "\n",
    "tensor_1 = torch.tensor([2., 4.])\n",
    "print(relu(tensor_1))\n",
    "\n",
    "tensor_2 = torch.tensor([[2., -4.], [1.2, 0.]])\n",
    "print(relu(tensor_2))\n",
    " ```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Neural networks\n",
    "\n",
    "Let us see the differences between neural networks which apply `ReLU` and those which do not apply `ReLU`. We have already initialized the input called `input_layer`, and three sets of weights, called `weight_1`, `weight_2` and `weight_3`.\n",
    "\n",
    "We are going to convince ourselves that networks with multiple layers which do not contain non-linearity can be expressed as neural networks with one layer.\n",
    "\n",
    "The network and the shape of layers and weights is shown below.\n",
    "\n",
    "<img src=\"image/net-ex.jpg\">\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Calculate the first and second hidden layer by multiplying the appropriate inputs with the corresponding weights.\n",
    "- Calculate and print the results of the output.\n",
    "- Set `weight_composed_1` to the product of `weight_1` with `weight_2`, then set weight to the product of `weight_composed_1` with `weight_3`.\n",
    "- Calculate and print the output.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the first and second hidden layer\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "hidden_2 = torch.matmul(hidden_1, weight_2)\n",
    "\n",
    "# Calculate the output\n",
    "print(torch.matmul(hidden_2, weight_3))\n",
    "\n",
    "# Calculate weight_composed_1 and weight\n",
    "weight_composed_1 = torch.matmul(weight_1, weight_2)\n",
    "weight = torch.matmul(weight_composed_1, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "source": [
    "# Exercise II: ReLU activation\n",
    "\n",
    "n this exercise, we have the same settings as the previous exercise. In addition, we have instantiated the `ReLU` activation function called `relu()`.\n",
    "\n",
    "Now we are going to build a neural network which has non-linearity and by doing so, we are going to convince ourselves that networks with multiple layers and non-linearity functions cannot be expressed as a neural network with one layer.\n",
    "\n",
    "<img src=\"image/net-ex.jpg\">\n",
    "\n",
    "- Apply non-linearity on `hidden_1` and `hidden_2`.\n",
    "- Apply non-linearity in the product of first two weight.\n",
    "- Multiply the result of the previous step with `weight_3`.\n",
    "- Multiply `input_layer` with `weight` and print the results.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply non-linearity on hidden_1 and hidden_2\n",
    "hidden_1_activated = relu(torch.matmul(input_layer, weight_1))\n",
    "hidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))\n",
    "print(torch.matmul(hidden_2_activated, weight_3))\n",
    "\n",
    "# Apply non-linearity in the product of first two weights. \n",
    "weight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))\n",
    "\n",
    "# Multiply `weight_composed_1_activated` with `weight_3\n",
    "weight = torch.matmul(weight_composed_1_activated, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "source": [
    "# Exercise III: ReLU activation again\n",
    "\n",
    "Neural networks don't need to have the same number of units in each layer. Here, you are going to experiment with the `ReLU` activation function again, but this time we are going to have a different number of units in the layers of the neural network. The input layer will still have `4` features, but then the first hidden layer will have `6` units and the output layer will have `2` units.\n",
    "\n",
    "<img src=\"image/net-ex2.jpg\">\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Instantiate the `ReLU()` activation function as `relu` (the function is part of `nn` module).\n",
    "- Initialize `weight_1` and `weight_2` with random numbers.\n",
    "- Multiply the `input_layer` with `weight_1`, storing results in `hidden_1`.\n",
    "- Apply the `relu` activation function over `hidden_1`, and then multiply the output of it with `weight_2`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ReLU activation function as relu\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Initialize weight_1 and weight_2 with random numbers\n",
    "weight_1 = torch.rand(4, 6)\n",
    "weight_2 = torch.rand(6, 2)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Apply ReLU activation function over hidden_1 and multiply with weight_2\n",
    "hidden_1_activated = relu(hidden_1)\n",
    "print(torch.matmul(hidden_1_activated, weight_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}