{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Using Convolutional Neural Networks\n",
    "\n",
    "In this last chapter, we learn how to make neural networks work well in practice, using concepts like regularization, batch-normalization and transfer learning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) The sequential module\n",
    "\n",
    "## AlexNet - declearing the modules\n",
    "\n",
    "```\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        self.relu = nn.Relu(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "```\n",
    "\n",
    "## AlexNet - forward() methods\n",
    "\n",
    "```\n",
    "def forward(self, x):\n",
    "    x = self.relu(self.conv1(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = self.relu(self.conv2(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = self.relu(self.conv3(x))\n",
    "    x = self.relu(self.conv4(x))\n",
    "    x = self.relu(self.conv5(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = self.avgpool(x)\n",
    "    x = x.view(x.size(0), 256 * 6 * 6)\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.relu(self.fc2(x))\n",
    "    return self.fc3(x)\n",
    "```\n",
    "\n",
    "## The sequential modulde - declaring the modules\n",
    "\n",
    "```\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes), )\n",
    "```\n",
    "\n",
    "## The sequential module - forward() method\n",
    "\n",
    "```\n",
    "def forward(self, x):\n",
    "    x = self.features(x)\n",
    "    x = self.avgpool(x)\n",
    "    x = x.view(x.size(0), 256 * 6 * 6)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Sequential module - init method\n",
    "\n",
    "Having learned about the sequential module, now is the time to see how you can convert a neural network that doesn't use sequential modules to one that uses them. We are giving the code to build the network in the usual way, and you are going to write the code for the same network using sequential modules.\n",
    "\n",
    "```\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(7 * 7 * 40, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 10) \n",
    "```\n",
    "\n",
    "We want the pooling layer to be used after the second and fourth convolutional layers, while the relu nonlinearity needs to be used after each layer except the last (fully-connected) layer. For the number of filters (kernels), stride, passing, number of channels and number of units, use the same numbers as above.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Declare all the layers needed for feature extraction in the `self.features`.\n",
    "- Declare the three linear layers in `self.classifier`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Declare all the layers for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1), \n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Declare all the layers for classification\n",
    "        self.classifier = nn.Sequential(nn.Linear(7 * 7 * 40, 1024),                  nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 2048), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, 10))"
   ]
  },
  {
   "source": [
    "# Exercise II: Sequential module - forward() method\n",
    "\n",
    "Now, that you have defined all the modules that the network needs, it is time to apply them in the `forward()` method. For context, we are giving the code for the `forward()` method, if the net was written in the usual way.\n",
    "\n",
    "```\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(7 * 7 * 40, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 10) \n",
    "\n",
    "    def forward():\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.pool(self.conv2(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.pool(self.conv4(x)))\n",
    "        x = x.view(-1, 7 * 7 * 40)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "Note: for evaluation purposes, the entire code of the class needs to be in the script. We are using the `__init__` method as you have coded it on the previous exercise, while you are going to code the `forward()` method here.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Extract the features from the images.\n",
    "- Squeeze the three spatial dimensions of the feature maps into one using the `view()` method.\n",
    "- Classify images based on the extracted features.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Declare all the layers for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1), \n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Declare all the layers for classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 40, 1024), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 2048), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Apply the feature extractor in the input\n",
    "        x = self.ReLU(self.features(x))\n",
    "        \n",
    "        # Squeeze the three spatial dimensions in one\n",
    "        x = x.view(-1, 7 * 7 * 40)\n",
    "        \n",
    "        # Classify the images\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "# (2) The problem of overfitting\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-28 020321.png\">\n",
    "\n",
    "## Detecting overfitting\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-28 020426.png\">\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-28 020451.png\">\n",
    "\n",
    "## Overfitting in the testing set\n",
    "\n",
    "- Training set\n",
    "- Validation set\n",
    "- Testing set\n",
    "\n",
    "## Validation set\n",
    "\n",
    "- Training set: train the model\n",
    "- Validation set: select the model\n",
    "- Testing set: test the model\n",
    "\n",
    "## Using validation sets in PyTorch\n",
    "\n",
    "```\n",
    "indices = np.arange(50000)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "        transform=transform.Compose([transform.ToTensor(),\n",
    "        transform.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])),\n",
    "        batch_size=1, shuffle=False, sampler=torch.Utils.data.SubsetRandomSampler(indices[:45000]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "        transform=transform.Compose([transform.ToTensor(),\n",
    "        transform.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])),\n",
    "        batch_size=1, shuffle=False, sampler=torch.Utils.data.SubsetRandomSampler(indices[45000:50000]))  \n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise III: Validation set\n",
    "\n",
    "You saw the need for validation set in the previous video. Problem is that the datasets typically are not separated into training, validation and testing. It is your job as a data scientist to split the dataset into training, testing and validation. The easiest (and most used) way of doing so is to do a random splitting of the dataset. In PyTorch, that can be done using `SubsetRandomSampler` object. You are going to split the training part of `MNIST` dataset into training and validation. After randomly shuffling the dataset, use the first `55000` points for training, and the remaining `5000` points for validation.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use `numpy.arange()` to create an array containing numbers [0, 59999] and then randomly shuffle the array.\n",
    "- In the `train_loader` using `SubsetRandomSampler()` use the first `55k` points for training.\n",
    "- In the `val_loader` use the remaining `5k` points for validation.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the indices\n",
    "indices = np.arange(60000)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Build the train loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('mnist', download=True, train=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor(),         \n",
    "            transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=64, shuffle=False,       \n",
    "    sampler=torch.utils.data.SubsetRandomSampler(indices[:55000]))\n",
    "\n",
    "# Build the validation loader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('mnist', download=True, train=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor(), \n",
    "            transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=64, shuffle=False,   \n",
    "    sampler=torch.utils.data.SubsetRandomSampler(indices[55000:60000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise IV: Detecting overfitting\n",
    "\n",
    "Overfitting is arguably the biggest problem in machine learning and data science, and being able to detect it will make you a much better data scientist. While reaching a high (or even perfect) accuracy on training sets is quite easy when you use neural networks, reaching a high accuracy on validation and testing sets is a very different thing.\n",
    "\n",
    "Let's see if you can now detect overfitting. Amongst the accuracy scores below, which network presents the biggest overfitting problem. ?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "- The accuracy in the training set is 90%, the accuracy in the validation set is 88%.\n",
    "- The accuracy in the training set is 90%, the accuracy in the testing set is 70%.\n",
    "- The accuracy in the training set is 90%, the accuracy in the validation set is 70%. (T)\n",
    "- The accuracy in the validation set is 85%, the accuracy in the testing set is 82%."
   ]
  }
 ]
}