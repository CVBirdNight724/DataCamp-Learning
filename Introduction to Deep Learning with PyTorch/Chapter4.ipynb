{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Using Convolutional Neural Networks\n",
    "\n",
    "In this last chapter, we learn how to make neural networks work well in practice, using concepts like regularization, batch-normalization and transfer learning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (1) The sequential module\n",
    "\n",
    "## AlexNet - declearing the modules\n",
    "\n",
    "```\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        self.relu = nn.Relu(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "```\n",
    "\n",
    "## AlexNet - forward() methods\n",
    "\n",
    "```\n",
    "def forward(self, x):\n",
    "    x = self.relu(self.conv1(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = self.relu(self.conv2(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = self.relu(self.conv3(x))\n",
    "    x = self.relu(self.conv4(x))\n",
    "    x = self.relu(self.conv5(x))\n",
    "    x = self.maxpool(x)\n",
    "    x = self.avgpool(x)\n",
    "    x = x.view(x.size(0), 256 * 6 * 6)\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.relu(self.fc2(x))\n",
    "    return self.fc3(x)\n",
    "```\n",
    "\n",
    "## The sequential modulde - declaring the modules\n",
    "\n",
    "```\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes), )\n",
    "```\n",
    "\n",
    "## The sequential module - forward() method\n",
    "\n",
    "```\n",
    "def forward(self, x):\n",
    "    x = self.features(x)\n",
    "    x = self.avgpool(x)\n",
    "    x = x.view(x.size(0), 256 * 6 * 6)\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise I: Sequential module - init method\n",
    "\n",
    "Having learned about the sequential module, now is the time to see how you can convert a neural network that doesn't use sequential modules to one that uses them. We are giving the code to build the network in the usual way, and you are going to write the code for the same network using sequential modules.\n",
    "\n",
    "```\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(7 * 7 * 40, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 10) \n",
    "```\n",
    "\n",
    "We want the pooling layer to be used after the second and fourth convolutional layers, while the relu nonlinearity needs to be used after each layer except the last (fully-connected) layer. For the number of filters (kernels), stride, passing, number of channels and number of units, use the same numbers as above.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Declare all the layers needed for feature extraction in the `self.features`.\n",
    "- Declare the three linear layers in `self.classifier`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Declare all the layers for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1), \n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Declare all the layers for classification\n",
    "        self.classifier = nn.Sequential(nn.Linear(7 * 7 * 40, 1024),                  nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 2048), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, 10))"
   ]
  },
  {
   "source": [
    "# Exercise II: Sequential module - forward() method\n",
    "\n",
    "Now, that you have defined all the modules that the network needs, it is time to apply them in the `forward()` method. For context, we are giving the code for the `forward()` method, if the net was written in the usual way.\n",
    "\n",
    "```\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(7 * 7 * 40, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 10) \n",
    "\n",
    "    def forward():\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.pool(self.conv2(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.pool(self.conv4(x)))\n",
    "        x = x.view(-1, 7 * 7 * 40)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "Note: for evaluation purposes, the entire code of the class needs to be in the script. We are using the `__init__` method as you have coded it on the previous exercise, while you are going to code the `forward()` method here.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Extract the features from the images.\n",
    "- Squeeze the three spatial dimensions of the feature maps into one using the `view()` method.\n",
    "- Classify images based on the extracted features.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Declare all the layers for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1), \n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Declare all the layers for classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(7 * 7 * 40, 1024), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 2048), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Apply the feature extractor in the input\n",
    "        x = self.ReLU(self.features(x))\n",
    "        \n",
    "        # Squeeze the three spatial dimensions in one\n",
    "        x = x.view(-1, 7 * 7 * 40)\n",
    "        \n",
    "        # Classify the images\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "# (2) The problem of overfitting\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-28 020321.png\">\n",
    "\n",
    "## Detecting overfitting\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-28 020426.png\">\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-28 020451.png\">\n",
    "\n",
    "## Overfitting in the testing set\n",
    "\n",
    "- Training set\n",
    "- Validation set\n",
    "- Testing set\n",
    "\n",
    "## Validation set\n",
    "\n",
    "- Training set: train the model\n",
    "- Validation set: select the model\n",
    "- Testing set: test the model\n",
    "\n",
    "## Using validation sets in PyTorch\n",
    "\n",
    "```\n",
    "indices = np.arange(50000)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "        transform=transform.Compose([transform.ToTensor(),\n",
    "        transform.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])),\n",
    "        batch_size=1, shuffle=False, sampler=torch.Utils.data.SubsetRandomSampler(indices[:45000]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "        transform=transform.Compose([transform.ToTensor(),\n",
    "        transform.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])),\n",
    "        batch_size=1, shuffle=False, sampler=torch.Utils.data.SubsetRandomSampler(indices[45000:50000]))  \n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise III: Validation set\n",
    "\n",
    "You saw the need for validation set in the previous video. Problem is that the datasets typically are not separated into training, validation and testing. It is your job as a data scientist to split the dataset into training, testing and validation. The easiest (and most used) way of doing so is to do a random splitting of the dataset. In PyTorch, that can be done using `SubsetRandomSampler` object. You are going to split the training part of `MNIST` dataset into training and validation. After randomly shuffling the dataset, use the first `55000` points for training, and the remaining `5000` points for validation.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Use `numpy.arange()` to create an array containing numbers [0, 59999] and then randomly shuffle the array.\n",
    "- In the `train_loader` using `SubsetRandomSampler()` use the first `55k` points for training.\n",
    "- In the `val_loader` use the remaining `5k` points for validation.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the indices\n",
    "indices = np.arange(60000)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Build the train loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('mnist', download=True, train=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor(),         \n",
    "            transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=64, shuffle=False,       \n",
    "    sampler=torch.utils.data.SubsetRandomSampler(indices[:55000]))\n",
    "\n",
    "# Build the validation loader\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('mnist', download=True, train=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor(), \n",
    "            transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=64, shuffle=False,   \n",
    "    sampler=torch.utils.data.SubsetRandomSampler(indices[55000:60000]))"
   ]
  },
  {
   "source": [
    "# Exercise IV: Detecting overfitting\n",
    "\n",
    "Overfitting is arguably the biggest problem in machine learning and data science, and being able to detect it will make you a much better data scientist. While reaching a high (or even perfect) accuracy on training sets is quite easy when you use neural networks, reaching a high accuracy on validation and testing sets is a very different thing.\n",
    "\n",
    "Let's see if you can now detect overfitting. Amongst the accuracy scores below, which network presents the biggest overfitting problem. ?\n",
    "\n",
    "### Possible Answers\n",
    "\n",
    "- The accuracy in the training set is 90%, the accuracy in the validation set is 88%.\n",
    "- The accuracy in the training set is 90%, the accuracy in the testing set is 70%.\n",
    "- The accuracy in the training set is 90%, the accuracy in the validation set is 70%. (T)\n",
    "- The accuracy in the validation set is 85%, the accuracy in the testing set is 82%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# (3) Regularization techniques\n",
    "\n",
    "## L2-regularization\n",
    "\n",
    "$$C = -\\frac{1}{n}\\sum_{xj} [y_j lna_j ^L + (1-a_j ^L)] + \\frac{\\lambda}{2n} \\sum_{w} {w^2}$$\n",
    "\n",
    "```\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4, weight_decay=0.0001)\n",
    "```\n",
    "\n",
    "## Dropout\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-28 031145.png\">\n",
    "\n",
    "## Dropout in AlexNet - PyTorch code\n",
    "\n",
    "```\n",
    "self.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(256 * 6 * 6, 4096)\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(4096, num_classes,\n",
    ")\n",
    "```\n",
    "\n",
    "## Batch - normalization\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-28 031513.png\">\n",
    "\n",
    "## Early - stopping\n",
    "\n",
    "<img src=\"image/Screenshot 2021-01-28 031601.png\">\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "Question: How to choose all these hyperparameters (l2 regularization, dropout parameter, optimizers (Adam vs gradient descent etc), batch-norm momentum and epsilonm number of epochs for early stopping etc)?\n",
    "\n",
    "Answer: Train many networks with different hyperparameters (typically use random values for them), and test them in the validation set. Then use the best performing net in the validation set to know the expected accuracy of the network in new data\n",
    "\n",
    "## Eval() mode\n",
    "\n",
    "```\n",
    "# Sets the net in trian mode\n",
    "model.train()\n",
    "\n",
    "# Sets the net in evaluation mode\n",
    "model.eval()\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercise V: L2-regularization\n",
    "\n",
    "You are going to implement each of the regularization techniques explained in the previous video. Doing so, you will also remember important concepts studied throughout the course. You will start with l2-regularization, the most important regularization technique in machine learning. As you saw in the video, l2-regularization simply penalizes large weights, and thus enforces the network to use only small weights.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Instantiate an object called `model` from class `Net()`, which is available in your workspace (consider it as a blackbox).\n",
    "- Instantiate the cross-entropy loss.\n",
    "- Instantiate `Adam` optimizer with `learning_rate` equals to `3e-4`, and `l2` regularization parameter equals to `0.001`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the network\n",
    "model = Net()\n",
    "\n",
    "# Instantiate the cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate the Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.001)"
   ]
  },
  {
   "source": [
    "# Exercise VI: Dropout\n",
    "\n",
    "You saw that dropout is an effective technique to avoid overfitting. Typically, dropout is applied in fully-connected neural networks, or in the fully-connected layers of a convolutional neural network. You are now going to implement dropout and use it on a small fully-connected neural network.\n",
    "\n",
    "For the first hidden layer use `200` units, for the second hidden layer use `500` units, and for the output layer use `10` units (one for each class). For the activation function, use ReLU. Use `.Dropout()` with strength `0.5`, between the first and second hidden layer. Use the sequential module, with the order being: `fully-connected`, `activation`, `dropout`, `fully-connected`, `activation`, `fully-connected`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Implement the `__init__` method, based on the description of the network in the context."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Define all the parameters of the net\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(28*28, 200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(200, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(500, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "    \t# Do the forward pass\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "source": [
    "# Exercise VII: Batch-normalization\n",
    "\n",
    "Dropout is used to regularize fully-connected layers. Batch-normalization is used to make the training of convolutional neural networks more efficient, while at the same time having regularization effects. You are going to implement the `__init__` method of a small convolutional neural network, with batch-normalization. The feature extraction part of the CNN will contain the following modules (in order): `convolution`, `max-pool`, `activation`, `batch-norm`, `convolution`, `max-pool`, `relu`, `batch-norm`.\n",
    "\n",
    "The first convolutional layer will contain 10 output channels, while the second will contain 20 output channels. As always, we are going to use MNIST dataset, with images having shape (28, 28) in grayscale format (1 channel). In all cases, the size of the `filter` should be 3, the `stride` should be 1 and the `padding` should be `1`.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Implement the feature extraction part of the network, using the description in the context.\n",
    "- Implement the fully-connected (classifier) part of the network.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Implement the sequential module for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(10),\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(20))\n",
    "        \n",
    "        # Implement the fully connected layer for classification\n",
    "        self.fc = nn.Linear(in_features=7*7*20, out_features=10)"
   ]
  }
 ]
}